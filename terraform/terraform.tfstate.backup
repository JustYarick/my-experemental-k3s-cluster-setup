{
  "version": 4,
  "terraform_version": "1.13.4",
  "serial": 64,
  "lineage": "25dafca4-3c71-3aff-1d81-4b0f38c24506",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "argo-cd",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "argo-cd",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "argo-cd",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v3.1.9",
                "chart": "argo-cd",
                "first_deployed": 1761502371,
                "last_deployed": 1761503348,
                "name": "argo-cd",
                "namespace": "ci-cd",
                "notes": "\nIn order to access the server UI you have the following options:\n\n1. kubectl port-forward service/argo-cd-argocd-server -n ci-cd 8080:443\n\n    and then open the browser on http://localhost:8080 and accept the certificate\n\n2. enable ingress in the values file `server.ingress.enabled` and either\n      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough\n      - Set the `configs.params.\"server.insecure\"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts\n\n\nAfter reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:\n\nkubectl -n ci-cd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n\n(You should delete the initial secret afterwards as suggested by the Getting Started Guide: https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli)\n",
                "revision": 3,
                "values": "{\"apiVersionOverrides\":{},\"applicationSet\":{\"affinity\":{},\"allowAnyNamespace\":false,\"automountServiceAccountToken\":true,\"certificate\":{\"additionalHosts\":[],\"annotations\":{},\"domain\":\"\",\"duration\":\"\",\"enabled\":false,\"issuer\":{\"group\":\"\",\"kind\":\"\",\"name\":\"\"},\"privateKey\":{\"algorithm\":\"RSA\",\"encoding\":\"PKCS1\",\"rotationPolicy\":\"Never\",\"size\":2048},\"renewBefore\":\"\"},\"containerPorts\":{\"metrics\":8080,\"probe\":8081,\"webhook\":7000},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"sizeLimit\":\"\"},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraHosts\":[],\"extraPaths\":[],\"extraRules\":[],\"extraTls\":[],\"hostname\":\"\",\"ingressClassName\":\"\",\"labels\":{},\"path\":\"/api/webhook\",\"pathType\":\"Prefix\",\"tls\":false},\"initContainers\":[],\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8080,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"scrapeTimeout\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"applicationset-controller\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"runtimeClassName\":\"\",\"service\":{\"annotations\":{},\"labels\":{},\"port\":7000,\"portName\":\"http-webhook\",\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-applicationset-controller\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"commitServer\":{\"affinity\":{},\"automountServiceAccountToken\":false,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"enabled\":false,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"metrics\",\"servicePort\":8087,\"type\":\"ClusterIP\"}},\"name\":\"commit-server\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"timeoutSeconds\":1},\"resources\":{},\"runtimeClassName\":\"\",\"service\":{\"annotations\":{},\"labels\":{},\"port\":8086,\"portName\":\"server\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-commit-server\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"configs\":{\"clusterCredentials\":{},\"cm\":{\"admin.enabled\":true,\"annotations\":{},\"application.instanceLabelKey\":\"argocd.argoproj.io/instance\",\"application.sync.impersonation.enabled\":false,\"create\":true,\"exec.enabled\":false,\"resource.customizations.ignoreResourceUpdates.ConfigMap\":\"jqPathExpressions:\\n  # Ignore the cluster-autoscaler status\\n  - '.metadata.annotations.\\\"cluster-autoscaler.kubernetes.io/last-updated\\\"'\\n  # Ignore the annotation of the legacy Leases election\\n  - '.metadata.annotations.\\\"control-plane.alpha.kubernetes.io/leader\\\"'\\n\",\"resource.customizations.ignoreResourceUpdates.Endpoints\":\"jsonPointers:\\n  - /metadata\\n  - /subsets\\n\",\"resource.customizations.ignoreResourceUpdates.all\":\"jsonPointers:\\n  - /status\\n\",\"resource.customizations.ignoreResourceUpdates.apps_ReplicaSet\":\"jqPathExpressions:\\n  - '.metadata.annotations.\\\"deployment.kubernetes.io/desired-replicas\\\"'\\n  - '.metadata.annotations.\\\"deployment.kubernetes.io/max-replicas\\\"'\\n  - '.metadata.annotations.\\\"rollout.argoproj.io/desired-replicas\\\"'\\n\",\"resource.customizations.ignoreResourceUpdates.argoproj.io_Application\":\"jqPathExpressions:\\n  - '.metadata.annotations.\\\"notified.notifications.argoproj.io\\\"'\\n  - '.metadata.annotations.\\\"argocd.argoproj.io/refresh\\\"'\\n  - '.metadata.annotations.\\\"argocd.argoproj.io/hydrate\\\"'\\n  - '.operation'\\n\",\"resource.customizations.ignoreResourceUpdates.argoproj.io_Rollout\":\"jqPathExpressions:\\n  - '.metadata.annotations.\\\"notified.notifications.argoproj.io\\\"'\\n\",\"resource.customizations.ignoreResourceUpdates.autoscaling_HorizontalPodAutoscaler\":\"jqPathExpressions:\\n  - '.metadata.annotations.\\\"autoscaling.alpha.kubernetes.io/behavior\\\"'\\n  - '.metadata.annotations.\\\"autoscaling.alpha.kubernetes.io/conditions\\\"'\\n  - '.metadata.annotations.\\\"autoscaling.alpha.kubernetes.io/metrics\\\"'\\n  - '.metadata.annotations.\\\"autoscaling.alpha.kubernetes.io/current-metrics\\\"'\\n\",\"resource.customizations.ignoreResourceUpdates.discovery.k8s.io_EndpointSlice\":\"jsonPointers:\\n  - /metadata\\n  - /endpoints\\n  - /ports\\n\",\"resource.exclusions\":\"### Network resources created by the Kubernetes control plane and excluded to reduce the number of watched events and UI clutter\\n- apiGroups:\\n  - ''\\n  - discovery.k8s.io\\n  kinds:\\n  - Endpoints\\n  - EndpointSlice\\n### Internal Kubernetes resources excluded reduce the number of watched events\\n- apiGroups:\\n  - coordination.k8s.io\\n  kinds:\\n  - Lease\\n### Internal Kubernetes Authz/Authn resources excluded reduce the number of watched events\\n- apiGroups:\\n  - authentication.k8s.io\\n  - authorization.k8s.io\\n  kinds:\\n  - SelfSubjectReview\\n  - TokenReview\\n  - LocalSubjectAccessReview\\n  - SelfSubjectAccessReview\\n  - SelfSubjectRulesReview\\n  - SubjectAccessReview\\n### Intermediate Certificate Request excluded reduce the number of watched events\\n- apiGroups:\\n  - certificates.k8s.io\\n  kinds:\\n  - CertificateSigningRequest\\n- apiGroups:\\n  - cert-manager.io\\n  kinds:\\n  - CertificateRequest\\n### Cilium internal resources excluded reduce the number of watched events and UI Clutter\\n- apiGroups:\\n  - cilium.io\\n  kinds:\\n  - CiliumIdentity\\n  - CiliumEndpoint\\n  - CiliumEndpointSlice\\n### Kyverno intermediate and reporting resources excluded reduce the number of watched events and improve performance\\n- apiGroups:\\n  - kyverno.io\\n  - reports.kyverno.io\\n  - wgpolicyk8s.io\\n  kinds:\\n  - PolicyReport\\n  - ClusterPolicyReport\\n  - EphemeralReport\\n  - ClusterEphemeralReport\\n  - AdmissionReport\\n  - ClusterAdmissionReport\\n  - BackgroundScanReport\\n  - ClusterBackgroundScanReport\\n  - UpdateRequest\\n\",\"server.rbac.log.enforce.enable\":false,\"statusbadge.enabled\":false,\"timeout.hard.reconciliation\":\"0s\",\"timeout.reconciliation\":\"180s\"},\"cmp\":{\"annotations\":{},\"create\":false,\"plugins\":{}},\"credentialTemplates\":{},\"credentialTemplatesAnnotations\":{},\"gpg\":{\"annotations\":{},\"keys\":{}},\"params\":{\"annotations\":{},\"create\":true},\"rbac\":{\"annotations\":{},\"create\":true,\"policy.csv\":\"\",\"policy.default\":\"\",\"policy.matchMode\":\"glob\",\"scopes\":\"[groups]\"},\"repositories\":{},\"repositoriesAnnotations\":{},\"secret\":{\"annotations\":{},\"argocdServerAdminPassword\":\"\",\"argocdServerAdminPasswordMtime\":\"\",\"azureDevops\":{\"password\":\"\",\"username\":\"\"},\"bitbucketServerSecret\":\"\",\"bitbucketUUID\":\"\",\"createSecret\":true,\"extra\":{},\"githubSecret\":\"\",\"gitlabSecret\":\"\",\"gogsSecret\":\"\",\"labels\":{}},\"ssh\":{\"annotations\":{},\"create\":true,\"extraHosts\":\"\",\"knownHosts\":\"[ssh.github.com]:443 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\n[ssh.github.com]:443 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\n[ssh.github.com]:443 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\nbitbucket.org ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBPIQmuzMBuKdWeF4+a2sjSSpBK0iqitSQ+5BM9KhpexuGt20JpTVM7u5BDZngncgrqDMbWdxMWWOGtZ9UgbqgZE=\\nbitbucket.org ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIazEu89wgQZ4bqs3d63QSMzYVa0MuJ2e2gKTKqu+UUO\\nbitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDQeJzhupRu0u0cdegZIa8e86EG2qOCsIsD1Xw0xSeiPDlCr7kq97NLmMbpKTX6Esc30NuoqEEHCuc7yWtwp8dI76EEEB1VqY9QJq6vk+aySyboD5QF61I/1WeTwu+deCbgKMGbUijeXhtfbxSxm6JwGrXrhBdofTsbKRUsrN1WoNgUa8uqN1Vx6WAJw1JHPhglEGGHea6QICwJOAr/6mrui/oB7pkaWKHj3z7d1IC4KWLtY47elvjbaTlkN04Kc/5LFEirorGYVbt15kAUlqGM65pk6ZBxtaO3+30LVlORZkxOh+LKL/BvbZ/iRNhItLqNyieoQj/uh/7Iv4uyH/cV/0b4WDSd3DptigWq84lJubb9t/DnZlrJazxyDCulTmKdOR7vs9gMTo+uoIrPSb8ScTtvw65+odKAlBj59dhnVp9zd7QUojOpXlL62Aw56U4oO+FALuevvMjiWeavKhJqlR7i5n9srYcrNV7ttmDw7kf/97P5zauIhxcjX+xHv4M=\\ngithub.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\ngithub.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\ngithub.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\ngitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=\\ngitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf\\ngitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\\nssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\\nvs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\\n\"},\"styles\":\"\",\"tls\":{\"annotations\":{},\"certificates\":{},\"create\":true}},\"controller\":{\"affinity\":{},\"automountServiceAccountToken\":true,\"clusterRoleRules\":{\"enabled\":false,\"rules\":[]},\"containerPorts\":{\"metrics\":8082},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"dynamicClusterDistribution\":false,\"emptyDir\":{\"sizeLimit\":\"\"},\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"heartbeatTime\":10,\"hostNetwork\":false,\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainers\":[],\"metrics\":{\"applicationLabels\":{\"enabled\":false,\"labels\":[]},\"enabled\":false,\"rules\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"namespace\":\"\",\"selector\":{},\"spec\":[]},\"scrapeTimeout\":\"\",\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8082,\"type\":\"NodePort\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"application-controller\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":5,\"roleRules\":[],\"runtimeClassName\":\"\",\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-application-controller\"},\"statefulsetAnnotations\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[],\"vpa\":{\"annotations\":{},\"containerPolicy\":{},\"enabled\":false,\"labels\":{},\"updateMode\":\"Initial\"}},\"crds\":{\"additionalLabels\":{},\"annotations\":{},\"install\":true,\"keep\":true},\"createAggregateRoles\":false,\"createClusterRoles\":true,\"dex\":{\"affinity\":{},\"automountServiceAccountToken\":true,\"certificateSecret\":{\"annotations\":{},\"ca\":\"\",\"crt\":\"\",\"enabled\":false,\"key\":\"\",\"labels\":{}},\"containerPorts\":{\"grpc\":5557,\"http\":5556,\"metrics\":5558},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"sizeLimit\":\"\"},\"enabled\":false,\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"ghcr.io/dexidp/dex\",\"tag\":\"v2.44.0\"},\"imagePullSecrets\":[],\"initContainers\":[],\"initImage\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"resources\":{},\"tag\":\"\"},\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"httpPath\":\"/healthz/live\",\"httpPort\":\"metrics\",\"httpScheme\":\"HTTP\",\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"labels\":{},\"portName\":\"http-metrics\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"dex-server\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"httpPath\":\"/healthz/ready\",\"httpPort\":\"metrics\",\"httpScheme\":\"HTTP\",\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"runtimeClassName\":\"\",\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"argocd-dex-server\"},\"servicePortGrpc\":5557,\"servicePortGrpcName\":\"grpc\",\"servicePortHttp\":5556,\"servicePortHttpName\":\"http\",\"servicePortMetrics\":5558,\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"externalRedis\":{\"existingSecret\":\"\",\"host\":\"\",\"password\":\"\",\"port\":6379,\"secretAnnotations\":{},\"username\":\"\"},\"extraObjects\":[],\"fullnameOverride\":\"\",\"global\":{\"addPrometheusAnnotations\":false,\"additionalLabels\":{},\"affinity\":{\"nodeAffinity\":{\"matchExpressions\":[],\"type\":\"hard\"},\"podAntiAffinity\":\"soft\"},\"certificateAnnotations\":{},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"deploymentStrategy\":{},\"domain\":\"argocd.example.com\",\"dualStack\":{\"ipFamilies\":[],\"ipFamilyPolicy\":\"\"},\"env\":[],\"hostAliases\":[],\"image\":{\"imagePullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/argoproj/argocd\",\"tag\":\"\"},\"imagePullSecrets\":[],\"logging\":{\"format\":\"text\",\"level\":\"info\"},\"networkPolicy\":{\"create\":false,\"defaultDenyIngress\":false},\"nodeSelector\":{\"kubernetes.io/os\":\"linux\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"revisionHistoryLimit\":3,\"runtimeClassName\":\"\",\"securityContext\":{},\"statefulsetAnnotations\":{},\"tolerations\":[],\"topologySpreadConstraints\":[]},\"kubeVersionOverride\":\"\",\"nameOverride\":\"argocd\",\"namespaceOverride\":\"\",\"notifications\":{\"affinity\":{},\"argocdUrl\":\"\",\"automountServiceAccountToken\":true,\"clusterRoleRules\":{\"rules\":[]},\"cm\":{\"create\":true},\"containerPorts\":{\"metrics\":9001},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"context\":{},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"deploymentStrategy\":{\"type\":\"Recreate\"},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"enabled\":false,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainers\":[],\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"port\":9001,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"honorLabels\":false,\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"notifications-controller\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"notifiers\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"runtimeClassName\":\"\",\"secret\":{\"annotations\":{},\"create\":true,\"items\":{},\"labels\":{},\"name\":\"argocd-notifications-secret\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-notifications-controller\"},\"subscriptions\":[],\"templates\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"triggers\":{}},\"openshift\":{\"enabled\":false},\"redis\":{\"affinity\":{},\"automountServiceAccountToken\":true,\"containerPorts\":{\"metrics\":9121,\"redis\":6379},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"enabled\":true,\"env\":[],\"envFrom\":[],\"exporter\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"enabled\":false,\"env\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"ghcr.io/oliver006/redis_exporter\",\"tag\":\"v1.79.0\"},\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":5,\"initialDelaySeconds\":30,\"periodSeconds\":15,\"successThreshold\":1,\"timeoutSeconds\":15},\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":5,\"initialDelaySeconds\":30,\"periodSeconds\":15,\"successThreshold\":1,\"timeoutSeconds\":15},\"resources\":{}},\"extraArgs\":[],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"ecr-public.aws.com/docker/library/redis\",\"tag\":\"7.2.11-alpine\"},\"imagePullSecrets\":[],\"initContainers\":[],\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":5,\"initialDelaySeconds\":30,\"periodSeconds\":15,\"successThreshold\":1,\"timeoutSeconds\":15},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"None\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":9121,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"redis\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":5,\"initialDelaySeconds\":30,\"periodSeconds\":15,\"successThreshold\":1,\"timeoutSeconds\":15},\"resources\":{},\"runtimeClassName\":\"\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":999,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"annotations\":{},\"labels\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":false,\"create\":false,\"name\":\"\"},\"servicePort\":6379,\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"redis-ha\":{\"additionalAffinities\":{},\"affinity\":\"\",\"auth\":true,\"containerSecurityContext\":{\"readOnlyRootFilesystem\":true},\"enabled\":false,\"existingSecret\":\"argocd-redis\",\"exporter\":{\"enabled\":false,\"image\":\"ghcr.io/oliver006/redis_exporter\",\"tag\":\"v1.75.0\"},\"haproxy\":{\"additionalAffinities\":{},\"affinity\":\"\",\"containerSecurityContext\":{\"readOnlyRootFilesystem\":true},\"enabled\":true,\"hardAntiAffinity\":true,\"image\":{\"repository\":\"ecr-public.aws.com/docker/library/haproxy\"},\"labels\":{\"app.kubernetes.io/name\":\"argocd-redis-ha-haproxy\"},\"metrics\":{\"enabled\":true},\"tolerations\":[]},\"hardAntiAffinity\":true,\"image\":{\"repository\":\"ecr-public.aws.com/docker/library/redis\",\"tag\":\"7.2.11-alpine\"},\"persistentVolume\":{\"enabled\":false},\"redis\":{\"config\":{\"save\":\"\\\"\\\"\"},\"masterGroupName\":\"argocd\"},\"tolerations\":[],\"topologySpreadConstraints\":{\"enabled\":false,\"maxSkew\":\"\",\"topologyKey\":\"\",\"whenUnsatisfiable\":\"\"}},\"redisSecretInit\":{\"affinity\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"enabled\":true,\"extraArgs\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"jobAnnotations\":{},\"name\":\"redis-secret-init\",\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"resources\":{},\"runtimeClassName\":\"\",\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"tolerations\":[]},\"repoServer\":{\"affinity\":{},\"automountServiceAccountToken\":true,\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":5,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"certificateSecret\":{\"annotations\":{},\"ca\":\"\",\"crt\":\"\",\"enabled\":false,\"key\":\"\",\"labels\":{}},\"clusterRoleRules\":{\"enabled\":false,\"rules\":[]},\"containerPorts\":{\"metrics\":8084,\"server\":8081},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"copyutil\":{\"resources\":{}},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"sizeLimit\":\"\"},\"env\":[],\"envFrom\":[],\"existingVolumes\":{},\"extraArgs\":[],\"extraContainers\":[],\"hostNetwork\":false,\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainers\":[],\"lifecycle\":{},\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8084,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"scrapeTimeout\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"repo-server\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"rbac\":[],\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"runtimeClassName\":\"\",\"service\":{\"annotations\":{},\"labels\":{},\"port\":8081,\"portName\":\"tcp-repo-server\",\"trafficDistribution\":\"\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"useEphemeralHelmWorkingDir\":true,\"volumeMounts\":[],\"volumes\":[]},\"server\":{\"affinity\":{},\"automountServiceAccountToken\":true,\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":5,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"backendTLSPolicy\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"targetRefs\":[],\"validation\":{}},\"certificate\":{\"additionalHosts\":[],\"annotations\":{},\"domain\":\"\",\"duration\":\"\",\"enabled\":false,\"issuer\":{\"group\":\"\",\"kind\":\"\",\"name\":\"\"},\"privateKey\":{\"algorithm\":\"RSA\",\"encoding\":\"PKCS1\",\"rotationPolicy\":\"Never\",\"size\":2048},\"renewBefore\":\"\",\"secretTemplateAnnotations\":{},\"usages\":[]},\"certificateSecret\":{\"annotations\":{},\"crt\":\"\",\"enabled\":false,\"key\":\"\",\"labels\":{}},\"clusterRoleRules\":{\"enabled\":false,\"rules\":[]},\"containerPorts\":{\"metrics\":8083,\"server\":8080},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentLabels\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"sizeLimit\":\"\"},\"env\":[],\"envFrom\":[],\"extensions\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"enabled\":false,\"extensionList\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"quay.io/argoprojlabs/argocd-extension-installer\",\"tag\":\"v0.0.8\"},\"resources\":{}},\"extraArgs\":[],\"extraContainers\":[],\"grpcroute\":{\"annotations\":{},\"enabled\":false,\"hostnames\":[],\"labels\":{},\"parentRefs\":[],\"rules\":[{\"matches\":[{\"method\":{\"type\":\"Exact\"}}]}]},\"hostNetwork\":false,\"httproute\":{\"annotations\":{},\"enabled\":false,\"hostnames\":[],\"labels\":{},\"parentRefs\":[],\"rules\":[{\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}]}]},\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"ingress\":{\"annotations\":{},\"aws\":{\"backendProtocolVersion\":\"GRPC\",\"serviceType\":\"NodePort\"},\"controller\":\"generic\",\"enabled\":false,\"extraHosts\":[],\"extraPaths\":[],\"extraRules\":[],\"extraTls\":[],\"gke\":{\"backendConfig\":{},\"frontendConfig\":{},\"managedCertificate\":{\"create\":true,\"extraDomains\":[]}},\"hostname\":\"\",\"ingressClassName\":\"\",\"labels\":{},\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":false},\"ingressGrpc\":{\"annotations\":{},\"enabled\":false,\"extraHosts\":[],\"extraPaths\":[],\"extraRules\":[],\"extraTls\":[],\"hostname\":\"\",\"ingressClassName\":\"\",\"labels\":{},\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":false},\"initContainers\":[],\"lifecycle\":{},\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8083,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"honorLabels\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"scrapeTimeout\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"server\",\"networkPolicy\":{\"create\":false},\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"route\":{\"annotations\":{},\"enabled\":false,\"hostname\":\"\",\"termination_policy\":\"None\",\"termination_type\":\"passthrough\"},\"runtimeClassName\":\"\",\"service\":{\"annotations\":{},\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"labels\":{},\"loadBalancerClass\":\"\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePortHttp\":30080,\"nodePortHttps\":30443,\"servicePortHttp\":80,\"servicePortHttpName\":\"http\",\"servicePortHttps\":443,\"servicePortHttpsAppProtocol\":\"\",\"servicePortHttpsName\":\"https\",\"sessionAffinity\":\"None\",\"type\":\"NodePort\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-server\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]}}",
                "version": "9.0.5"
              }
            ],
            "name": "argo-cd",
            "namespace": "ci-cd",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "## Argo CD configuration\n## Ref: https://github.com/argoproj/argo-cd\n##\n\n# -- Provide a name in place of `argocd`\nnameOverride: argocd\n# -- String to fully override `\"argo-cd.fullname\"`\nfullnameOverride: \"\"\n# -- Override the namespace\n# @default -- `.Release.Namespace`\nnamespaceOverride: \"\"\n# -- Override the Kubernetes version, which is used to evaluate certain manifests\nkubeVersionOverride: \"\"\n# Override APIVersions\n# If you want to template helm charts but cannot access k8s API server\n# you can set api versions here\napiVersionOverrides: {}\n\n# -- Create aggregated roles that extend existing cluster roles to interact with argo-cd resources\n## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\ncreateAggregateRoles: false\n# -- Create cluster roles for cluster-wide installation.\n## Used when you manage applications in the same cluster where Argo CD runs\ncreateClusterRoles: true\n\nopenshift:\n  # -- enables using arbitrary uid for argo repo server\n  enabled: false\n\n## Custom resource configuration\ncrds:\n  # -- Install and upgrade CRDs\n  install: true\n  # -- Keep CRDs on chart uninstall\n  keep: true\n  # -- Annotations to be added to all CRDs\n  annotations: {}\n  # -- Additional labels to be added to all CRDs\n  additionalLabels: {}\n\n## Globally shared configuration\nglobal:\n  # -- Default domain used by all components\n  ## Used for ingresses, certificates, SSO, notifications, etc.\n  domain: argocd.example.com\n\n  # -- Runtime class name for all components\n  runtimeClassName: \"\"\n\n  # -- Common labels for the all resources\n  additionalLabels: {}\n    # app: argo-cd\n\n  # -- Number of old deployment ReplicaSets to retain. The rest will be garbage collected.\n  revisionHistoryLimit: 3\n\n  # Default image used by all components\n  image:\n    # -- If defined, a repository applied to all Argo CD deployments\n    repository: quay.io/argoproj/argocd\n    # -- Overrides the global Argo CD image tag whose default is the chart appVersion\n    tag: \"\"\n    # -- If defined, a imagePullPolicy applied to all Argo CD deployments\n    imagePullPolicy: IfNotPresent\n\n  # -- Secrets with credentials to pull images from a private registry\n  imagePullSecrets: []\n\n  # Default logging options used by all components\n  logging:\n    # -- Set the global logging format. Either: `text` or `json`\n    format: text\n    # -- Set the global logging level. One of: `debug`, `info`, `warn` or `error`\n    level: info\n\n  # -- Annotations for the all deployed Statefulsets\n  statefulsetAnnotations: {}\n\n  # -- Annotations for the all deployed Deployments\n  deploymentAnnotations: {}\n\n  # -- Labels for the all deployed Deployments\n  deploymentLabels: {}\n\n  # -- Annotations for the all deployed pods\n  podAnnotations: {}\n\n  # -- Labels for the all deployed pods\n  podLabels: {}\n\n  # -- Add Prometheus scrape annotations to all metrics services. This can be used as an alternative to the ServiceMonitors.\n  addPrometheusAnnotations: false\n\n  # -- Toggle and define pod-level security context.\n  # @default -- `{}` (See [values.yaml])\n  securityContext: {}\n  #  runAsUser: 999\n  #  runAsGroup: 999\n  #  fsGroup: 999\n\n  # -- Mapping between IP and hostnames that will be injected as entries in the pod's hosts files\n  hostAliases: []\n  # - ip: 10.20.30.40\n  #   hostnames:\n  #   - git.myhostname\n\n  # Configure dual-stack used by all component services\n  dualStack:\n    # -- IP family policy to configure dual-stack see [Configure dual-stack](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services)\n    ipFamilyPolicy: \"\"\n    # -- IP families that should be supported and the order in which they should be applied to ClusterIP as well. Can be IPv4 and/or IPv6.\n    ipFamilies: []\n\n  # Default network policy rules used by all components\n  networkPolicy:\n    # -- Create NetworkPolicy objects for all components\n    create: false\n    # -- Default deny all ingress traffic\n    defaultDenyIngress: false\n\n  # -- Default priority class for all components\n  priorityClassName: \"\"\n\n  # -- Default node selector for all components\n  nodeSelector:\n    kubernetes.io/os: linux\n\n  # -- Default tolerations for all components\n  tolerations: []\n\n  # Default affinity preset for all components\n  affinity:\n    # -- Default pod anti-affinity rules. Either: `none`, `soft` or `hard`\n    podAntiAffinity: soft\n    # Node affinity rules\n    nodeAffinity:\n      # -- Default node affinity rules. Either: `none`, `soft` or `hard`\n      type: hard\n      # -- Default match expressions for node affinity\n      matchExpressions: []\n        # - key: topology.kubernetes.io/zone\n        #   operator: In\n        #   values:\n        #    - antarctica-east1\n        #    - antarctica-west1\n\n  # -- Default [TopologySpreadConstraints] rules for all components\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector of the component\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy for the all deployed Deployments\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Environment variables to pass to all deployed Deployments\n  env: []\n\n  # -- Annotations for the all deployed Certificates\n  certificateAnnotations: {}\n\n## Argo Configs\nconfigs:\n  # General Argo CD configuration. Any values you put under `.configs.cm` are passed to argocd-cm ConfigMap.\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cm.yaml\n  cm:\n    # -- Create the argocd-cm configmap for [declarative setup]\n    create: true\n\n    # -- Annotations to be added to argocd-cm configmap\n    annotations: {}\n\n    # -- The name of tracking label used by Argo CD for resource pruning\n    application.instanceLabelKey: argocd.argoproj.io/instance\n\n    # -- Enable control of the service account used for the sync operation (alpha)\n    ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/app-sync-using-impersonation/\n    application.sync.impersonation.enabled: false\n\n    # -- Enable logs RBAC enforcement\n    ## Ref: https://argo-cd.readthedocs.io/en/latest/operator-manual/upgrading/2.3-2.4/#enable-logs-rbac-enforcement\n    server.rbac.log.enforce.enable: false\n\n    # -- Enable exec feature in Argo UI\n    ## Ref: https://argo-cd.readthedocs.io/en/latest/operator-manual/rbac/#exec-resource\n    exec.enabled: false\n\n    # -- Enable local admin user\n    ## Ref: https://argo-cd.readthedocs.io/en/latest/faq/#how-to-disable-admin-user\n    admin.enabled: true\n\n    # -- Timeout to discover if a new manifests version got published to the repository\n    timeout.reconciliation: 180s\n\n    # -- Timeout to refresh application data as well as target manifests cache\n    timeout.hard.reconciliation: 0s\n\n    # -- Enable Status Badge\n    ## Ref: https://argo-cd.readthedocs.io/en/stable/user-guide/status-badge/\n    statusbadge.enabled: false\n\n    # Dex configuration\n    # dex.config: |\n    #   connectors:\n    #     # GitHub example\n    #     - type: github\n    #       id: github\n    #       name: GitHub\n    #       config:\n    #         clientID: aabbccddeeff00112233\n    #         clientSecret: $dex.github.clientSecret # Alternatively $\u003csome_K8S_secret\u003e:dex.github.clientSecret\n    #         orgs:\n    #         - name: your-github-org\n\n    # OIDC configuration as an alternative to dex (optional).\n    # oidc.config: |\n    #   name: AzureAD\n    #   issuer: https://login.microsoftonline.com/TENANT_ID/v2.0\n    #   clientID: aaaabbbbccccddddeee\n    #   clientSecret: $oidc.azuread.clientSecret\n\n    # Some OIDC providers require a separate clientID for different callback URLs.\n    # For example, if configuring Argo CD with self-hosted Dex, you will need a separate client ID\n    # for the 'localhost' (CLI) client to Dex. This field is optional. If omitted, the CLI will\n    # use the same clientID as the Argo CD server\n    #   cliClientID: vvvvwwwwxxxxyyyyzzzz\n\n    #   rootCA: |\n    #     -----BEGIN CERTIFICATE-----\n    #     ... encoded certificate data here ...\n    #     -----END CERTIFICATE-----\n\n    # Optional list of allowed aud claims. If omitted or empty, defaults to the clientID value above (and the\n    # cliClientID, if that is also specified). If you specify a list and want the clientID to be allowed, you must\n    # explicitly include it in the list.\n    # Token verification will pass if any of the token's audiences matches any of the audiences in this list.\n    #   allowedAudiences:\n    #     - aaaabbbbccccddddeee\n    #     - qqqqwwwweeeerrrrttt\n\n    # Optional set of OIDC claims to request on the ID token.\n    #   requestedIDTokenClaims:\n    #     groups:\n    #       essential: true\n\n    # Optional set of OIDC scopes to request. If omitted, defaults to: [\"openid\", \"profile\", \"email\", \"groups\"]\n    #   requestedScopes:\n    #     - openid\n    #     - profile\n    #     - email\n\n    # PKCE authentication flow processes authorization flow from browser only - default false\n    # uses the clientID\n    # make sure the Identity Provider (IdP) is public and doesn't need clientSecret\n    # make sure the Identity Provider (IdP) has this redirect URI registered: https://argocd.example.com/pkce/verify\n    #   enablePKCEAuthentication: true\n\n    # Extension Configuration\n    ## Ref: https://argo-cd.readthedocs.io/en/latest/developer-guide/extensions/proxy-extensions/\n    # extension.config: |\n    #   extensions:\n    #   - name: httpbin\n    #     backend:\n    #       connectionTimeout: 2s\n    #       keepAlive: 15s\n    #       idleConnectionTimeout: 60s\n    #       maxIdleConnections: 30\n    #       services:\n    #       - url: http://httpbin.org\n    #         headers:\n    #         - name: some-header\n    #           value: '$some.argocd.secret.key'\n    #         cluster:\n    #           name: some-cluster\n    #           server: https://some-cluster\n\n    ## Default configuration for ignoreResourceUpdates.\n    ## The ignoreResourceUpdates list contains K8s resource's properties that are known to be frequently updated\n    ## by controllers and operators. These resources, when watched by argo, will cause many unnecessary updates.\n\n    # -- Ignoring status for all resources. An update will still be sent if the status update causes the health to change.\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.all: |\n      jsonPointers:\n        - /status\n    # -- Some Application fields are generated and not related to the application updates itself\n    ## The Application itself is already watched by the controller lister, but this configuration is applied for apps of apps\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.argoproj.io_Application: |\n      jqPathExpressions:\n        - '.metadata.annotations.\"notified.notifications.argoproj.io\"'\n        - '.metadata.annotations.\"argocd.argoproj.io/refresh\"'\n        - '.metadata.annotations.\"argocd.argoproj.io/hydrate\"'\n        - '.operation'\n    # -- Ignore Argo Rollouts generated fields\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.argoproj.io_Rollout: |\n      jqPathExpressions:\n        - '.metadata.annotations.\"notified.notifications.argoproj.io\"'\n    # -- Legacy annotations used on HPA autoscaling/v1\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.autoscaling_HorizontalPodAutoscaler: |\n      jqPathExpressions:\n        - '.metadata.annotations.\"autoscaling.alpha.kubernetes.io/behavior\"'\n        - '.metadata.annotations.\"autoscaling.alpha.kubernetes.io/conditions\"'\n        - '.metadata.annotations.\"autoscaling.alpha.kubernetes.io/metrics\"'\n        - '.metadata.annotations.\"autoscaling.alpha.kubernetes.io/current-metrics\"'\n    # -- Ignore the cluster-autoscaler status\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.ConfigMap: |\n      jqPathExpressions:\n        # Ignore the cluster-autoscaler status\n        - '.metadata.annotations.\"cluster-autoscaler.kubernetes.io/last-updated\"'\n        # Ignore the annotation of the legacy Leases election\n        - '.metadata.annotations.\"control-plane.alpha.kubernetes.io/leader\"'\n    # -- Ignore the common scaling annotations\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.apps_ReplicaSet: |\n      jqPathExpressions:\n        - '.metadata.annotations.\"deployment.kubernetes.io/desired-replicas\"'\n        - '.metadata.annotations.\"deployment.kubernetes.io/max-replicas\"'\n        - '.metadata.annotations.\"rollout.argoproj.io/desired-replicas\"'\n    # -- Ignores update if EndpointSlice is not excluded globally\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.discovery.k8s.io_EndpointSlice: |\n      jsonPointers:\n        - /metadata\n        - /endpoints\n        - /ports\n    # -- Ignores update if Endpoints is not excluded globally\n    # @default -- See [values.yaml]\n    resource.customizations.ignoreResourceUpdates.Endpoints: |\n      jsonPointers:\n        - /metadata\n        - /subsets\n\n    ## Default configuration for exclusions.\n    ## The exclusion list are K8s resources that we assume will never be declared in Git,\n    ## and are never child objects of managed resources that need to be presented in the resource tree.\n    ## This list contains high volume and  high churn metadata objects which we exclude for performance\n    ## reasons, reducing connections and load to the K8s API servers of managed clusters.\n\n    # -- Resource Exclusion/Inclusion\n    # @default -- See [values.yaml]\n    resource.exclusions: |\n      ### Network resources created by the Kubernetes control plane and excluded to reduce the number of watched events and UI clutter\n      - apiGroups:\n        - ''\n        - discovery.k8s.io\n        kinds:\n        - Endpoints\n        - EndpointSlice\n      ### Internal Kubernetes resources excluded reduce the number of watched events\n      - apiGroups:\n        - coordination.k8s.io\n        kinds:\n        - Lease\n      ### Internal Kubernetes Authz/Authn resources excluded reduce the number of watched events\n      - apiGroups:\n        - authentication.k8s.io\n        - authorization.k8s.io\n        kinds:\n        - SelfSubjectReview\n        - TokenReview\n        - LocalSubjectAccessReview\n        - SelfSubjectAccessReview\n        - SelfSubjectRulesReview\n        - SubjectAccessReview\n      ### Intermediate Certificate Request excluded reduce the number of watched events\n      - apiGroups:\n        - certificates.k8s.io\n        kinds:\n        - CertificateSigningRequest\n      - apiGroups:\n        - cert-manager.io\n        kinds:\n        - CertificateRequest\n      ### Cilium internal resources excluded reduce the number of watched events and UI Clutter\n      - apiGroups:\n        - cilium.io\n        kinds:\n        - CiliumIdentity\n        - CiliumEndpoint\n        - CiliumEndpointSlice\n      ### Kyverno intermediate and reporting resources excluded reduce the number of watched events and improve performance\n      - apiGroups:\n        - kyverno.io\n        - reports.kyverno.io\n        - wgpolicyk8s.io\n        kinds:\n        - PolicyReport\n        - ClusterPolicyReport\n        - EphemeralReport\n        - ClusterEphemeralReport\n        - AdmissionReport\n        - ClusterAdmissionReport\n        - BackgroundScanReport\n        - ClusterBackgroundScanReport\n        - UpdateRequest\n\n\n  # Argo CD configuration parameters\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cmd-params-cm.yaml\n  params:\n    # -- Create the argocd-cmd-params-cm configmap\n    # If false, it is expected the configmap will be created by something else.\n    create: true\n\n    # -- Annotations to be added to the argocd-cmd-params-cm ConfigMap\n    annotations: {}\n\n    # You can customize parameters by adding parameters here.\n    # (e.g.)\n    # otlp.address: ''\n\n  # Argo CD RBAC policy configuration\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/rbac.md\n  rbac:\n    # -- Create the argocd-rbac-cm configmap with ([Argo CD RBAC policy]) definitions.\n    # If false, it is expected the configmap will be created by something else.\n    # Argo CD will not work if there is no configmap created with the name above.\n    create: true\n\n    # -- Annotations to be added to argocd-rbac-cm configmap\n    annotations: {}\n\n    # -- The name of the default role which Argo CD will falls back to, when authorizing API requests (optional).\n    # If omitted or empty, users may be still be able to login, but will see no apps, projects, etc...\n    policy.default: ''\n\n    # -- File containing user-defined policies and role definitions.\n    # @default -- `''` (See [values.yaml])\n    policy.csv: ''\n    # Policy rules are in the form:\n    #  p, subject, resource, action, object, effect\n    # Role definitions and bindings are in the form:\n    #  g, subject, inherited-subject\n    # policy.csv: |\n    #   p, role:org-admin, applications, *, */*, allow\n    #   p, role:org-admin, clusters, get, *, allow\n    #   p, role:org-admin, repositories, *, *, allow\n    #   p, role:org-admin, logs, get, *, allow\n    #   p, role:org-admin, exec, create, */*, allow\n    #   g, your-github-org:your-team, role:org-admin\n\n    # -- OIDC scopes to examine during rbac enforcement (in addition to `sub` scope).\n    # The scope value can be a string, or a list of strings.\n    scopes: \"[groups]\"\n\n    # -- Matcher function for Casbin, `glob` for glob matcher and `regex` for regex matcher.\n    policy.matchMode: \"glob\"\n\n  # GnuPG public keys for commit verification\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/user-guide/gpg-verification/\n  gpg:\n    # -- Annotations to be added to argocd-gpg-keys-cm configmap\n    annotations: {}\n\n    # -- [GnuPG] public keys to add to the keyring\n    # @default -- `{}` (See [values.yaml])\n    ## Note: Public keys should be exported with `gpg --export --armor \u003cKEY\u003e`\n    keys: {}\n      # 4AEE18F83AFDEB23: |\n      #   -----BEGIN PGP PUBLIC KEY BLOCK-----\n      #   ...\n      #   -----END PGP PUBLIC KEY BLOCK-----\n\n  # SSH known hosts for Git repositories\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#ssh-known-host-public-keys\n  ssh:\n    # -- Specifies if the argocd-ssh-known-hosts-cm configmap should be created by Helm.\n    create: true\n\n    # -- Annotations to be added to argocd-ssh-known-hosts-cm configmap\n    annotations: {}\n\n    # -- Known hosts to be added to the known host list by default.\n    # @default -- See [values.yaml]\n    knownHosts: |\n      [ssh.github.com]:443 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\n      [ssh.github.com]:443 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\n      [ssh.github.com]:443 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\n      bitbucket.org ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBPIQmuzMBuKdWeF4+a2sjSSpBK0iqitSQ+5BM9KhpexuGt20JpTVM7u5BDZngncgrqDMbWdxMWWOGtZ9UgbqgZE=\n      bitbucket.org ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIazEu89wgQZ4bqs3d63QSMzYVa0MuJ2e2gKTKqu+UUO\n      bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDQeJzhupRu0u0cdegZIa8e86EG2qOCsIsD1Xw0xSeiPDlCr7kq97NLmMbpKTX6Esc30NuoqEEHCuc7yWtwp8dI76EEEB1VqY9QJq6vk+aySyboD5QF61I/1WeTwu+deCbgKMGbUijeXhtfbxSxm6JwGrXrhBdofTsbKRUsrN1WoNgUa8uqN1Vx6WAJw1JHPhglEGGHea6QICwJOAr/6mrui/oB7pkaWKHj3z7d1IC4KWLtY47elvjbaTlkN04Kc/5LFEirorGYVbt15kAUlqGM65pk6ZBxtaO3+30LVlORZkxOh+LKL/BvbZ/iRNhItLqNyieoQj/uh/7Iv4uyH/cV/0b4WDSd3DptigWq84lJubb9t/DnZlrJazxyDCulTmKdOR7vs9gMTo+uoIrPSb8ScTtvw65+odKAlBj59dhnVp9zd7QUojOpXlL62Aw56U4oO+FALuevvMjiWeavKhJqlR7i5n9srYcrNV7ttmDw7kf/97P5zauIhxcjX+xHv4M=\n      github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\n      github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\n      github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\n      gitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=\n      gitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf\n      gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\n      ssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\n      vs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\n\n    # -- Additional known hosts for private repositories\n    extraHosts: ''\n\n  # Repository TLS certificates\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#repositories-using-self-signed-tls-certificates-or-are-signed-by-custom-ca\n  tls:\n    # -- Annotations to be added to argocd-tls-certs-cm configmap\n    annotations: {}\n\n    # -- TLS certificates for Git repositories\n    # @default -- `{}` (See [values.yaml])\n    certificates: {}\n      # server.example.com: |\n      #   -----BEGIN CERTIFICATE-----\n      #   ...\n      #   -----END CERTIFICATE-----\n\n    # -- Specifies if the argocd-tls-certs-cm configmap should be created by Helm.\n    create: true\n\n  # ConfigMap for Config Management Plugins\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/config-management-plugins/\n  cmp:\n    # -- Create the argocd-cmp-cm configmap\n    create: false\n\n    # -- Annotations to be added to argocd-cmp-cm configmap\n    annotations: {}\n\n    # -- Plugin yaml files to be added to argocd-cmp-cm\n    plugins: {}\n      # --- First plugin\n      # my-plugin:\n      #   init:\n      #     command: [sh]\n      #     args: [-c, 'echo \"Initializing...\"']\n      #   generate:\n      #     command: [sh, -c]\n      #     args:\n      #       - |\n      #         echo \"{\\\"kind\\\": \\\"ConfigMap\\\", \\\"apiVersion\\\": \\\"v1\\\", \\\"metadata\\\": { \\\"name\\\": \\\"$ARGOCD_APP_NAME\\\", \\\"namespace\\\": \\\"$ARGOCD_APP_NAMESPACE\\\", \\\"annotations\\\": {\\\"Foo\\\": \\\"$ARGOCD_ENV_FOO\\\", \\\"KubeVersion\\\": \\\"$KUBE_VERSION\\\", \\\"KubeApiVersion\\\": \\\"$KUBE_API_VERSIONS\\\",\\\"Bar\\\": \\\"baz\\\"}}}\"\n      #   discover:\n      #     fileName: \"./subdir/s*.yaml\"\n      #     find:\n      #       glob: \"**/Chart.yaml\"\n      #       command: [sh, -c, find . -name env.yaml]\n\n      # --- Second plugin\n      # my-plugin2:\n      #   init:\n      #     command: [sh]\n      #     args: [-c, 'echo \"Initializing...\"']\n      #   generate:\n      #     command: [sh, -c]\n      #     args:\n      #       - |\n      #         echo \"{\\\"kind\\\": \\\"ConfigMap\\\", \\\"apiVersion\\\": \\\"v1\\\", \\\"metadata\\\": { \\\"name\\\": \\\"$ARGOCD_APP_NAME\\\", \\\"namespace\\\": \\\"$ARGOCD_APP_NAMESPACE\\\", \\\"annotations\\\": {\\\"Foo\\\": \\\"$ARGOCD_ENV_FOO\\\", \\\"KubeVersion\\\": \\\"$KUBE_VERSION\\\", \\\"KubeApiVersion\\\": \\\"$KUBE_API_VERSIONS\\\",\\\"Bar\\\": \\\"baz\\\"}}}\"\n      #   discover:\n      #     fileName: \"./subdir/s*.yaml\"\n      #     find:\n      #       glob: \"**/Chart.yaml\"\n      #       command: [sh, -c, find . -name env.yaml]\n\n  # -- Provide one or multiple [external cluster credentials]\n  # @default -- `{}` (See [values.yaml])\n  ## Ref:\n  ## - https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#clusters\n  ## - https://argo-cd.readthedocs.io/en/stable/operator-manual/security/#external-cluster-credentials\n  ## - https://argo-cd.readthedocs.io/en/stable/user-guide/projects/#project-scoped-repositories-and-clusters\n  clusterCredentials: {}\n    # mycluster:\n    #   server: https://mycluster.example.com\n    #   labels: {}\n    #   annotations: {}\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n    # mycluster2:\n    #   server: https://mycluster2.example.com\n    #   labels: {}\n    #   annotations: {}\n    #   namespaces: namespace1,namespace2\n    #   clusterResources: true\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n    # mycluster3-project-scoped:\n    #   server: https://mycluster3.example.com\n    #   labels: {}\n    #   annotations: {}\n    #   project: my-project1\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n    # mycluster4-sharded:\n    #   shard: 1\n    #   server: https://mycluster4.example.com\n    #   labels: {}\n    #   annotations: {}\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n\n  # -- Repository credentials to be used as Templates for other repos\n  ## Creates a secret for each key/value specified below to create repository credentials\n  credentialTemplates: {}\n    # github-enterprise-creds-1:\n    #   url: https://github.com/argoproj\n    #   githubAppID: 1\n    #   githubAppInstallationID: 2\n    #   githubAppEnterpriseBaseUrl: https://ghe.example.com/api/v3\n    #   githubAppPrivateKey: |\n    #     -----BEGIN OPENSSH PRIVATE KEY-----\n    #     ...\n    #     -----END OPENSSH PRIVATE KEY-----\n    # https-creds:\n    #   url: https://github.com/argoproj\n    #   password: my-password\n    #   username: my-username\n    # ssh-creds:\n    #  url: git@github.com:argoproj-labs\n    #  sshPrivateKey: |\n    #    -----BEGIN OPENSSH PRIVATE KEY-----\n    #    ...\n    #    -----END OPENSSH PRIVATE KEY-----\n\n  # -- Annotations to be added to `configs.credentialTemplates` Secret\n  credentialTemplatesAnnotations: {}\n\n  # -- Repositories list to be used by applications\n  ## Creates a secret for each key/value specified below to create repositories\n  ## Note: the last example in the list would use a repository credential template, configured under \"configs.credentialTemplates\".\n  repositories: {}\n    # istio-helm-repo:\n    #   url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts\n    #   name: istio.io\n    #   type: helm\n    # private-helm-repo:\n    #   url: https://my-private-chart-repo.internal\n    #   name: private-repo\n    #   type: helm\n    #   password: my-password\n    #   username: my-username\n    # private-repo:\n    #   url: https://github.com/argoproj/private-repo\n\n  # -- Annotations to be added to `configs.repositories` Secret\n  repositoriesAnnotations: {}\n\n  # Argo CD sensitive data\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets\n  secret:\n    # -- Create the argocd-secret\n    createSecret: true\n    # -- Labels to be added to argocd-secret\n    labels: {}\n    # -- Annotations to be added to argocd-secret\n    annotations: {}\n\n    # -- Shared secret for authenticating GitHub webhook events\n    githubSecret: \"\"\n    # -- Shared secret for authenticating GitLab webhook events\n    gitlabSecret: \"\"\n    # -- Shared secret for authenticating BitbucketServer webhook events\n    bitbucketServerSecret: \"\"\n    # -- UUID for authenticating Bitbucket webhook events\n    bitbucketUUID: \"\"\n    # -- Shared secret for authenticating Gogs webhook events\n    gogsSecret: \"\"\n    ## Azure DevOps\n    azureDevops:\n      # -- Shared secret username for authenticating Azure DevOps webhook events\n      username: \"\"\n      # -- Shared secret password for authenticating Azure DevOps webhook events\n      password: \"\"\n\n    # -- add additional secrets to be added to argocd-secret\n    ## Custom secrets. Useful for injecting SSO secrets into environment variables.\n    ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets\n    ## Note that all values must be non-empty.\n    extra:\n      {}\n      # LDAP_PASSWORD: \"mypassword\"\n\n    # -- Bcrypt hashed admin password\n    ## Argo expects the password in the secret to be bcrypt hashed. You can create this hash with\n    ## `htpasswd -nbBC 10 \"\" $ARGO_PWD | tr -d ':\\n' | sed 's/$2y/$2a/'`\n    argocdServerAdminPassword: \"\"\n    # -- Admin password modification time. Eg. `\"2006-01-02T15:04:05Z\"`\n    # @default -- `\"\"` (defaults to current time)\n    argocdServerAdminPasswordMtime: \"\"\n\n  # -- Define custom [CSS styles] for your argo instance.\n  # This setting will automatically mount the provided CSS and reference it in the argo configuration.\n  # @default -- `\"\"` (See [values.yaml])\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/custom-styles/\n  styles: \"\"\n  # styles: |\n  #  .sidebar {\n  #    background: linear-gradient(to bottom, #999, #777, #333, #222, #111);\n  #  }\n\n# -- Array of extra K8s manifests to deploy\n## Note: Supports use of custom Helm templates\nextraObjects: []\n  # - apiVersion: secrets-store.csi.x-k8s.io/v1\n  #   kind: SecretProviderClass\n  #   metadata:\n  #     name: argocd-secrets-store\n  #   spec:\n  #     provider: aws\n  #     parameters:\n  #       objects: |\n  #         - objectName: \"argocd\"\n  #           objectType: \"secretsmanager\"\n  #           jmesPath:\n  #               - path: \"client_id\"\n  #                 objectAlias: \"client_id\"\n  #               - path: \"client_secret\"\n  #                 objectAlias: \"client_secret\"\n  #     secretObjects:\n  #     - data:\n  #       - key: client_id\n  #         objectName: client_id\n  #       - key: client_secret\n  #         objectName: client_secret\n  #       secretName: argocd-secrets-store\n  #       type: Opaque\n  #       labels:\n  #         app.kubernetes.io/part-of: argocd\n\n## Application controller\ncontroller:\n  # -- Application controller name string\n  name: application-controller\n\n  # -- The number of application controller pods to run.\n  # Additional replicas will cause sharding of managed clusters across number of replicas.\n  ## With dynamic cluster distribution turned on, sharding of the clusters will gracefully\n  ## rebalance if the number of replica's changes or one becomes unhealthy. (alpha)\n  replicas: 1\n\n  # -- Enable dynamic cluster distribution (alpha)\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/dynamic-cluster-distribution\n  ## This is done using a deployment instead of a statefulSet\n  ## When replicas are added or removed, the sharding algorithm is re-run to ensure that the\n  ## clusters are distributed according to the algorithm. If the algorithm is well-balanced,\n  ## like round-robin, then the shards will be well-balanced.\n  dynamicClusterDistribution: false\n\n  # -- Runtime class name for the application controller\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  # -- Application controller heartbeat time\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/dynamic-cluster-distribution/#working-of-dynamic-distribution\n  heartbeatTime: 10\n\n  # -- Maximum number of controller revisions that will be maintained in StatefulSet history\n  revisionHistoryLimit: 5\n\n  ## Application controller Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the application controller\n    enabled: false\n    # -- Labels to be added to application controller pdb\n    labels: {}\n    # -- Annotations to be added to application controller pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `controller.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Application controller Vertical Pod Autoscaler\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/#scaling-workloads-vertically/\n  vpa:\n    # -- Deploy a [VerticalPodAutoscaler](https://kubernetes.io/docs/concepts/workloads/autoscaling/#scaling-workloads-vertically/) for the application controller\n    enabled: false\n    # -- Labels to be added to application controller vpa\n    labels: {}\n    # -- Annotations to be added to application controller vpa\n    annotations: {}\n    # -- One of the VPA operation modes\n    ## Ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/#scaling-workloads-vertically\n    ## Note: Recreate update mode requires more than one replica unless the min-replicas VPA controller flag is overridden\n    updateMode: Initial\n    # -- Controls how VPA computes the recommended resources for application controller container\n    ## Ref: https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/examples/hamster.yaml\n    containerPolicy: {}\n      # controlledResources: [\"cpu\", \"memory\"]\n      # minAllowed:\n      #   cpu: 250m\n      #   memory: 256Mi\n      # maxAllowed:\n      #   cpu: 1\n      #   memory: 1Gi\n\n\n  ## Application controller image\n  image:\n    # -- Repository to use for the application controller\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the application controller\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the application controller\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Additional command line arguments to pass to application controller\n  extraArgs: []\n\n  # -- Environment variables to pass to application controller\n  env: []\n\n  # -- envFrom to pass to application controller\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Additional containers to be added to the application controller pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the application controller pod\n  ## If your target Kubernetes cluster(s) require a custom credential (exec) plugin\n  ## you could use this (and the same in the server pod) to provide such executable\n  ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO /custom-tools/kubelogin.zip https://github.com/Azure/kubelogin/releases/download/v0.2.7/kubelogin-linux-amd64.zip \u0026\u0026\n  #        mkdir /custom-tools/tmp \u0026\u0026 unzip -d /custom-tools/tmp /custom-tools/kubelogin.zip  \u0026\u0026\n  #        mv /custom-tools/tmp/bin/linux_amd64/kubelogin /custom-tools/ \u0026\u0026 rm -rf custom-tools/tmp \u0026\u0026 rm /custom-tools/kubelogin.zip\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n\n  # -- Additional volumeMounts to the application controller main container\n  volumeMounts: []\n  #  - mountPath: /usr/local/bin/kubelogin\n  #    name: custom-tools\n  #    subPath: kubelogin\n\n  # -- Additional volumes to the application controller pod\n  volumes: []\n  #  - name: custom-tools\n  #    emptyDir: {}\n\n  ## Application controller emptyDir volumes\n  emptyDir:\n    # -- EmptyDir size limit for application controller\n    # @default -- `\"\"` (defaults not set if not specified i.e. no size limit)\n    sizeLimit: \"\"\n    # sizeLimit: \"1Gi\"\n\n  # -- Annotations for the application controller StatefulSet\n  statefulsetAnnotations: {}\n\n  # -- Annotations for the application controller Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the application controller Deployment\n  deploymentLabels: {}\n\n  # -- Annotations to be added to application controller pods\n  podAnnotations: {}\n\n  # -- Labels to be added to application controller pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the application controller pods\n  resources: {}\n  #  limits:\n  #    cpu: 500m\n  #    memory: 512Mi\n  #  requests:\n  #    cpu: 250m\n  #    memory: 256Mi\n\n  # Application controller container ports\n  containerPorts:\n    # -- Metrics container port\n    metrics: 8082\n\n  # -- Host Network for application controller pods\n  hostNetwork: false\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for application controller pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Application controller container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  # Readiness probe for application controller\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- Priority class for the application controller pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the application controller\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: true\n\n  serviceAccount:\n    # -- Create a service account for the application controller\n    create: true\n    # -- Service account name\n    name: argocd-application-controller\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  ## Application controller metrics configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    # -- Prometheus ServiceMonitor scrapeTimeout. If empty, Prometheus uses the global scrape timeout unless it is less than the target's scrape interval value in which the latter is used.\n    scrapeTimeout: \"\"\n    applicationLabels:\n      # -- Enables additional labels in argocd_app_labels metric\n      enabled: false\n      # -- Additional labels\n      labels: []\n    service:\n      # -- Metrics service type\n      type: NodePort\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8082\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- When true, honorLabels preserves the metrics labels when they collide with the targets labels.\n      honorLabels: false\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n    rules:\n      # -- Deploy a PrometheusRule for the application controller\n      enabled: false\n      # -- PrometheusRule namespace\n      namespace: \"\" # \"monitoring\"\n      # -- PrometheusRule selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- PrometheusRule labels\n      additionalLabels: {}\n      # -- PrometheusRule annotations\n      annotations: {}\n\n      # -- PrometheusRule.Spec for the application controller\n      spec: []\n      # - alert: ArgoAppMissing\n      #   expr: |\n      #     absent(argocd_app_info) == 1\n      #   for: 15m\n      #   labels:\n      #     severity: critical\n      #   annotations:\n      #     summary: \"[Argo CD] No reported applications\"\n      #     description: \u003e\n      #       Argo CD has not reported any applications data for the past 15 minutes which\n      #       means that it must be down or not functioning properly.  This needs to be\n      #       resolved for this cloud to continue to maintain state.\n      # - alert: ArgoAppNotSynced\n      #   expr: |\n      #     argocd_app_info{sync_status!=\"Synced\"} == 1\n      #   for: 12h\n      #   labels:\n      #     severity: warning\n      #   annotations:\n      #     summary: \"[{{`{{$labels.name}}`}}] Application not synchronized\"\n      #     description: \u003e\n      #       The application [{{`{{$labels.name}}`}} has not been synchronized for over\n      #       12 hours which means that the state of this cloud has drifted away from the\n      #       state inside Git.\n\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- Enable custom rules for the application controller's ClusterRole resource\n    enabled: false\n    # -- List of custom rules for the application controller's ClusterRole resource\n    rules: []\n\n  ## Enable this and set the rules: to whatever custom rules you want for the Role resource.\n  ## Defaults to off\n  # -- List of custom rules for the application controller's Role resource\n  roleRules: []\n\n  # Default application controller's network policy\n  networkPolicy:\n    # -- Default network policy rules used by application controller\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n\n## Dex\ndex:\n  # -- Enable dex\n  enabled: false\n  # -- Dex name\n  name: dex-server\n\n  # -- Additional command line arguments to pass to the Dex server\n  extraArgs: []\n\n  # -- Runtime class name for Dex\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- When true, honorLabels preserves the metrics labels when they collide with the targets labels.\n      honorLabels: false\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  ## Dex Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the Dex server\n    enabled: false\n    # -- Labels to be added to Dex server pdb\n    labels: {}\n    # -- Annotations to be added to Dex server pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailble after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `dex.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Dex image\n  image:\n    # -- Dex image repository\n    repository: ghcr.io/dexidp/dex\n    # -- Dex image tag\n    tag: v2.44.0\n    # -- Dex imagePullPolicy\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # Argo CD init image that creates Dex config\n  initImage:\n    # -- Argo CD init image repository\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Argo CD init image tag\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Argo CD init image imagePullPolicy\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n    # -- Argo CD init image resources\n    # @default -- `{}` (defaults to dex.resources)\n    resources: {}\n    #  requests:\n    #    cpu: 5m\n    #    memory: 96Mi\n    #  limits:\n    #    cpu: 10m\n    #    memory: 144Mi\n\n  # -- Environment variables to pass to the Dex server\n  env: []\n\n  # -- envFrom to pass to the Dex server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Additional containers to be added to the dex pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the dex pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- Additional volumeMounts to the dex main container\n  volumeMounts: []\n\n  # -- Additional volumes to the dex pod\n  volumes: []\n\n  ## Dex server emptyDir volumes\n  emptyDir:\n    # -- EmptyDir size limit for Dex server\n    # @default -- `\"\"` (defaults not set if not specified i.e. no size limit)\n    sizeLimit: \"\"\n    # sizeLimit: \"1Gi\"\n\n  # TLS certificate configuration via Secret\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#configuring-tls-to-argocd-dex-server\n  ## Note: Issuing certificates via cert-manager in not supported right now because it's not possible to restart Dex automatically without extra controllers.\n  certificateSecret:\n    # -- Create argocd-dex-server-tls secret\n    enabled: false\n    # -- Labels to be added to argocd-dex-server-tls secret\n    labels: {}\n    # -- Annotations to be added to argocd-dex-server-tls secret\n    annotations: {}\n    # -- Certificate authority. Required for self-signed certificates.\n    ca: ''\n    # -- Certificate private key\n    key: ''\n    # -- Certificate data. Must contain SANs of Dex service (ie: argocd-dex-server, argocd-dex-server.argo-cd.svc)\n    crt: ''\n\n  # -- Annotations to be added to the Dex server Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the Dex server Deployment\n  deploymentLabels: {}\n\n  # -- Annotations to be added to the Dex server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to the Dex server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for dex\n  resources: {}\n  #  limits:\n  #    cpu: 50m\n  #    memory: 64Mi\n  #  requests:\n  #    cpu: 10m\n  #    memory: 32Mi\n\n  # Dex container ports\n  # NOTE: These ports are currently hardcoded and cannot be changed\n  containerPorts:\n    # -- HTTP container port\n    http: 5556\n    # -- gRPC container port\n    grpc: 5557\n    # -- Metrics container port\n    metrics: 5558\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Dex server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Dex container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Probes for Dex server\n  ## Supported from Dex \u003e= 2.28.0\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for Dex \u003e= 2.28.0\n    enabled: false\n    # -- Http path to use for the liveness probe\n    httpPath: /healthz/live\n    # -- Http port to use for the liveness probe\n    httpPort: metrics\n    # -- Scheme to use for for the liveness probe (can be HTTP or HTTPS)\n    httpScheme: HTTP\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  readinessProbe:\n    # -- Enable Kubernetes readiness probe for Dex \u003e= 2.28.0\n    enabled: false\n    # -- Http path to use for the readiness probe\n    httpPath: /healthz/ready\n    # -- Http port to use for the readiness probe\n    httpPort: metrics\n    # -- Scheme to use for for the liveness probe (can be HTTP or HTTPS)\n    httpScheme: HTTP\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: true\n\n  serviceAccount:\n    # -- Create dex service account\n    create: true\n    # -- Dex service account name\n    name: argocd-dex-server\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Service port for HTTP access\n  servicePortHttp: 5556\n  # -- Service port name for HTTP access\n  servicePortHttpName: http\n  # -- Service port for gRPC access\n  servicePortGrpc: 5557\n  # -- Service port name for gRPC access\n  servicePortGrpcName: grpc\n  # -- Service port for metrics access\n  servicePortMetrics: 5558\n\n  # -- Priority class for the dex pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to dex\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the Dex server Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # Default Dex server's network policy\n  networkPolicy:\n    # -- Default network policy rules used by Dex server\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n\n  # DEPRECATED - Use configs.params to override\n  # -- Dex log format. Either `text` or `json`\n  # @default -- `\"\"` (defaults to global.logging.format)\n  # logFormat: \"\"\n  # -- Dex log level. One of: `debug`, `info`, `warn`, `error`\n  # @default -- `\"\"` (defaults to global.logging.level)\n  # logLevel: \"\"\n\n## Redis\nredis:\n  # -- Enable redis\n  enabled: true\n  # -- Redis name\n  name: redis\n\n  # -- Runtime class name for redis\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  ## Redis Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the Redis\n    enabled: false\n    # -- Labels to be added to Redis pdb\n    labels: {}\n    # -- Annotations to be added to Redis pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailble after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `redis.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Redis image\n  image:\n    # -- Redis repository\n    repository: ecr-public.aws.com/docker/library/redis\n    # -- Redis tag\n    ## Do not upgrade to \u003e= 7.4.0, otherwise you are no longer using an open source version of Redis\n    tag: 7.2.11-alpine\n    # -- Redis image pull policy\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  ## Prometheus redis-exporter sidecar\n  exporter:\n    # -- Enable Prometheus redis-exporter sidecar\n    enabled: false\n    # -- Environment variables to pass to the Redis exporter\n    env: []\n    ## Prometheus redis-exporter image\n    image:\n      # -- Repository to use for the redis-exporter\n      repository: ghcr.io/oliver006/redis_exporter\n      # -- Tag to use for the redis-exporter\n      tag: v1.79.0\n      # -- Image pull policy for the redis-exporter\n      # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n      imagePullPolicy: \"\"\n\n    # -- Redis exporter security context\n    # @default -- See [values.yaml]\n    containerSecurityContext:\n      runAsNonRoot: true\n      readOnlyRootFilesystem: true\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: RuntimeDefault\n      capabilities:\n        drop:\n        - ALL\n\n    ## Probes for Redis exporter (optional)\n    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n    readinessProbe:\n      # -- Enable Kubernetes liveness probe for Redis exporter (optional)\n      enabled: false\n      # -- Number of seconds after the container has started before [probe] is initiated\n      initialDelaySeconds: 30\n      # -- How often (in seconds) to perform the [probe]\n      periodSeconds: 15\n      # -- Number of seconds after which the [probe] times out\n      timeoutSeconds: 15\n      # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n      successThreshold: 1\n      # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n      failureThreshold: 5\n    livenessProbe:\n      # -- Enable Kubernetes liveness probe for Redis exporter\n      enabled: false\n      # -- Number of seconds after the container has started before [probe] is initiated\n      initialDelaySeconds: 30\n      # -- How often (in seconds) to perform the [probe]\n      periodSeconds: 15\n      # -- Number of seconds after which the [probe] times out\n      timeoutSeconds: 15\n      # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n      successThreshold: 1\n      # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n      failureThreshold: 5\n\n    # -- Resource limits and requests for redis-exporter sidecar\n    resources: {}\n      # limits:\n      #   cpu: 50m\n      #   memory: 64Mi\n      # requests:\n      #   cpu: 10m\n      #   memory: 32Mi\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Additional command line arguments to pass to redis-server\n  extraArgs: []\n  # - --bind\n  # - \"0.0.0.0\"\n\n  # -- Environment variables to pass to the Redis server\n  env: []\n\n  # -- envFrom to pass to the Redis server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  ## Probes for Redis server (optional)\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  readinessProbe:\n    # -- Enable Kubernetes liveness probe for Redis server\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 30\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 15\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 15\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 5\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for Redis server\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 30\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 15\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 15\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 5\n\n  # -- Additional containers to be added to the redis pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the redis pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- Additional volumeMounts to the redis container\n  volumeMounts: []\n\n  # -- Additional volumes to the redis pod\n  volumes: []\n\n  # -- Annotations to be added to the Redis server Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the Redis server Deployment\n  deploymentLabels: {}\n\n  # -- Annotations to be added to the Redis server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to the Redis server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for redis\n  resources: {}\n  #  limits:\n  #    cpu: 200m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 100m\n  #    memory: 64Mi\n\n  # -- Redis pod-level security context\n  # @default -- See [values.yaml]\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 999\n    seccompProfile:\n      type: RuntimeDefault\n\n  # Redis container ports\n  containerPorts:\n    # -- Redis container port\n    redis: 6379\n    # -- Metrics container port\n    metrics: 9121\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Redis server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Redis container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop:\n      - ALL\n\n  # -- Redis service port\n  servicePort: 6379\n\n  # -- Priority class for redis pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to redis\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: true\n\n  serviceAccount:\n    # -- Create a service account for the redis pod\n    create: false\n    # -- Service account name for redis pod\n    name: \"\"\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: false\n\n  service:\n    # -- Redis service annotations\n    annotations: {}\n    # -- Additional redis service labels\n    labels: {}\n\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n\n    # Redis metrics service configuration\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: None\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 9121\n      # -- Metrics service port name\n      portName: http-metrics\n\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Interval at which metrics should be scraped\n      interval: 30s\n      # -- When true, honorLabels preserves the metrics labels when they collide with the targets labels.\n      honorLabels: false\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  # Default redis's network policy\n  networkPolicy:\n    # -- Default network policy rules used by redis\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n\n## Redis-HA subchart replaces custom redis deployment when `redis-ha.enabled=true`\n# Ref: https://github.com/DandyDeveloper/charts/blob/master/charts/redis-ha/values.yaml\nredis-ha:\n  # -- Enables the Redis HA subchart and disables the custom Redis single node deployment\n  enabled: false\n  ## Redis image\n  image:\n    # -- Redis repository\n    repository: ecr-public.aws.com/docker/library/redis\n    # -- Redis tag\n    ## Do not upgrade to \u003e= 7.4.0, otherwise you are no longer using an open source version of Redis\n    tag: 7.2.11-alpine\n  ## Prometheus redis-exporter sidecar\n  exporter:\n    # -- Enable Prometheus redis-exporter sidecar\n    enabled: false\n    # -- Repository to use for the redis-exporter\n    image: ghcr.io/oliver006/redis_exporter\n    # -- Tag to use for the redis-exporter\n    tag: v1.75.0\n  persistentVolume:\n    # -- Configures persistence on Redis nodes\n    enabled: false\n  ## Redis specific configuration options\n  redis:\n    # -- Redis convention for naming the cluster group: must match `^[\\\\w-\\\\.]+$` and can be templated\n    masterGroupName: argocd\n    # -- Any valid redis config options in this section will be applied to each server (see `redis-ha` chart)\n    # @default -- See [values.yaml]\n    config:\n      # -- Will save the DB if both the given number of seconds and the given number of write operations against the DB occurred. `\"\"`  is disabled\n      # @default -- `'\"\"'`\n      save: '\"\"'\n  ## Enables a HA Proxy for better LoadBalancing / Sentinel Master support. Automatically proxies to Redis master.\n  haproxy:\n    # -- Enabled HAProxy LoadBalancing/Proxy\n    enabled: true\n    # --  Custom labels for the haproxy pod. This is relevant for Argo CD CLI.\n    labels:\n      app.kubernetes.io/name: argocd-redis-ha-haproxy\n    image:\n      # -- HAProxy Image Repository\n      repository: ecr-public.aws.com/docker/library/haproxy\n    metrics:\n      # -- HAProxy enable prometheus metric scraping\n      enabled: true\n    # -- Whether the haproxy pods should be forced to run on separate nodes.\n    hardAntiAffinity: true\n    # -- Additional affinities to add to the haproxy pods.\n    additionalAffinities: {}\n    # -- Assign custom [affinity] rules to the haproxy pods.\n    affinity: |\n\n    # -- [Tolerations] for use with node taints for haproxy pods.\n    tolerations: []\n    # -- HAProxy container-level security context\n    # @default -- See [values.yaml]\n    containerSecurityContext:\n      readOnlyRootFilesystem: true\n\n  # -- Configures redis-ha with AUTH\n  auth: true\n  # -- Existing Secret to use for redis-ha authentication.\n  # By default the redis-secret-init Job is generating this Secret.\n  existingSecret: argocd-redis\n\n  # -- Whether the Redis server pods should be forced to run on separate nodes.\n  hardAntiAffinity: true\n\n  # -- Additional affinities to add to the Redis server pods.\n  additionalAffinities: {}\n\n  # -- Assign custom [affinity] rules to the Redis pods.\n  affinity: |\n\n  # -- [Tolerations] for use with node taints for Redis pods.\n  tolerations: []\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the Redis pods.\n  ## https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  topologySpreadConstraints:\n    # -- Enable Redis HA topology spread constraints\n    enabled: false\n    # -- Max skew of pods tolerated\n    # @default -- `\"\"` (defaults to `1`)\n    maxSkew: \"\"\n    # -- Topology key for spread\n    # @default -- `\"\"` (defaults to `topology.kubernetes.io/zone`)\n    topologyKey: \"\"\n    # -- Enforcement policy, hard or soft\n    # @default -- `\"\"` (defaults to `ScheduleAnyway`)\n    whenUnsatisfiable: \"\"\n  # -- Redis HA statefulset container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n\n# External Redis parameters\nexternalRedis:\n  # -- External Redis server host\n  host: \"\"\n  # -- External Redis username\n  username: \"\"\n  # -- External Redis password\n  password: \"\"\n  # -- External Redis server port\n  port: 6379\n  # -- The name of an existing secret with Redis (must contain key `redis-password`. And should contain `redis-username` if username is not `default`) and Sentinel credentials.\n  # When it's set, the `externalRedis.username` and `externalRedis.password` parameters are ignored\n  existingSecret: \"\"\n  # -- External Redis Secret annotations\n  secretAnnotations: {}\n\nredisSecretInit:\n  # -- Enable Redis secret initialization. If disabled, secret must be provisioned by alternative methods\n  enabled: true\n  # -- Redis secret-init name\n  name: redis-secret-init\n\n  image:\n    # -- Repository to use for the Redis secret-init Job\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\" # defaults to global.image.repository\n    # -- Tag to use for the Redis secret-init Job\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\" # defaults to global.image.tag\n    # -- Image pull policy for the Redis secret-init Job\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\" # IfNotPresent\n\n  # -- Additional command line arguments for the Redis secret-init Job\n  extraArgs: []\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Runtime class name for the Redis secret-init Job\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  # -- Annotations to be added to the Redis secret-init Job\n  jobAnnotations: {}\n\n  # -- Annotations to be added to the Redis secret-init Job\n  podAnnotations: {}\n\n  # -- Labels to be added to the Redis secret-init Job\n  podLabels: {}\n\n  # -- Resource limits and requests for Redis secret-init Job\n  resources: {}\n  #  limits:\n  #    cpu: 200m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 100m\n  #    memory: 64Mi\n\n  # -- Application controller container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop:\n        - ALL\n    readOnlyRootFilesystem: true\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n\n  # -- Redis secret-init Job pod-level security context\n  securityContext: {}\n\n  serviceAccount:\n    # -- Create a service account for the redis pod\n    create: true\n    # -- Service account name for redis pod\n    name: \"\"\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Priority class for Redis secret-init Job\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- Assign custom [affinity] rules to the Redis secret-init Job\n  affinity: {}\n\n  # -- Node selector to be added to the Redis secret-init Job\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- Tolerations to be added to the Redis secret-init Job\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n## Server\nserver:\n  # -- Argo CD server name\n  name: server\n\n  # -- The number of server pods to run\n  replicas: 1\n\n  # -- Runtime class name for the Argo CD server\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  ## Argo CD server Horizontal Pod Autoscaler\n  autoscaling:\n    # -- Enable Horizontal Pod Autoscaler ([HPA]) for the Argo CD server\n    enabled: false\n    # -- Minimum number of replicas for the Argo CD server [HPA]\n    minReplicas: 1\n    # -- Maximum number of replicas for the Argo CD server [HPA]\n    maxReplicas: 5\n    # -- Average CPU utilization percentage for the Argo CD server [HPA]\n    targetCPUUtilizationPercentage: 50\n    # -- Average memory utilization percentage for the Argo CD server [HPA]\n    targetMemoryUtilizationPercentage: 50\n    # -- Configures the scaling behavior of the target in both Up and Down directions.\n    behavior: {}\n      # scaleDown:\n      #  stabilizationWindowSeconds: 300\n      #  policies:\n      #   - type: Pods\n      #     value: 1\n      #     periodSeconds: 180\n      # scaleUp:\n      #   stabilizationWindowSeconds: 300\n      #   policies:\n      #   - type: Pods\n      #     value: 2\n      #     periodSeconds: 60\n    # -- Configures custom HPA metrics for the Argo CD server\n    # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\n    metrics: []\n\n  ## Argo CD server Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the Argo CD server\n    enabled: false\n    # -- Labels to be added to Argo CD server pdb\n    labels: {}\n    # -- Annotations to be added to Argo CD server pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `server.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Argo CD server image\n  image:\n    # -- Repository to use for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\" # defaults to global.image.repository\n    # -- Tag to use for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\" # defaults to global.image.tag\n    # -- Image pull policy for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\" # IfNotPresent\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Additional command line arguments to pass to Argo CD server\n  extraArgs: []\n\n  # -- Environment variables to pass to Argo CD server\n  env: []\n\n  # -- envFrom to pass to Argo CD server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Specify postStart and preStop lifecycle hooks for your argo-cd-server container\n  lifecycle: {}\n\n  ## Argo CD extensions\n  ## This function in tech preview stage, do expect instability or breaking changes in newer versions.\n  ## Ref: https://github.com/argoproj-labs/argocd-extension-installer\n  ## When you enable extensions, you need to configure RBAC of logged in Argo CD user.\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/rbac/#the-extensions-resource\n  extensions:\n    # -- Enable support for Argo CD extensions\n    enabled: false\n\n    ## Argo CD extension installer image\n    image:\n      # -- Repository to use for extension installer image\n      repository: \"quay.io/argoprojlabs/argocd-extension-installer\"\n      # -- Tag to use for extension installer image\n      tag: \"v0.0.8\"\n      # -- Image pull policy for extensions\n      # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n      imagePullPolicy: \"\"\n\n    # -- Extensions for Argo CD\n    # @default -- `[]` (See [values.yaml])\n    ## Ref: https://github.com/argoproj-labs/argocd-extension-metrics#install-ui-extension\n    extensionList: []\n    #  - name: extension-metrics\n    #    env:\n    #      - name: EXTENSION_URL\n    #        value: https://github.com/argoproj-labs/argocd-extension-metrics/releases/download/v1.0.0/extension.tar.gz\n    #      - name: EXTENSION_CHECKSUM_URL\n    #        value: https://github.com/argoproj-labs/argocd-extension-metrics/releases/download/v1.0.0/extension_checksums.txt\n\n    # -- Server UI extensions container-level security context\n    # @default -- See [values.yaml]\n    containerSecurityContext:\n      runAsNonRoot: true\n      readOnlyRootFilesystem: true\n      allowPrivilegeEscalation: false\n      runAsUser: 1000\n      seccompProfile:\n        type: RuntimeDefault\n      capabilities:\n        drop:\n        - ALL\n\n    # -- Resource limits and requests for the argocd-extensions container\n    resources: {}\n    #  limits:\n    #    cpu: 50m\n    #    memory: 128Mi\n    #  requests:\n    #    cpu: 10m\n    #    memory: 64Mi\n\n  # -- Additional containers to be added to the server pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n  # - name: my-sidecar\n  #   image: nginx:latest\n  # - name: lemonldap-ng-controller\n  #   image: lemonldapng/lemonldap-ng-controller:0.2.0\n  #   args:\n  #     - /lemonldap-ng-controller\n  #     - --alsologtostderr\n  #     - --configmap=$(POD_NAMESPACE)/lemonldap-ng-configuration\n  #   env:\n  #     - name: POD_NAME\n  #       valueFrom:\n  #         fieldRef:\n  #           fieldPath: metadata.name\n  #     - name: POD_NAMESPACE\n  #       valueFrom:\n  #         fieldRef:\n  #           fieldPath: metadata.namespace\n  #   volumeMounts:\n  #   - name: copy-portal-skins\n  #     mountPath: /srv/var/lib/lemonldap-ng/portal/skins\n\n  # -- Init containers to add to the server pod\n  ## If your target Kubernetes cluster(s) require a custom credential (exec) plugin\n  ## you could use this (and the same in the application controller pod) to provide such executable\n  ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO /custom-tools/kubelogin.zip https://github.com/Azure/kubelogin/releases/download/v0.2.7/kubelogin-linux-amd64.zip \u0026\u0026\n  #        mkdir /custom-tools/tmp \u0026\u0026 unzip -d /custom-tools/tmp /custom-tools/kubelogin.zip  \u0026\u0026\n  #        mv /custom-tools/tmp/bin/linux_amd64/kubelogin /custom-tools/ \u0026\u0026 rm -rf custom-tools/tmp \u0026\u0026 rm /custom-tools/kubelogin.zip\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n\n  # -- Additional volumeMounts to the server main container\n  volumeMounts: []\n  #  - mountPath: /usr/local/bin/kubelogin\n  #    name: custom-tools\n  #    subPath: kubelogin\n\n  # -- Additional volumes to the server pod\n  volumes: []\n  #  - name: custom-tools\n  #    emptyDir: {}\n\n  ## Argo CD server emptyDir volumes\n  emptyDir:\n    # -- EmptyDir size limit for the Argo CD server\n    # @default -- `\"\"` (defaults not set if not specified i.e. no size limit)\n    sizeLimit: \"\"\n    # sizeLimit: \"1Gi\"\n\n  # -- Annotations to be added to server Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the server Deployment\n  deploymentLabels: {}\n\n  # -- Annotations to be added to server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the Argo CD server\n  resources: {}\n  #  limits:\n  #    cpu: 100m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 50m\n  #    memory: 64Mi\n\n  # Server container ports\n  containerPorts:\n    # -- Server container port\n    server: 8080\n    # -- Metrics container port\n    metrics: 8083\n\n  # -- Host Network for Server pods\n  hostNetwork: false\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Server container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  livenessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- Priority class for the Argo CD server pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the Argo CD server\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the server Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # TLS certificate configuration via cert-manager\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#tls-certificates-used-by-argocd-server\n  certificate:\n    # -- Deploy a Certificate resource (requires cert-manager)\n    enabled: false\n    # -- Certificate primary domain (commonName)\n    # @default -- `\"\"` (defaults to global.domain)\n    domain: \"\"\n    # -- Certificate Subject Alternate Names (SANs)\n    additionalHosts: []\n    # -- The requested 'duration' (i.e. lifetime) of the certificate.\n    # @default -- `\"\"` (defaults to 2160h = 90d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    duration: \"\"\n    # -- How long before the expiry a certificate should be renewed.\n    # @default -- `\"\"` (defaults to 360h = 15d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    renewBefore: \"\"\n    # Certificate issuer\n    ## Ref: https://cert-manager.io/docs/concepts/issuer\n    issuer:\n      # -- Certificate issuer group. Set if using an external issuer. Eg. `cert-manager.io`\n      group: \"\"\n      # -- Certificate issuer kind. Either `Issuer` or `ClusterIssuer`\n      kind: \"\"\n      # -- Certificate issuer name. Eg. `letsencrypt`\n      name: \"\"\n    # Private key of the certificate\n    privateKey:\n      # -- Rotation policy of private key when certificate is re-issued. Either: `Never` or `Always`\n      rotationPolicy: Never\n      # -- The private key cryptography standards (PKCS) encoding for private key. Either: `PCKS1` or `PKCS8`\n      encoding: PKCS1\n      # -- Algorithm used to generate certificate private key. One of: `RSA`, `Ed25519` or `ECDSA`\n      algorithm: RSA\n      # -- Key bit size of the private key. If algorithm is set to `Ed25519`, size is ignored.\n      size: 2048\n    # -- Annotations to be applied to the Server Certificate\n    annotations: {}\n    # -- Usages for the certificate\n    ### Ref: https://cert-manager.io/docs/reference/api-docs/#cert-manager.io/v1.KeyUsage\n    usages: []\n    # -- Annotations that allow the certificate to be composed from data residing in existing Kubernetes Resources\n    secretTemplateAnnotations: {}\n\n  # TLS certificate configuration via Secret\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#tls-certificates-used-by-argocd-server\n  certificateSecret:\n    # -- Create argocd-server-tls secret\n    enabled: false\n    # -- Annotations to be added to argocd-server-tls secret\n    annotations: {}\n    # -- Labels to be added to argocd-server-tls secret\n    labels: {}\n    # -- Private Key of the certificate\n    key: ''\n    # -- Certificate data\n    crt: ''\n\n  ## Server service configuration\n  service:\n    # -- Server service annotations\n    annotations: {}\n    # -- Server service labels\n    labels: {}\n    # -- Server service type\n    type: NodePort\n    # -- Server service http port for NodePort service type (only if `server.service.type` is set to \"NodePort\")\n    nodePortHttp: 30080\n    # -- Server service https port for NodePort service type (only if `server.service.type` is set to \"NodePort\")\n    nodePortHttps: 30443\n    # -- Server service http port\n    servicePortHttp: 80\n    # -- Server service https port\n    servicePortHttps: 443\n    # -- Server service http port name, can be used to route traffic via istio\n    servicePortHttpName: http\n    # -- Server service https port name, can be used to route traffic via istio\n    servicePortHttpsName: https\n    # -- Server service https port appProtocol\n    ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#application-protocol\n    servicePortHttpsAppProtocol: \"\"\n    # -- The class of the load balancer implementation\n    loadBalancerClass: \"\"\n    # -- LoadBalancer will get created with the IP specified in this field\n    loadBalancerIP: \"\"\n    # -- Source IP ranges to allow access to service from\n    ## EKS Ref: https://repost.aws/knowledge-center/eks-cidr-ip-address-loadbalancer\n    ## GKE Ref: https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview#limit-connectivity-ext-lb\n    loadBalancerSourceRanges: []\n    # -- Server service external IPs\n    externalIPs: []\n    # -- Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ## Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    externalTrafficPolicy: Cluster\n    # -- Used to maintain session affinity. Supports `ClientIP` and `None`\n    ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n    sessionAffinity: None\n\n  ## Server metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8083\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus ServiceMonitor scrapeTimeout. If empty, Prometheus uses the global scrape timeout unless it is less than the target's scrape interval value in which the latter is used.\n      scrapeTimeout: \"\"\n      # -- When true, honorLabels preserves the metrics labels when they collide with the targets labels.\n      honorLabels: false\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\"  # monitoring\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: true\n\n  serviceAccount:\n    # -- Create server service account\n    create: true\n    # -- Server service account name\n    name: argocd-server\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # Argo CD server ingress configuration\n  ingress:\n    # -- Enable an ingress resource for the Argo CD server\n    enabled: false\n    # -- Specific implementation for ingress controller. One of `generic`, `aws` or `gke`\n    ## Additional configuration might be required in related configuration sections\n    controller: generic\n    # -- Additional ingress labels\n    labels: {}\n    # -- Additional ingress annotations\n    ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough\n    annotations: {}\n      # nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n      # nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n\n    # -- Defines which ingress controller will implement the resource\n    ingressClassName: \"\"\n\n    # -- Argo CD server hostname\n    # @default -- `\"\"` (defaults to global.domain)\n    hostname: \"\"\n\n    # -- The path to Argo CD server\n    path: /\n\n    # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`\n    pathType: Prefix\n\n    # -- Enable TLS configuration for the hostname defined at `server.ingress.hostname`\n    ## TLS certificate will be retrieved from a TLS secret `argocd-server-tls`\n    ## You can create this secret via `certificate` or `certificateSecret` option\n    tls: false\n\n    # -- The list of additional hostnames to be covered by ingress record\n    # @default -- `[]` (See [values.yaml])\n    extraHosts: []\n      # - name: argocd.example.com\n      #   path: /\n\n    # -- Additional ingress paths\n    # @default -- `[]` (See [values.yaml])\n    ## Note: Supports use of custom Helm templates\n    extraPaths: []\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service:\n      #       name: ssl-redirect\n      #       port:\n      #         name: use-annotation\n\n    # -- Additional ingress rules\n    # @default -- `[]` (See [values.yaml])\n    ## Note: Supports use of custom Helm templates\n    extraRules: []\n      # - http:\n      #     paths:\n      #     - path: /\n      #       pathType: Prefix\n      #       backend:\n      #         service:\n      #           name: '{{ include \"argo-cd.server.fullname\" . }}'\n      #           port:\n      #             name: '{{ .Values.server.service.servicePortHttpsName }}'\n\n    # -- Additional TLS configuration\n    # @default -- `[]` (See [values.yaml])\n    extraTls: []\n      # - hosts:\n      #   - argocd.example.com\n      #   secretName: your-certificate-name\n\n    # AWS specific options for Application Load Balancer\n    # Applies only when `serv.ingress.controller` is set to `aws`\n    ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#aws-application-load-balancers-albs-and-classic-elb-http-mode\n    aws:\n      # -- Backend protocol version for the AWS ALB gRPC service\n      ## This tells AWS to send traffic from the ALB using gRPC.\n      ## For more information: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html#health-check-settings\n      backendProtocolVersion: GRPC\n      # -- Service type for the AWS ALB gRPC service\n      ## Can be of type NodePort or ClusterIP depending on which mode you are running.\n      ## Instance mode needs type NodePort, IP mode needs type ClusterIP\n      ## Ref: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/how-it-works/#ingress-traffic\n      serviceType: NodePort\n\n    # Google specific options for Google Application Load Balancer\n    # Applies only when `server.ingress.controller` is set to `gke`\n    ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#google-cloud-load-balancers-with-kubernetes-ingress\n    gke:\n      # -- Google [BackendConfig] resource, for use with the GKE Ingress Controller\n      # @default -- `{}` (See [values.yaml])\n      ## Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features#configuring_ingress_features_through_frontendconfig_parameters\n      backendConfig: {}\n        # iap:\n        #  enabled: true\n        #  oauthclientCredentials:\n        #    secretName: argocd-secret\n\n      # -- Google [FrontendConfig] resource, for use with the GKE Ingress Controller\n      # @default -- `{}` (See [values.yaml])\n      ## Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features#configuring_ingress_features_through_frontendconfig_parameters\n      frontendConfig: {}\n        # redirectToHttps:\n        #   enabled: true\n        #   responseCodeName: RESPONSE_CODE\n\n      # Managed GKE certificate for ingress hostname\n      managedCertificate:\n        # -- Create ManagedCertificate resource and annotations for Google Load balancer\n        ## Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs\n        create: true\n        # -- Additional domains for ManagedCertificate resource\n        extraDomains: []\n          # - argocd.example.com\n\n  # Dedicated gRPC ingress for ingress controllers that supports only single backend protocol per Ingress resource\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts\n  ingressGrpc:\n    # -- Enable an ingress resource for the Argo CD server for dedicated [gRPC-ingress]\n    enabled: false\n    # -- Additional ingress annotations for dedicated [gRPC-ingress]\n    annotations: {}\n    # -- Additional ingress labels for dedicated [gRPC-ingress]\n    labels: {}\n    # -- Defines which ingress controller will implement the resource [gRPC-ingress]\n    ingressClassName: \"\"\n\n    # -- Argo CD server hostname for dedicated [gRPC-ingress]\n    # @default -- `\"\"` (defaults to grpc.`server.ingress.hostname`)\n    hostname: \"\"\n\n    # -- Argo CD server ingress path for dedicated [gRPC-ingress]\n    path: /\n\n    # -- Ingress path type for dedicated [gRPC-ingress]. One of `Exact`, `Prefix` or `ImplementationSpecific`\n    pathType: Prefix\n\n    # -- Enable TLS configuration for the hostname defined at `server.ingressGrpc.hostname`\n    ## TLS certificate will be retrieved from a TLS secret with name: `argocd-server-grpc-tls`\n    tls: false\n\n    # -- The list of additional hostnames to be covered by ingress record\n    # @default -- `[]` (See [values.yaml])\n    extraHosts: []\n      # - name: grpc.argocd.example.com\n      #   path: /\n\n    # -- Additional ingress paths for dedicated [gRPC-ingress]\n    # @default -- `[]` (See [values.yaml])\n    ## Note: Supports use of custom Helm templates\n    extraPaths: []\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service:\n      #       name: ssl-redirect\n      #       port:\n      #         name: use-annotation\n\n    # -- Additional ingress rules\n    # @default -- `[]` (See [values.yaml])\n    ## Note: Supports use of custom Helm templates\n    extraRules: []\n      # - http:\n      #     paths:\n      #     - path: /\n      #       pathType: Prefix\n      #       backend:\n      #         service:\n      #           name: '{{ include \"argo-cd.server.fullname\" . }}'\n      #           port:\n      #             name: '{{ .Values.server.service.servicePortHttpName }}'\n\n    # -- Additional TLS configuration for dedicated [gRPC-ingress]\n    # @default -- `[]` (See [values.yaml])\n    extraTls: []\n      # - secretName: your-certificate-name\n      #   hosts:\n      #     - argocd.example.com\n\n  # Create a OpenShift Route with SSL passthrough for UI and CLI\n  # Consider setting 'hostname' e.g. https://argocd.apps-crc.testing/ using your Default Ingress Controller Domain\n  # Find your domain with: kubectl describe --namespace=openshift-ingress-operator ingresscontroller/default | grep Domain:\n  # If 'hostname' is an empty string \"\" OpenShift will create a hostname for you.\n  route:\n    # -- Enable an OpenShift Route for the Argo CD server\n    enabled: false\n    # -- Openshift Route annotations\n    annotations: {}\n    # -- Hostname of OpenShift Route\n    hostname: \"\"\n    # -- Termination type of Openshift Route\n    termination_type: passthrough\n    # -- Termination policy of Openshift Route\n    termination_policy: None\n\n  # Gateway API HTTPRoute configuration\n  # NOTE: Gateway API support is in EXPERIMENTAL status\n  # Support depends on your Gateway controller implementation\n  # Some controllers may require additional configuration (e.g., BackendTLSPolicy for HTTPS backends)\n  # Refer to https://gateway-api.sigs.k8s.io/implementations/ for controller-specific details\n  httproute:\n    # -- Enable HTTPRoute resource for Argo CD server (Gateway API)\n    enabled: false\n    # -- Additional HTTPRoute labels\n    labels: {}\n    # -- Additional HTTPRoute annotations\n    annotations: {}\n    # -- Gateway API parentRefs for the HTTPRoute\n    ## Must reference an existing Gateway\n    # @default -- `[]` (See [values.yaml])\n    parentRefs: []\n      # - name: example-gateway\n      #   namespace: example-gateway-namespace\n      #   sectionName: https\n    # -- List of hostnames for the HTTPRoute\n    # @default -- `[]` (See [values.yaml])\n    hostnames: []\n      # - argocd.example.com\n    # -- HTTPRoute rules configuration\n    # @default -- `[]` (See [values.yaml])\n    rules:\n      - matches:\n          - path:\n              type: PathPrefix\n              value: /\n        # filters: []\n        #   - type: RequestHeaderModifier\n        #     requestHeaderModifier:\n        #       add:\n        #         - name: X-Custom-Header\n        #           value: custom-value\n\n  # Gateway API GRPCRoute configuration\n  # NOTE: Gateway API support is in EXPERIMENTAL status\n  # Support depends on your Gateway controller implementation\n  # Refer to https://gateway-api.sigs.k8s.io/implementations/ for controller-specific details\n  grpcroute:\n    # -- Enable GRPCRoute resource for Argo CD server (Gateway API)\n    enabled: false\n    # -- Additional GRPCRoute labels\n    labels: {}\n    # -- Additional GRPCRoute annotations\n    annotations: {}\n    # -- Gateway API parentRefs for the GRPCRoute\n    ## Must reference an existing Gateway\n    # @default -- `[]` (See [values.yaml])\n    parentRefs: []\n      # - name: example-gateway\n      #   namespace: example-gateway-namespace\n      #   sectionName: grpc\n    # -- List of hostnames for the GRPCRoute\n    # @default -- `[]` (See [values.yaml])\n    hostnames: []\n      # - grpc.argocd.example.com\n    # -- GRPCRoute rules configuration\n    # @default -- `[]` (See [values.yaml])\n    rules:\n      - matches:\n          - method:\n              type: Exact\n        # filters: []\n        #   - type: RequestHeaderModifier\n        #     requestHeaderModifier:\n        #       add:\n        #         - name: X-Custom-Header\n        #           value: custom-value\n\n  # Gateway API BackendTLSPolicy configuration\n  # NOTE: BackendTLSPolicy is in EXPERIMENTAL status (v1alpha3)\n  # Required for HTTPS backends when using Gateway API\n  # Not all Gateway controllers support this resource (e.g., Cilium does not support it yet)\n  backendTLSPolicy:\n    # -- Enable BackendTLSPolicy resource for Argo CD server (Gateway API)\n    enabled: false\n    # -- Additional BackendTLSPolicy labels\n    labels: {}\n    # -- Additional BackendTLSPolicy annotations\n    annotations: {}\n    # -- Target references for the BackendTLSPolicy\n    # @default -- `[]` (See [values.yaml])\n    targetRefs: []\n      # - group: \"\"\n      #   kind: Service\n      #   name: argocd-server\n      #   sectionName: https\n    # -- TLS validation configuration\n    # @default -- `{}` (See [values.yaml])\n    validation: {}\n      # hostname: argocd-server.argocd.svc.cluster.local\n      # caCertificateRefs:\n      #   - name: example-ca-cert\n      #     group: \"\"\n      #     kind: ConfigMap\n      # wellKnownCACertificates: System\n\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- Enable custom rules for the server's ClusterRole resource\n    enabled: false\n    # -- List of custom rules for the server's ClusterRole resource\n    rules: []\n\n  # Default ArgoCD Server's network policy\n  networkPolicy:\n    # -- Default network policy rules used by ArgoCD Server\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n\n## Repo Server\nrepoServer:\n  # -- Repo server name\n  name: repo-server\n\n  # -- The number of repo server pods to run\n  replicas: 1\n\n  # -- Runtime class name for the repo server\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  ## Repo server Horizontal Pod Autoscaler\n  autoscaling:\n    # -- Enable Horizontal Pod Autoscaler ([HPA]) for the repo server\n    enabled: false\n    # -- Minimum number of replicas for the repo server [HPA]\n    minReplicas: 1\n    # -- Maximum number of replicas for the repo server [HPA]\n    maxReplicas: 5\n    # -- Average CPU utilization percentage for the repo server [HPA]\n    targetCPUUtilizationPercentage: 50\n    # -- Average memory utilization percentage for the repo server [HPA]\n    targetMemoryUtilizationPercentage: 50\n    # -- Configures the scaling behavior of the target in both Up and Down directions.\n    behavior: {}\n      # scaleDown:\n      #  stabilizationWindowSeconds: 300\n      #  policies:\n      #   - type: Pods\n      #     value: 1\n      #     periodSeconds: 180\n      # scaleUp:\n      #   stabilizationWindowSeconds: 300\n      #   policies:\n      #   - type: Pods\n      #     value: 2\n      #     periodSeconds: 60\n    # -- Configures custom HPA metrics for the Argo CD repo server\n    # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\n    metrics: []\n\n  ## Repo server Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the repo server\n    enabled: false\n    # -- Labels to be added to repo server pdb\n    labels: {}\n    # -- Annotations to be added to repo server pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `repoServer.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Repo server image\n  image:\n    # -- Repository to use for the repo server\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the repo server\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the repo server\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Additional command line arguments to pass to repo server\n  extraArgs: []\n\n  # -- Environment variables to pass to repo server\n  env: []\n\n  # -- envFrom to pass to repo server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Specify postStart and preStop lifecycle hooks for your argo-repo-server container\n  lifecycle: {}\n\n  # -- Additional containers to be added to the repo server pod\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/user-guide/config-management-plugins/\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n    # - name: cmp-my-plugin\n    #   command:\n    #     - \"/var/run/argocd/argocd-cmp-server\"\n    #   image: busybox\n    #   securityContext:\n    #     runAsNonRoot: true\n    #     runAsUser: 999\n    #   volumeMounts:\n    #     - mountPath: /var/run/argocd\n    #       name: var-files\n    #     - mountPath: /home/argocd/cmp-server/plugins\n    #       name: plugins\n    #     # Remove this volumeMount if you've chosen to bake the config file into the sidecar image.\n    #     - mountPath: /home/argocd/cmp-server/config/plugin.yaml\n    #       subPath: my-plugin.yaml\n    #       name: argocd-cmp-cm\n    #     # Starting with v2.4, do NOT mount the same tmp volume as the repo-server container. The filesystem separation helps\n    #     # mitigate path traversal attacks.\n    #     - mountPath: /tmp\n    #       name: cmp-tmp\n    # - name: cmp-my-plugin2\n    #   command:\n    #     - \"/var/run/argocd/argocd-cmp-server\"\n    #   image: busybox\n    #   securityContext:\n    #     runAsNonRoot: true\n    #     runAsUser: 999\n    #   volumeMounts:\n    #     - mountPath: /var/run/argocd\n    #       name: var-files\n    #     # Remove this volumeMount if you've chosen to bake the config file into the sidecar image.\n    #     - mountPath: /home/argocd/cmp-server/plugins\n    #       name: plugins\n    #     - mountPath: /home/argocd/cmp-server/config/plugin.yaml\n    #       subPath: my-plugin2.yaml\n    #       name: argocd-cmp-cm\n    #     # Starting with v2.4, do NOT mount the same tmp volume as the repo-server container. The filesystem separation helps\n    #     # mitigate path traversal attacks.\n    #     - mountPath: /tmp\n    #       name: cmp-tmp\n\n  # -- Init containers to add to the repo server pods\n  initContainers: []\n\n  copyutil:\n    # -- Resource limits and requests for the repo server copyutil initContainer\n    resources: {}\n    #  limits:\n    #    cpu: 100m\n    #    memory: 128Mi\n    #  requests:\n    #    cpu: 50m\n    #    memory: 64Mi\n\n  # -- Additional volumeMounts to the repo server main container\n  volumeMounts: []\n\n  # -- Additional volumes to the repo server pod\n  volumes: []\n  #  - name: argocd-cmp-cm\n  #    configMap:\n  #      name: argocd-cmp-cm\n  #  - name: cmp-tmp\n  #    emptyDir: {}\n\n  # -- Volumes to be used in replacement of emptydir on default volumes\n  existingVolumes: {}\n  #  gpgKeyring:\n  #    persistentVolumeClaim:\n  #      claimName: pvc-argocd-repo-server-keyring\n  #  helmWorkingDir:\n  #    persistentVolumeClaim:\n  #      claimName: pvc-argocd-repo-server-workdir\n  #  tmp:\n  #    persistentVolumeClaim:\n  #      claimName: pvc-argocd-repo-server-tmp\n  #  varFiles:\n  #    persistentVolumeClaim:\n  #      claimName: pvc-argocd-repo-server-varfiles\n  #  plugins:\n  #    persistentVolumeClaim:\n  #      claimName: pvc-argocd-repo-server-plugins\n\n  ## RepoServer emptyDir volumes\n  emptyDir:\n    # -- EmptyDir size limit for repo server\n    # @default -- `\"\"` (defaults not set if not specified i.e. no size limit)\n    sizeLimit: \"\"\n    # sizeLimit: \"1Gi\"\n\n  # -- Toggle the usage of a ephemeral Helm working directory\n  useEphemeralHelmWorkingDir: true\n\n  # -- Annotations to be added to repo server Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the repo server Deployment\n  deploymentLabels: {}\n\n  # -- Annotations to be added to repo server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to repo server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the repo server pods\n  resources: {}\n  #  limits:\n  #    cpu: 50m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 10m\n  #    memory: 64Mi\n\n  # Repo server container ports\n  containerPorts:\n    # -- Repo server container port\n    server: 8081\n    # -- Metrics container port\n    metrics: 8084\n\n  # -- Host Network for Repo server pods\n  hostNetwork: false\n\n    # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Repo server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Repo server container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  livenessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the repo server\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the repo server Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Priority class for the repo server pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # TLS certificate configuration via Secret\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#configuring-tls-to-argocd-repo-server\n  ## Note: Issuing certificates via cert-manager in not supported right now because it's not possible to restart repo server automatically without extra controllers.\n  certificateSecret:\n    # -- Create argocd-repo-server-tls secret\n    enabled: false\n    # -- Annotations to be added to argocd-repo-server-tls secret\n    annotations: {}\n    # -- Labels to be added to argocd-repo-server-tls secret\n    labels: {}\n    # -- Certificate authority. Required for self-signed certificates.\n    ca: ''\n    # -- Certificate private key\n    key: ''\n    # -- Certificate data. Must contain SANs of Repo service (ie: argocd-repo-server, argocd-repo-server.argo-cd.svc)\n    crt: ''\n\n  ## Repo server service configuration\n  service:\n    # -- Repo server service annotations\n    annotations: {}\n    # -- Repo server service labels\n    labels: {}\n    # -- Repo server service port\n    port: 8081\n    # -- Repo server service port name\n    portName: tcp-repo-server\n    # -- Traffic distribution preference for the repo server service. If the field is not set, the implementation will apply its default routing strategy.\n    trafficDistribution: \"\"\n\n  ## Repo server metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8084\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus ServiceMonitor scrapeTimeout. If empty, Prometheus uses the global scrape timeout unless it is less than the target's scrape interval value in which the latter is used.\n      scrapeTimeout: \"\"\n      # -- When true, honorLabels preserves the metrics labels when they collide with the targets labels.\n      honorLabels: false\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  ## Enable Custom Rules for the Repo server's Cluster Role resource\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- Enable custom rules for the Repo server's Cluster Role resource\n    enabled: false\n    # -- List of custom rules for the Repo server's Cluster Role resource\n    rules: []\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: true\n\n  ## Repo server service account\n  ## If create is set to true, make sure to uncomment the name and update the rbac section below\n  serviceAccount:\n    # -- Create repo server service account\n    create: true\n    # -- Repo server service account name\n    name: \"\" # \"argocd-repo-server\"\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Repo server rbac rules\n  rbac: []\n  #   - apiGroups:\n  #     - argoproj.io\n  #     resources:\n  #     - applications\n  #     verbs:\n  #     - get\n  #     - list\n  #     - watch\n\n  # Default repo server's network policy\n  networkPolicy:\n    # -- Default network policy rules used by repo server\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n\n## ApplicationSet controller\napplicationSet:\n  # -- ApplicationSet controller name string\n  name: applicationset-controller\n\n  # -- The number of ApplicationSet controller pods to run\n  replicas: 1\n\n  # -- Runtime class name for the ApplicationSet controller\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  ## ApplicationSet controller Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the ApplicationSet controller\n    enabled: false\n    # -- Labels to be added to ApplicationSet controller pdb\n    labels: {}\n    # -- Annotations to be added to ApplicationSet controller pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `applicationSet.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## ApplicationSet controller image\n  image:\n    # -- Repository to use for the ApplicationSet controller\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the ApplicationSet controller\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the ApplicationSet controller\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- If defined, uses a Secret to pull an image from a private Docker registry or repository.\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- ApplicationSet controller command line flags\n  extraArgs: []\n\n  # -- Environment variables to pass to the ApplicationSet controller\n  extraEnv: []\n    # - name: \"MY_VAR\"\n    #   value: \"value\"\n\n  # -- envFrom to pass to the ApplicationSet controller\n  # @default -- `[]` (See [values.yaml])\n  extraEnvFrom: []\n    # - configMapRef:\n    #     name: config-map-name\n    # - secretRef:\n    #     name: secret-name\n\n  # -- Additional containers to be added to the ApplicationSet controller pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the ApplicationSet controller pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- List of extra mounts to add (normally used with extraVolumes)\n  extraVolumeMounts: []\n\n  # -- List of extra volumes to add\n  extraVolumes: []\n\n  ## ApplicationSet controller emptyDir volumes\n  emptyDir:\n    # -- EmptyDir size limit for applicationSet controller\n    # @default -- `\"\"` (defaults not set if not specified i.e. no size limit)\n    sizeLimit: \"\"\n    # sizeLimit: \"1Gi\"\n\n  ## Metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8080\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus ServiceMonitor scrapeTimeout. If empty, Prometheus uses the global scrape timeout unless it is less than the target's scrape interval value in which the latter is used.\n      scrapeTimeout: \"\"\n      # -- When true, honorLabels preserves the metrics labels when they collide with the targets labels.\n      honorLabels: false\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\"  # monitoring\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  ## ApplicationSet service configuration\n  service:\n    # -- ApplicationSet service annotations\n    annotations: {}\n    # -- ApplicationSet service labels\n    labels: {}\n    # -- ApplicationSet service type\n    type: ClusterIP\n    # -- ApplicationSet service port\n    port: 7000\n    # -- ApplicationSet service port name\n    portName: http-webhook\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: true\n\n  serviceAccount:\n    # -- Create ApplicationSet controller service account\n    create: true\n    # -- ApplicationSet controller service account name\n    name: argocd-applicationset-controller\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Annotations to be added to ApplicationSet controller Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the ApplicationSet controller Deployment\n  deploymentLabels: {}\n\n  # -- Annotations for the ApplicationSet controller pods\n  podAnnotations: {}\n\n  # -- Labels for the ApplicationSet controller pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the ApplicationSet controller pods.\n  resources: {}\n    # limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n  # ApplicationSet controller container ports\n  containerPorts:\n    # -- Metrics container port\n    metrics: 8080\n    # -- Probe container port\n    probe: 8081\n    # -- Webhook container port\n    webhook: 7000\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for ApplicationSet controller pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- ApplicationSet controller container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Probes for ApplicationSet controller (optional)\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  readinessProbe:\n    # -- Enable Kubernetes liveness probe for ApplicationSet controller\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for ApplicationSet controller\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the ApplicationSet controller\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the ApplicationSet controller Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Priority class for the ApplicationSet controller pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # TLS certificate configuration via cert-manager\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#tls-configuration\n  certificate:\n    # -- Deploy a Certificate resource (requires cert-manager)\n    enabled: false\n    # -- Certificate primary domain (commonName)\n    # @default -- `\"\"` (defaults to global.domain)\n    domain: \"\"\n    # -- Certificate Subject Alternate Names (SANs)\n    additionalHosts: []\n    # -- The requested 'duration' (i.e. lifetime) of the certificate.\n    # @default -- `\"\"` (defaults to 2160h = 90d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    duration: \"\"\n    # -- How long before the expiry a certificate should be renewed.\n    # @default -- `\"\"` (defaults to 360h = 15d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    renewBefore: \"\"\n    # Certificate issuer\n    ## Ref: https://cert-manager.io/docs/concepts/issuer\n    issuer:\n      # -- Certificate issuer group. Set if using an external issuer. Eg. `cert-manager.io`\n      group: \"\"\n      # -- Certificate issuer kind. Either `Issuer` or `ClusterIssuer`\n      kind: \"\"\n      # -- Certificate issuer name. Eg. `letsencrypt`\n      name: \"\"\n    # Private key of the certificate\n    privateKey:\n      # -- Rotation policy of private key when certificate is re-issued. Either: `Never` or `Always`\n      rotationPolicy: Never\n      # -- The private key cryptography standards (PKCS) encoding for private key. Either: `PCKS1` or `PKCS8`\n      encoding: PKCS1\n      # -- Algorithm used to generate certificate private key. One of: `RSA`, `Ed25519` or `ECDSA`\n      algorithm: RSA\n      # -- Key bit size of the private key. If algorithm is set to `Ed25519`, size is ignored.\n      size: 2048\n    # -- Annotations to be applied to the ApplicationSet Certificate\n    annotations: {}\n\n  ## Ingress for the Git Generator webhook\n  ## Ref: https://argocd-applicationset.readthedocs.io/en/master/Generators-Git/#webhook-configuration)\n  ingress:\n    # -- Enable an ingress resource for ApplicationSet webhook\n    enabled: false\n    # -- Additional ingress labels\n    labels: {}\n    # -- Additional ingress annotations\n    annotations: {}\n\n    # -- Defines which ingress ApplicationSet controller will implement the resource\n    ingressClassName: \"\"\n\n    # -- Argo CD ApplicationSet hostname\n    # @default -- `\"\"` (defaults to global.domain)\n    hostname: \"\"\n\n    # -- List of ingress paths\n    path: /api/webhook\n\n    # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`\n    pathType: Prefix\n\n    # -- Enable TLS configuration for the hostname defined at `applicationSet.webhook.ingress.hostname`\n    ## TLS certificate will be retrieved from a TLS secret with name:`argocd-applicationset-controller-tls`\n    tls: false\n\n    # -- The list of additional hostnames to be covered by ingress record\n    # @default -- `[]` (See [values.yaml])\n    extraHosts: []\n      # - name: argocd.example.com\n      #   path: /\n\n    # -- Additional ingress paths\n    # @default -- `[]` (See [values.yaml])\n    extraPaths: []\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service:\n      #       name: ssl-redirect\n      #       port:\n      #         name: use-annotation\n\n    # -- Additional ingress rules\n    # @default -- `[]` (See [values.yaml])\n    ## Note: Supports use of custom Helm templates\n    extraRules: []\n      # - http:\n      #    paths:\n      #    - path: /api/webhook\n      #      pathType: Prefix\n      #      backend:\n      #        service:\n      #          name: '{{ include \"argo-cd.applicationSet.fullname\" . }}'\n      #          port:\n      #            name: '{{ .Values.applicationSet.service.portName }}'\n\n    # -- Additional ingress TLS configuration\n    # @default -- `[]` (See [values.yaml])\n    extraTls: []\n      # - secretName: argocd-applicationset-tls\n      #   hosts:\n      #     - argocd-applicationset.example.com\n  # -- Enable ApplicationSet in any namespace feature\n  allowAnyNamespace: false\n\n  # Default ApplicationSet controller's network policy\n  networkPolicy:\n    # -- Default network policy rules used by ApplicationSet controller\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n\n## Notifications controller\nnotifications:\n  # -- Enable notifications controller\n  enabled: false\n\n  # -- Notifications controller name string\n  name: notifications-controller\n\n  # -- Argo CD dashboard url; used in place of {{.context.argocdUrl}} in templates\n  # @default -- `\"\"` (defaults to https://`global.domain`)\n  argocdUrl: \"\"\n\n  # -- Runtime class name for the notifications controller\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  ## Notifications controller Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the notifications controller\n    enabled: false\n    # -- Labels to be added to notifications controller pdb\n    labels: {}\n    # -- Annotations to be added to notifications controller pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `notifications.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Notifications controller image\n  image:\n    # -- Repository to use for the notifications controller\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the notifications controller\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the notifications controller\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # DEPRECATED - Use configs.params to override\n  # -- Notifications controller log format. Either `text` or `json`\n  # @default -- `\"\"` (defaults to global.logging.format)\n  # logFormat: \"\"\n  # -- Notifications controller log level. One of: `debug`, `info`, `warn`, `error`\n  # @default -- `\"\"` (defaults to global.logging.level)\n  # logLevel: \"\"\n\n  # -- Extra arguments to provide to the notifications controller\n  extraArgs: []\n\n  # -- Additional container environment variables\n  extraEnv: []\n\n  # -- envFrom to pass to the notifications controller\n  # @default -- `[]` (See [values.yaml])\n  extraEnvFrom: []\n    # - configMapRef:\n    #     name: config-map-name\n    # - secretRef:\n    #     name: secret-name\n\n  # -- Additional containers to be added to the notifications controller pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the notifications controller pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- List of extra mounts to add (normally used with extraVolumes)\n  extraVolumeMounts: []\n\n  # -- List of extra volumes to add\n  extraVolumes: []\n\n  # -- Define user-defined context\n  ## For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/templates/#defining-user-defined-context\n  context: {}\n    # region: east\n    # environmentName: staging\n\n  secret:\n    # -- Whether helm chart creates notifications controller secret\n    ## If true, will create a secret with the name below. Otherwise, will assume existence of a secret with that name.\n    create: true\n\n    # -- notifications controller Secret name\n    name: \"argocd-notifications-secret\"\n\n    # -- key:value pairs of annotations to be added to the secret\n    annotations: {}\n\n    # -- key:value pairs of labels to be added to the secret\n    labels: {}\n\n    # -- Generic key:value pairs to be inserted into the secret\n    ## Can be used for templates, notification services etc. Some examples given below.\n    ## For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/overview/\n    items: {}\n      # slack-token:\n      #   # For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/slack/\n\n      # grafana-apiKey:\n      #   # For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/grafana/\n\n      # webhooks-github-token:\n\n      # email-username:\n      # email-password:\n        # For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/email/\n\n  metrics:\n    # -- Enables prometheus metrics server\n    enabled: false\n    # -- Metrics port\n    port: 9001\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n      # namespace: monitoring\n      # interval: 30s\n      # scrapeTimeout: 10s\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- When true, honorLabels preserves the metrics labels when they collide with the targets labels.\n      honorLabels: false\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n\n  # -- Configures notification services such as slack, email or custom webhook\n  # @default -- See [values.yaml]\n  ## For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/overview/\n  notifiers: {}\n    # service.slack: |\n    #   token: $slack-token\n\n  # -- Annotations to be applied to the notifications controller Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the notifications controller Deployment\n  deploymentLabels: {}\n\n  # -- Annotations to be applied to the notifications controller Pods\n  podAnnotations: {}\n\n  # -- Labels to be applied to the notifications controller Pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the notifications controller\n  resources: {}\n    # limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n  # Notification controller container ports\n  containerPorts:\n    # -- Metrics container port\n    metrics: 9001\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for notifications controller Pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Notification controller container-level security Context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Probes for notifications controller Pods (optional)\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  readinessProbe:\n    # -- Enable Kubernetes liveness probe for notifications controller Pods\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for notifications controller Pods\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the application controller\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the notifications controller Deployment\n  deploymentStrategy:\n    type: Recreate\n\n  # -- Priority class for the notifications controller pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: true\n\n  serviceAccount:\n    # -- Create notifications controller service account\n    create: true\n    # -- Notification controller service account name\n    name: argocd-notifications-controller\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  cm:\n    # -- Whether helm chart creates notifications controller config map\n    create: true\n\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- List of custom rules for the notifications controller's ClusterRole resource\n    rules: []\n\n  # -- Contains centrally managed global application subscriptions\n  ## For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/subscriptions/\n  subscriptions: []\n    # # subscription for on-sync-status-unknown trigger notifications\n    # - recipients:\n    #   - slack:test2\n    #   - email:test@gmail.com\n    #   triggers:\n    #   - on-sync-status-unknown\n    # # subscription restricted to applications with matching labels only\n    # - recipients:\n    #   - slack:test3\n    #   selector: test=true\n    #   triggers:\n    #   - on-sync-status-unknown\n\n  # -- The notification template is used to generate the notification content\n  ## For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/templates/\n  templates: {}\n    # template.app-deployed: |\n    #   email:\n    #     subject: New version of an application {{.app.metadata.name}} is up and running.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:white_check_mark:{{end}} Application {{.app.metadata.name}} is now running new version of deployments manifests.\n    #   slack:\n    #     attachments: |\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#18be52\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Revision\",\n    #           \"value\": \"{{.app.status.sync.revision}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-health-degraded: |\n    #   email:\n    #     subject: Application {{.app.metadata.name}} has degraded.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:exclamation:{{end}} Application {{.app.metadata.name}} has degraded.\n    #     Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}.\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#f4c030\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-failed: |\n    #   email:\n    #     subject: Failed to sync application {{.app.metadata.name}}.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:exclamation:{{end}}  The sync operation of application {{.app.metadata.name}} has failed at {{.app.status.operationState.finishedAt}} with the following error: {{.app.status.operationState.message}}\n    #     Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#E96D76\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-running: |\n    #   email:\n    #     subject: Start syncing application {{.app.metadata.name}}.\n    #   message: |\n    #     The sync operation of application {{.app.metadata.name}} has started at {{.app.status.operationState.startedAt}}.\n    #     Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#0DADEA\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-status-unknown: |\n    #   email:\n    #     subject: Application {{.app.metadata.name}} sync status is 'Unknown'\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:exclamation:{{end}} Application {{.app.metadata.name}} sync is 'Unknown'.\n    #     Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}.\n    #     {{if ne .serviceType \"slack\"}}\n    #     {{range $c := .app.status.conditions}}\n    #         * {{$c.message}}\n    #     {{end}}\n    #     {{end}}\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#E96D76\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-succeeded: |\n    #   email:\n    #     subject: Application {{.app.metadata.name}} has been successfully synced.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:white_check_mark:{{end}} Application {{.app.metadata.name}} has been successfully synced at {{.app.status.operationState.finishedAt}}.\n    #     Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#18be52\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n\n  # -- The trigger defines the condition when the notification should be sent\n  ## For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/triggers/\n  triggers: {}\n    # trigger.on-deployed: |\n    #   - description: Application is synced and healthy. Triggered once per commit.\n    #     oncePer: app.status.sync.revision\n    #     send:\n    #     - app-deployed\n    #     when: app.status.operationState.phase in ['Succeeded'] and app.status.health.status == 'Healthy'\n    # trigger.on-health-degraded: |\n    #   - description: Application has degraded\n    #     send:\n    #     - app-health-degraded\n    #     when: app.status.health.status == 'Degraded'\n    # trigger.on-sync-failed: |\n    #   - description: Application syncing has failed\n    #     send:\n    #     - app-sync-failed\n    #     when: app.status.operationState.phase in ['Error', 'Failed']\n    # trigger.on-sync-running: |\n    #   - description: Application is being synced\n    #     send:\n    #     - app-sync-running\n    #     when: app.status.operationState.phase in ['Running']\n    # trigger.on-sync-status-unknown: |\n    #   - description: Application status is 'Unknown'\n    #     send:\n    #     - app-sync-status-unknown\n    #     when: app.status.sync.status == 'Unknown'\n    # trigger.on-sync-succeeded: |\n    #   - description: Application syncing has succeeded\n    #     send:\n    #     - app-sync-succeeded\n    #     when: app.status.operationState.phase in ['Succeeded']\n    #\n    # For more information: https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/triggers/#default-triggers\n    # defaultTriggers: |\n    #   - on-sync-status-unknown\n\n  # Default notifications controller's network policy\n  networkPolicy:\n    # -- Default network policy rules used by notifications controller\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n\ncommitServer:\n  # -- Enable commit server\n  enabled: false\n\n  # -- Commit server name\n  name: commit-server\n\n  # -- Runtime class name for the commit server\n  # @default -- `\"\"` (defaults to global.runtimeClassName)\n  runtimeClassName: \"\"\n\n  ## commit server controller image\n  image:\n    # -- Repository to use for the commit server\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the commit server\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the commit server\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- commit server command line flags\n  extraArgs: []\n\n  # -- Environment variables to pass to the commit server\n  extraEnv: []\n    # - name: \"MY_VAR\"\n    #   value: \"value\"\n\n  # -- envFrom to pass to the commit server\n  # @default -- `[]` (See [values.yaml])\n  extraEnvFrom: []\n    # - configMapRef:\n    #     name: config-map-name\n    # - secretRef:\n    #     name: secret-name\n\n  # -- List of extra mounts to add (normally used with extraVolumes)\n  extraVolumeMounts: []\n\n  # -- List of extra volumes to add\n  extraVolumes: []\n\n  metrics:\n    # -- Enables prometheus metrics server\n    enabled: false\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8087\n      # -- Metrics service port name\n      portName: metrics\n\n  ## commit server service configuration\n  service:\n    # -- commit server service annotations\n    annotations: {}\n    # -- commit server service labels\n    labels: {}\n    # -- commit server service port\n    port: 8086\n    # -- commit server service port name\n    portName: server\n\n  # -- Automount API credentials for the Service Account into the pod.\n  automountServiceAccountToken: false\n\n  serviceAccount:\n    # -- Create commit server service account\n    create: true\n    # -- commit server service account name\n    name: argocd-commit-server\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Annotations to be added to commit server Deployment\n  deploymentAnnotations: {}\n\n  # -- Labels for the commit server Deployment\n  deploymentLabels: {}\n\n  # -- Annotations for the commit server pods\n  podAnnotations: {}\n\n  # -- Labels for the commit server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the commit server pods.\n  resources: {}\n    # limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for commit server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- commit server container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop:\n      - ALL\n    seccompProfile:\n      type: RuntimeDefault\n\n  ## Probes for commit server (optional)\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  readinessProbe:\n    # -- Enable Kubernetes liveness probe for commit server\n    enabled: true\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 5\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for commit server\n    enabled: true\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 30\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 30\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 5\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the commit server\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the commit server Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Priority class for the commit server pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # Default commit server's network policy\n  networkPolicy:\n    # -- Default network policy rules used by commit server\n    # @default -- `false` (defaults to global.networkPolicy.create)\n    create: false\n"
            ],
            "verify": false,
            "version": "9.0.5",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "grafana",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "grafana",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "grafana",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "12.2.0",
                "chart": "grafana",
                "first_deployed": 1761176462,
                "last_deployed": 1761177593,
                "name": "grafana",
                "namespace": "monitoring",
                "notes": "1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   grafana.monitoring.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\n     export NODE_PORT=$(kubectl get --namespace monitoring -o jsonpath=\"{.spec.ports[0].nodePort}\" services grafana)\n     export NODE_IP=$(kubectl get nodes --namespace monitoring -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n     echo http://$NODE_IP:$NODE_PORT\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n",
                "revision": 3,
                "values": "{\"admin\":{\"existingSecret\":\"\",\"passwordKey\":\"admin-password\",\"userKey\":\"admin-user\"},\"adminPassword\":\"strongpassword\",\"adminUser\":\"admin\",\"affinity\":{},\"alerting\":{},\"assertNoLeakedSecrets\":true,\"automountServiceAccountToken\":true,\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":5,\"minReplicas\":1,\"targetCPU\":\"60\",\"targetMemory\":\"\"},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"createConfigmap\":true,\"dashboardProviders\":{},\"dashboards\":{},\"dashboardsConfigMaps\":{},\"datasources\":{\"datasources.yaml\":{\"apiVersion\":1,\"datasources\":[{\"access\":\"proxy\",\"editable\":true,\"isDefault\":true,\"jsonData\":{\"httpMethod\":\"POST\",\"timeInterval\":\"30s\"},\"name\":\"Mimir\",\"type\":\"prometheus\",\"uid\":\"mimir\",\"url\":\"http://mimir-nginx.monitoring.svc.cluster.local/prometheus\"},{\"access\":\"proxy\",\"editable\":true,\"jsonData\":{\"derivedFields\":[{\"datasourceUid\":\"tempo\",\"matcherRegex\":\"trace_id=(\\\\w+)\",\"name\":\"TraceID\",\"url\":\"$${__value.raw}\"}],\"maxLines\":1000},\"name\":\"Loki\",\"type\":\"loki\",\"uid\":\"loki\",\"url\":\"http://loki-gateway.monitoring.svc.cluster.local\"},{\"access\":\"proxy\",\"editable\":true,\"jsonData\":{\"lokiSearch\":{\"datasourceUid\":\"loki\"},\"nodeGraph\":{\"enabled\":true},\"search\":{\"hide\":false},\"serviceMap\":{\"datasourceUid\":\"mimir\"},\"tracesToLogsV2\":{\"datasourceUid\":\"loki\",\"filterBySpanID\":false,\"filterByTraceID\":true,\"spanEndTimeShift\":\"1h\",\"spanStartTimeShift\":\"-1h\",\"tags\":[{\"key\":\"service.name\",\"value\":\"service_name\"}]},\"tracesToMetrics\":{\"datasourceUid\":\"mimir\",\"queries\":[{\"name\":\"Sample query\",\"query\":\"sum(rate(traces_spanmetrics_latency_bucket{$$__tags}[5m]))\"}],\"spanEndTimeShift\":\"1h\",\"spanStartTimeShift\":\"-1h\",\"tags\":[{\"key\":\"service.name\",\"value\":\"service\"}]}},\"name\":\"Tempo\",\"type\":\"tempo\",\"uid\":\"tempo\",\"url\":\"http://tempo.monitoring.svc.cluster.local:3200\"},{\"access\":\"proxy\",\"editable\":true,\"jsonData\":{\"timeInterval\":\"30s\"},\"name\":\"Prometheus\",\"type\":\"prometheus\",\"uid\":\"prometheus\",\"url\":\"http://prometheus-server.monitoring.svc.cluster.local\"}]}},\"defaultCurlOptions\":\"-skf\",\"deploymentStrategy\":{\"type\":\"RollingUpdate\"},\"dnsConfig\":{},\"dnsPolicy\":null,\"downloadDashboards\":{\"env\":{},\"envFromSecret\":\"\",\"envValueFrom\":{},\"resources\":{},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"seccompProfile\":{\"type\":\"RuntimeDefault\"}}},\"downloadDashboardsImage\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"curlimages/curl\",\"sha\":\"\",\"tag\":\"8.9.1\"},\"enableKubeBackwardCompatibility\":false,\"enableServiceLinks\":true,\"env\":{},\"envFromConfigMaps\":[],\"envFromSecret\":\"\",\"envFromSecrets\":[],\"envRenderSecret\":{},\"envValueFrom\":{},\"extraConfigmapMounts\":[],\"extraContainerVolumes\":[],\"extraContainers\":\"\",\"extraEmptyDirMounts\":[],\"extraExposePorts\":[],\"extraInitContainers\":[],\"extraLabels\":{},\"extraObjects\":[],\"extraSecretMounts\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":null},\"gossipPortName\":\"gossip\",\"grafana.ini\":{\"analytics\":{\"check_for_updates\":true},\"grafana_net\":{\"url\":\"https://grafana.net\"},\"log\":{\"mode\":\"console\"},\"paths\":{\"data\":\"/var/lib/grafana/\",\"logs\":\"/var/log/grafana\",\"plugins\":\"/var/lib/grafana/plugins\",\"provisioning\":\"/etc/grafana/provisioning\"},\"server\":{\"domain\":\"{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ tpl (.Values.ingress.hosts | first) . }}{{ else }}''{{ end }}\"}},\"headlessService\":false,\"hostAliases\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"grafana/grafana\",\"sha\":\"\",\"tag\":\"\"},\"imageRenderer\":{\"affinity\":{},\"automountServiceAccountToken\":false,\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":5,\"minReplicas\":1,\"targetCPU\":\"60\",\"targetMemory\":\"\"},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentStrategy\":{},\"enabled\":false,\"env\":{\"HTTP_HOST\":\"0.0.0.0\",\"XDG_CACHE_HOME\":\"/tmp/.chromium\",\"XDG_CONFIG_HOME\":\"/tmp/.chromium\"},\"envValueFrom\":{},\"extraConfigmapMounts\":[],\"extraSecretMounts\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"grafanaProtocol\":\"http\",\"grafanaSubPath\":\"\",\"hostAliases\":[],\"image\":{\"pullPolicy\":\"Always\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"grafana/grafana-image-renderer\",\"sha\":\"\",\"tag\":\"latest\"},\"networkPolicy\":{\"extraIngressSelectors\":[],\"limitEgress\":false,\"limitIngress\":true},\"nodeSelector\":{},\"podAnnotations\":{},\"podPortName\":\"http\",\"priorityClassName\":\"\",\"renderingCallbackURL\":\"\",\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":10,\"securityContext\":{},\"serverURL\":\"\",\"service\":{\"appProtocol\":\"\",\"enabled\":true,\"port\":8081,\"portName\":\"http\",\"targetPort\":8081},\"serviceAccountName\":\"\",\"serviceMonitor\":{\"enabled\":false,\"interval\":\"1m\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"targetLabels\":[],\"tlsConfig\":{}},\"tolerations\":[]},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraPaths\":[],\"hosts\":[\"chart-example.local\"],\"labels\":{},\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"initChownData\":{\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"library/busybox\",\"sha\":\"\",\"tag\":\"1.31.1\"},\"resources\":{},\"securityContext\":{\"capabilities\":{\"add\":[\"CHOWN\"],\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":false,\"runAsNonRoot\":false,\"runAsUser\":0,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}}},\"ldap\":{\"config\":\"\",\"enabled\":false,\"existingSecret\":\"\"},\"lifecycleHooks\":{},\"livenessProbe\":{\"failureThreshold\":10,\"httpGet\":{\"path\":\"/api/health\",\"port\":3000},\"initialDelaySeconds\":60,\"timeoutSeconds\":30},\"namespaceOverride\":\"\",\"networkPolicy\":{\"allowExternal\":true,\"egress\":{\"blockDNSResolution\":false,\"enabled\":false,\"ports\":[],\"to\":[]},\"enabled\":false,\"explicitNamespacesSelector\":{},\"ingress\":true},\"nodeSelector\":{},\"notifiers\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"disableWarning\":false,\"enabled\":false,\"extraPvcLabels\":{},\"finalizers\":[\"kubernetes.io/pvc-protection\"],\"inMemory\":{\"enabled\":false},\"lookupVolumeName\":true,\"size\":\"10Gi\",\"type\":\"pvc\",\"volumeName\":\"\"},\"plugins\":[],\"podDisruptionBudget\":{},\"podPortName\":\"grafana\",\"rbac\":{\"create\":true,\"extraClusterRoleRules\":[],\"extraRoleRules\":[],\"namespaced\":false,\"pspEnabled\":false,\"pspUseAppArmor\":false},\"readinessProbe\":{\"httpGet\":{\"path\":\"/api/health\",\"port\":3000}},\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":10,\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"gateway.networking.k8s.io/v1\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"kind\":\"HTTPRoute\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"securityContext\":{\"fsGroup\":472,\"runAsGroup\":472,\"runAsNonRoot\":true,\"runAsUser\":472},\"service\":{\"annotations\":{},\"appProtocol\":\"\",\"enabled\":true,\"ipFamilies\":[],\"ipFamilyPolicy\":\"\",\"labels\":{},\"loadBalancerClass\":\"\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"port\":80,\"portName\":\"service\",\"sessionAffinity\":\"\",\"targetPort\":3000,\"type\":\"NodePort\"},\"serviceAccount\":{\"automountServiceAccountToken\":false,\"create\":true,\"labels\":{},\"name\":null,\"nameTest\":null},\"serviceMonitor\":{\"basicAuth\":{},\"enabled\":false,\"interval\":\"30s\",\"labels\":{},\"metricRelabelings\":[],\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"targetLabels\":[],\"tlsConfig\":{}},\"shareProcessNamespace\":false,\"sidecar\":{\"alerts\":{\"enabled\":false,\"env\":{},\"envValueFrom\":{},\"extraMounts\":[],\"initAlerts\":false,\"label\":\"grafana_alert\",\"labelValue\":\"\",\"reloadURL\":\"http://localhost:3000/api/admin/provisioning/alerting/reload\",\"resource\":\"both\",\"resourceName\":\"\",\"script\":null,\"searchNamespace\":null,\"sizeLimit\":\"\",\"skipReload\":false,\"watchMethod\":\"WATCH\"},\"dashboards\":{\"SCProvider\":true,\"defaultFolderName\":null,\"enabled\":false,\"env\":{},\"envValueFrom\":{},\"extraMounts\":[],\"folder\":\"/tmp/dashboards\",\"folderAnnotation\":null,\"label\":\"grafana_dashboard\",\"labelValue\":\"\",\"provider\":{\"allowUiUpdates\":false,\"disableDelete\":false,\"folder\":\"\",\"folderUid\":\"\",\"foldersFromFilesStructure\":false,\"name\":\"sidecarProvider\",\"orgid\":1,\"type\":\"file\"},\"reloadURL\":\"http://localhost:3000/api/admin/provisioning/dashboards/reload\",\"resource\":\"both\",\"resourceName\":\"\",\"script\":null,\"searchNamespace\":null,\"sizeLimit\":\"\",\"skipReload\":false,\"watchMethod\":\"WATCH\"},\"datasources\":{\"enabled\":false,\"env\":{},\"envValueFrom\":{},\"extraMounts\":[],\"initDatasources\":false,\"label\":\"grafana_datasource\",\"labelValue\":\"\",\"reloadURL\":\"http://localhost:3000/api/admin/provisioning/datasources/reload\",\"resource\":\"both\",\"resourceName\":\"\",\"script\":null,\"searchNamespace\":null,\"sizeLimit\":\"\",\"skipReload\":false,\"watchMethod\":\"WATCH\"},\"enableUniqueFilenames\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"kiwigrid/k8s-sidecar\",\"sha\":\"\",\"tag\":\"1.30.10\"},\"imagePullPolicy\":\"IfNotPresent\",\"livenessProbe\":{},\"notifiers\":{\"enabled\":false,\"env\":{},\"extraMounts\":[],\"initNotifiers\":false,\"label\":\"grafana_notifier\",\"labelValue\":\"\",\"reloadURL\":\"http://localhost:3000/api/admin/provisioning/notifications/reload\",\"resource\":\"both\",\"resourceName\":\"\",\"script\":null,\"searchNamespace\":null,\"sizeLimit\":\"\",\"skipReload\":false,\"watchMethod\":\"WATCH\"},\"plugins\":{\"enabled\":false,\"env\":{},\"extraMounts\":[],\"initPlugins\":false,\"label\":\"grafana_plugin\",\"labelValue\":\"\",\"reloadURL\":\"http://localhost:3000/api/admin/provisioning/plugins/reload\",\"resource\":\"both\",\"resourceName\":\"\",\"script\":null,\"searchNamespace\":null,\"sizeLimit\":\"\",\"skipReload\":false,\"watchMethod\":\"WATCH\"},\"readinessProbe\":{},\"resources\":{},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"seccompProfile\":{\"type\":\"RuntimeDefault\"}}},\"smtp\":{\"existingSecret\":\"\",\"passwordKey\":\"password\",\"userKey\":\"user\"},\"testFramework\":{\"containerSecurityContext\":{},\"enabled\":true,\"image\":{\"registry\":\"docker.io\",\"repository\":\"bats/bats\",\"tag\":\"v1.4.1\"},\"imagePullPolicy\":\"IfNotPresent\",\"resources\":{},\"securityContext\":{}},\"tolerations\":[],\"topologySpreadConstraints\":[],\"useStatefulSet\":false}",
                "version": "10.1.2"
              }
            ],
            "name": "grafana",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "global:\n  # -- Overrides the Docker registry globally for all images\n  imageRegistry: null\n\n  # To help compatibility with other charts which use global.imagePullSecrets.\n  # Allow either an array of {name: pullSecret} maps (k8s-style), or an array of strings (more common helm-style).\n  # Can be templated.\n  # global:\n  #   imagePullSecrets:\n  #   - name: pullSecret1\n  #   - name: pullSecret2\n  # or\n  # global:\n  #   imagePullSecrets:\n  #   - pullSecret1\n  #   - pullSecret2\n  imagePullSecrets: []\n\nrbac:\n  create: true\n  ## Use an existing ClusterRole/Role (depending on rbac.namespaced false/true)\n  # useExistingRole: name-of-some-role\n  # useExistingClusterRole: name-of-some-clusterRole\n  pspEnabled: false\n  pspUseAppArmor: false\n  namespaced: false\n  extraRoleRules: []\n  # - apiGroups: []\n  #   resources: []\n  #   verbs: []\n  extraClusterRoleRules: []\n  # - apiGroups: []\n  #   resources: []\n  #   verbs: []\nserviceAccount:\n  create: true\n  name:\n  nameTest:\n  ## ServiceAccount labels.\n  labels: {}\n  ## Service account annotations. Can be templated.\n  #  annotations:\n  #    eks.amazonaws.com/role-arn: arn:aws:iam::123456789000:role/iam-role-name-here\n\n  ## autoMount is deprecated in favor of automountServiceAccountToken\n  # autoMount: false\n  automountServiceAccountToken: false\n\nreplicas: 1\n\n## Create a headless service for the deployment\nheadlessService: false\n\n## Should the service account be auto mounted on the pod\nautomountServiceAccountToken: true\n\n## Create HorizontalPodAutoscaler object for deployment type\n#\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 5\n  targetCPU: \"60\"\n  targetMemory: \"\"\n  behavior: {}\n\n## See `kubectl explain poddisruptionbudget.spec` for more\n## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\npodDisruptionBudget: {}\n#  apiVersion: \"\"\n#  minAvailable: 1\n#  maxUnavailable: 1\n#  unhealthyPodEvictionPolicy: IfHealthyBudget\n\n## See `kubectl explain deployment.spec.strategy` for more\n## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\ndeploymentStrategy:\n  type: RollingUpdate\n\nreadinessProbe:\n  httpGet:\n    path: /api/health\n    port: 3000\n\nlivenessProbe:\n  httpGet:\n    path: /api/health\n    port: 3000\n  initialDelaySeconds: 60\n  timeoutSeconds: 30\n  failureThreshold: 10\n\n## Use an alternate scheduler, e.g. \"stork\".\n## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n##\n# schedulerName: \"default-scheduler\"\n\nimage:\n  # -- The Docker registry\n  registry: docker.io\n  # -- Docker image repository\n  repository: grafana/grafana\n  # Overrides the Grafana image tag whose default is the chart appVersion\n  tag: \"\"\n  sha: \"\"\n  pullPolicy: IfNotPresent\n\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## Can be templated.\n  ##\n  pullSecrets: []\n  #   - myRegistrKeySecretName\n\ntestFramework:\n  enabled: true\n  ## The type of Helm hook used to run this test. Defaults to test.\n  ## ref: https://helm.sh/docs/topics/charts_hooks/#the-available-hooks\n  ##\n  # hookType: test\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    repository: bats/bats\n    tag: \"v1.4.1\"\n  imagePullPolicy: IfNotPresent\n  securityContext: {}\n  containerSecurityContext: {}\n  resources: {}\n  #  limits:\n  #    cpu: 100m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 100m\n  #    memory: 128Mi\n\n# dns configuration for pod\ndnsPolicy: ~\ndnsConfig: {}\n  # nameservers:\n  #   - 8.8.8.8\n  #   options:\n  #   - name: ndots\n  #     value: \"2\"\n  #   - name: edns0\n\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 472\n  runAsGroup: 472\n  fsGroup: 472\n\ncontainerSecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n    - ALL\n  seccompProfile:\n    type: RuntimeDefault\n\n# Enable creating the grafana configmap\ncreateConfigmap: true\n\n# Extra configmaps to mount in grafana pods\n# Values are templated.\nextraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   subPath: certificates.crt # (optional)\n  #   configMap: certs-configmap\n  #   readOnly: true\n  #   optional: false\n\n\nextraEmptyDirMounts: []\n  # - name: provisioning-notifiers\n  #   mountPath: /etc/grafana/provisioning/notifiers\n\n\n# Apply extra labels to common labels.\nextraLabels: {}\n\n## Assign a PriorityClassName to pods if set\n# priorityClassName:\n\ndownloadDashboardsImage:\n  # -- The Docker registry\n  registry: docker.io\n  repository: curlimages/curl\n  tag: 8.9.1\n  sha: \"\"\n  pullPolicy: IfNotPresent\n\ndownloadDashboards:\n  env: {}\n  envFromSecret: \"\"\n  resources: {}\n  securityContext:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop:\n      - ALL\n    seccompProfile:\n      type: RuntimeDefault\n  envValueFrom: {}\n  #  ENV_NAME:\n  #    configMapKeyRef:\n  #      name: configmap-name\n  #      key: value_key\n\n## Pod Annotations\n# podAnnotations: {}\n\n## ConfigMap Annotations\n# configMapAnnotations: {}\n  # argocd.argoproj.io/sync-options: Replace=true\n\n## Pod Labels\n# podLabels: {}\n\npodPortName: grafana\ngossipPortName: gossip\n## Deployment annotations\n# annotations: {}\n\n## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).\n## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.\n## ref: http://kubernetes.io/docs/user-guide/services/\n##\nservice:\n  enabled: true\n  type: NodePort\n  # Set the ip family policy to configure dual-stack see [Configure dual-stack](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services)\n  ipFamilyPolicy: \"\"\n  # Sets the families that should be supported and the order in which they should be applied to ClusterIP as well. Can be IPv4 and/or IPv6.\n  ipFamilies: []\n  loadBalancerIP: \"\"\n  loadBalancerClass: \"\"\n  loadBalancerSourceRanges: []\n  port: 80\n  targetPort: 3000\n    # targetPort: 4181 To be used with a proxy extraContainer\n  ## Service annotations. Can be templated.\n  annotations: {}\n  labels: {}\n  portName: service\n  # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: \"http\" or \"tcp\"\n  appProtocol: \"\"\n  sessionAffinity: \"\"\n\nserviceMonitor:\n  ## If true, a ServiceMonitor CR is created for a prometheus operator\n  ## https://github.com/coreos/prometheus-operator\n  ##\n  enabled: false\n  path: /metrics\n  #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n  labels: {}\n  interval: 30s\n  scheme: http\n  tlsConfig: {}\n  scrapeTimeout: 30s\n  relabelings: []\n  metricRelabelings: []\n  basicAuth: {}\n  targetLabels: []\n\nextraExposePorts: []\n # - name: keycloak\n #   port: 8080\n #   targetPort: 8080\n\n# overrides pod.spec.hostAliases in the grafana deployment's pods\nhostAliases: []\n  # - ip: \"1.2.3.4\"\n  #   hostnames:\n  #     - \"my.host.com\"\n\ningress:\n  enabled: false\n  # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n  # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n  # ingressClassName: nginx\n  # Values can be templated\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  labels: {}\n  path: /\n\n  # pathType is only for k8s \u003e= 1.1=\n  pathType: Prefix\n\n  hosts:\n    - chart-example.local\n  ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n  extraPaths: []\n  # - path: /*\n  #   backend:\n  #     serviceName: ssl-redirect\n  #     servicePort: use-annotation\n  ## Or for k8s \u003e 1.19\n  # - path: /*\n  #   pathType: Prefix\n  #   backend:\n  #     service:\n  #       name: ssl-redirect\n  #       port:\n  #         name: use-annotation\n\n\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\n# -- BETA: Configure the gateway routes for the chart here.\n# More routes can be added by adding a dictionary key like the 'main' route.\n# Be aware that this is an early beta of this feature,\n# kube-prometheus-stack does not guarantee this works and is subject to change.\n# Being BETA this can/will change in the future without notice, do not use unless you want to take that risk\n# [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)\nroute:\n  main:\n    # -- Enables or disables the route\n    enabled: false\n\n    # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2\n    apiVersion: gateway.networking.k8s.io/v1\n    # -- Set the route kind\n    # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute\n    kind: HTTPRoute\n\n    annotations: {}\n    labels: {}\n\n    hostnames: []\n    # - my-filter.example.com\n    parentRefs: []\n    # - name: acme-gw\n\n    matches:\n      - path:\n          type: PathPrefix\n          value: /\n\n    ## Filters define the filters that are applied to requests that match this rule.\n    filters: []\n\n    ## Additional custom rules that can be added to the route\n    additionalRules: []\n\nresources: {}\n#  limits:\n#    cpu: 100m\n#    memory: 128Mi\n#  requests:\n#    cpu: 100m\n#    memory: 128Mi\n\n## Node labels for pod assignment\n## ref: https://kubernetes.io/docs/user-guide/node-selection/\n#\nnodeSelector: {}\n\n## Tolerations for pod assignment\n## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n##\ntolerations: []\n\n## Affinity for pod assignment (evaluated as template)\n## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n##\naffinity: {}\n\n## Topology Spread Constraints\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n##\ntopologySpreadConstraints: []\n\n## Additional init containers (evaluated as template)\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\n##\nextraInitContainers: []\n\n## Enable an Specify container in extraContainers. This is meant to allow adding an authentication proxy to a grafana pod\nextraContainers: \"\"\n# extraContainers: |\n# - name: proxy\n#   image: quay.io/gambol99/keycloak-proxy:latest\n#   args:\n#   - -provider=github\n#   - -client-id=\n#   - -client-secret=\n#   - -github-org=\u003cORG_NAME\u003e\n#   - -email-domain=*\n#   - -cookie-secret=\n#   - -http-address=http://0.0.0.0:4181\n#   - -upstream-url=http://127.0.0.1:3000\n#   ports:\n#     - name: proxy-web\n#       containerPort: 4181\n\n## Volumes that can be used in init containers that will not be mounted to deployment pods\nextraContainerVolumes: []\n#  - name: volume-from-secret\n#    secret:\n#      secretName: secret-to-mount\n#  - name: empty-dir-volume\n#    emptyDir: {}\n\n## Enable persistence using Persistent Volume Claims\n## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/\n##\npersistence:\n  type: pvc\n  enabled: false\n  # storageClassName: default\n  ## (Optional) Use this to bind the claim to an existing PersistentVolume (PV) by name.\n  volumeName: \"\"\n  accessModes:\n    - ReadWriteOnce\n  size: 10Gi\n  # annotations: {}\n  finalizers:\n    - kubernetes.io/pvc-protection\n  # selectorLabels: {}\n  ## Sub-directory of the PV to mount. Can be templated.\n  # subPath: \"\"\n  ## Name of an existing PVC. Can be templated.\n  # existingClaim:\n  ## Extra labels to apply to a PVC.\n  extraPvcLabels: {}\n  disableWarning: false\n\n  ## If persistence is not enabled, this allows to mount the\n  ## local storage in-memory to improve performance\n  ##\n  inMemory:\n    enabled: false\n    ## The maximum usage on memory medium EmptyDir would be\n    ## the minimum value between the SizeLimit specified\n    ## here and the sum of memory limits of all containers in a pod\n    ##\n    # sizeLimit: 300Mi\n\n  ## If 'lookupVolumeName' is set to true, Helm will attempt to retrieve\n  ## the current value of 'spec.volumeName' and incorporate it into the template.\n  lookupVolumeName: true\n\ninitChownData:\n  ## If false, data ownership will not be reset at startup\n  ## This allows the grafana-server to be run with an arbitrary user\n  ##\n  enabled: true\n\n  ## initChownData container image\n  ##\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    repository: library/busybox\n    tag: \"1.31.1\"\n    sha: \"\"\n    pullPolicy: IfNotPresent\n\n  ## initChownData resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n  #  limits:\n  #    cpu: 100m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 100m\n  #    memory: 128Mi\n  securityContext:\n    readOnlyRootFilesystem: false\n    runAsNonRoot: false\n    runAsUser: 0\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      add:\n        - CHOWN\n      drop:\n        - ALL\n\n# Administrator credentials when not using an existing secret (see below)\nadminUser: admin\nadminPassword: strongpassword\n\n# Use an existing secret for the admin user.\nadmin:\n  ## Name of the secret. Can be templated.\n  existingSecret: \"\"\n  userKey: admin-user\n  passwordKey: admin-password\n\n## Define command to be executed at startup by grafana container\n## Needed if using `vault-env` to manage secrets (ref: https://banzaicloud.com/blog/inject-secrets-into-pods-vault/)\n## Default is \"run.sh\" as defined in grafana's Dockerfile\n# command:\n# - \"sh\"\n# - \"/run.sh\"\n\n## Optionally define args if command is used\n## Needed if using `hashicorp/envconsul` to manage secrets\n## By default no arguments are set\n# args:\n# - \"-secret\"\n# - \"secret/grafana\"\n# - \"./grafana\"\n\n## Extra environment variables that will be pass onto deployment pods\n##\n## to provide grafana with access to CloudWatch on AWS EKS:\n## 1. create an iam role of type \"Web identity\" with provider oidc.eks.* (note the provider for later)\n## 2. edit the \"Trust relationships\" of the role, add a line inside the StringEquals clause using the\n## same oidc eks provider as noted before (same as the existing line)\n## also, replace NAMESPACE and prometheus-operator-grafana with the service account namespace and name\n##\n##  \"oidc.eks.us-east-1.amazonaws.com/id/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:sub\": \"system:serviceaccount:NAMESPACE:prometheus-operator-grafana\",\n##\n## 3. attach a policy to the role, you can use a built in policy called CloudWatchReadOnlyAccess\n## 4. use the following env: (replace 123456789000 and iam-role-name-here with your aws account number and role name)\n##\n## env:\n##   AWS_ROLE_ARN: arn:aws:iam::123456789000:role/iam-role-name-here\n##   AWS_WEB_IDENTITY_TOKEN_FILE: /var/run/secrets/eks.amazonaws.com/serviceaccount/token\n##   AWS_REGION: us-east-1\n##\n## 5. uncomment the EKS section in extraSecretMounts: below\n## 6. uncomment the annotation section in the serviceAccount: above\n## make sure to replace arn:aws:iam::123456789000:role/iam-role-name-here with your role arn\n\nenv: {}\n\n## \"valueFrom\" environment variable references that will be added to deployment pods. Name is templated.\n## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core\n## Renders in container spec as:\n##   env:\n##     ...\n##     - name: \u003ckey\u003e\n##       valueFrom:\n##         \u003cvalue rendered as YAML\u003e\nenvValueFrom: {}\n  #  ENV_NAME:\n  #    configMapKeyRef:\n  #      name: configmap-name\n  #      key: value_key\n\n## The name of a secret in the same kubernetes namespace which contain values to be added to the environment\n## This can be useful for auth tokens, etc. Value is templated.\nenvFromSecret: \"\"\n\n## Sensible environment variables that will be rendered as new secret object\n## This can be useful for auth tokens, etc.\n## If the secret values contains \"{{\", they'll need to be properly escaped so that they are not interpreted by Helm\n## ref: https://helm.sh/docs/howto/charts_tips_and_tricks/#using-the-tpl-function\nenvRenderSecret: {}\n\n## The names of secrets in the same kubernetes namespace which contain values to be added to the environment\n## Each entry should contain a name key, and can optionally specify whether the secret must be defined with an optional key.\n## Name is templated.\nenvFromSecrets: []\n## - name: secret-name\n##   prefix: prefix\n##   optional: true\n\n## The names of configmaps in the same kubernetes namespace which contain values to be added to the environment\n## Each entry should contain a name key, and can optionally specify whether the configmap must be defined with an optional key.\n## Name is templated.\n## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#configmapenvsource-v1-core\nenvFromConfigMaps: []\n## - name: configmap-name\n##   prefix: prefix\n##   optional: true\n\n# Inject Kubernetes services as environment variables.\n# See https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#environment-variables\nenableServiceLinks: true\n\n## Additional grafana server secret mounts\n# Defines additional mounts with secrets. Secrets must be manually created in the namespace.\nextraSecretMounts: []\n  # - name: secret-files\n  #   mountPath: /etc/secrets\n  #   secretName: grafana-secret-files\n  #   readOnly: true\n  #   optional: false\n  #   subPath: \"\"\n  #\n  # for AWS EKS (cloudwatch) use the following (see also instruction in env: above)\n  # - name: aws-iam-token\n  #   mountPath: /var/run/secrets/eks.amazonaws.com/serviceaccount\n  #   readOnly: true\n  #   projected:\n  #     defaultMode: 420\n  #     sources:\n  #       - serviceAccountToken:\n  #           audience: sts.amazonaws.com\n  #           expirationSeconds: 86400\n  #           path: token\n  #\n  # for CSI e.g. Azure Key Vault use the following\n  # - name: secrets-store-inline\n  #  mountPath: /run/secrets\n  #  readOnly: true\n  #  csi:\n  #    driver: secrets-store.csi.k8s.io\n  #    readOnly: true\n  #    volumeAttributes:\n  #      secretProviderClass: \"akv-grafana-spc\"\n  #    nodePublishSecretRef:                       # Only required when using service principal mode\n  #       name: grafana-akv-creds                  # Only required when using service principal mode\n\n## Additional grafana server volume mounts\n# Defines additional volume mounts.\nextraVolumeMounts: []\n  # - name: extra-volume-0\n  #   mountPath: /mnt/volume0\n  #   readOnly: true\n  # - name: extra-volume-1\n  #   mountPath: /mnt/volume1\n  #   readOnly: true\n  # - name: grafana-secrets\n  #   mountPath: /mnt/volume2\n\n## Additional Grafana server volumes\nextraVolumes: []\n  # - name: extra-volume-0\n  #   existingClaim: volume-claim\n  # - name: extra-volume-1\n  #   hostPath:\n  #     path: /usr/shared/\n  #     type: \"\"\n  # - name: grafana-secrets\n  #   csi:\n  #     driver: secrets-store.csi.k8s.io\n  #     readOnly: true\n  #     volumeAttributes:\n  #       secretProviderClass: \"grafana-env-spc\"\n\n## Container Lifecycle Hooks. Execute a specific bash command or make an HTTP request\nlifecycleHooks: {}\n  # postStart:\n  #   exec:\n  #     command: []\n\n## Pass the plugins you want installed as a list.\n##\nplugins: []\n  # - digrich-bubblechart-panel\n  # - grafana-clock-panel\n  ## You can also use other plugin download URL, as long as they are valid zip files,\n  ## and specify the name of the plugin after the semicolon. Like this:\n  # - https://grafana.com/api/plugins/marcusolsson-json-datasource/versions/1.3.2/download;marcusolsson-json-datasource\n\n## Configure grafana datasources\n## ref: http://docs.grafana.org/administration/provisioning/#datasources\n##\ndatasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n      - name: Mimir\n        type: prometheus\n        uid: mimir\n        url: http://mimir-nginx.monitoring.svc.cluster.local/prometheus\n        access: proxy\n        isDefault: true\n        jsonData:\n          timeInterval: 30s\n          httpMethod: POST\n        editable: true\n      \n      - name: Loki\n        type: loki\n        uid: loki\n        url: http://loki-gateway.monitoring.svc.cluster.local\n        access: proxy\n        jsonData:\n          maxLines: 1000\n          derivedFields:\n            #   Tempo  trace_id\n            - datasourceUid: tempo\n              matcherRegex: \"trace_id=(\\\\w+)\"\n              name: TraceID\n              url: \"$${__value.raw}\"\n        editable: true\n      \n      - name: Tempo\n        type: tempo\n        uid: tempo\n        url: http://tempo.monitoring.svc.cluster.local:3200\n        access: proxy\n        jsonData:\n          tracesToLogsV2:\n            #   Loki\n            datasourceUid: loki\n            spanStartTimeShift: \"-1h\"\n            spanEndTimeShift: \"1h\"\n            tags:\n              - key: service.name\n                value: service_name\n            filterByTraceID: true\n            filterBySpanID: false\n          tracesToMetrics:\n            #   Mimir\n            datasourceUid: mimir\n            spanStartTimeShift: \"-1h\"\n            spanEndTimeShift: \"1h\"\n            tags:\n              - key: service.name\n                value: service\n            queries:\n              - name: Sample query\n                query: sum(rate(traces_spanmetrics_latency_bucket{$$__tags}[5m]))\n          serviceMap:\n            datasourceUid: mimir\n          nodeGraph:\n            enabled: true\n          search:\n            hide: false\n          lokiSearch:\n            datasourceUid: loki\n        editable: true\n      \n      - name: Prometheus\n        type: prometheus\n        uid: prometheus\n        url: http://prometheus-server.monitoring.svc.cluster.local\n        access: proxy\n        jsonData:\n          timeInterval: 30s\n        editable: true\n\n## Configure grafana alerting (can be templated)\n## ref: https://docs.grafana.com/alerting/set-up/provision-alerting-resources/file-provisioning/\n##\nalerting: {}\n  # policies.yaml:\n  #   apiVersion: 1\n  #   policies:\n  #     - orgId: 1\n  #       receiver: first_uid\n  #\n  # rules.yaml:\n  #   apiVersion: 1\n  #   groups:\n  #     - orgId: 1\n  #       name: '{{ .Chart.Name }}_my_rule_group'\n  #       folder: my_first_folder\n  #       interval: 60s\n  #       rules:\n  #         - uid: my_id_1\n  #           title: my_first_rule\n  #           condition: A\n  #           data:\n  #             - refId: A\n  #               datasourceUid: '-100'\n  #               model:\n  #                 conditions:\n  #                   - evaluator:\n  #                       params:\n  #                         - 3\n  #                       type: gt\n  #                     operator:\n  #                       type: and\n  #                     query:\n  #                       params:\n  #                         - A\n  #                     reducer:\n  #                       type: last\n  #                     type: query\n  #                 datasource:\n  #                   type: __expr__\n  #                   uid: '-100'\n  #                 expression: 1==0\n  #                 intervalMs: 1000\n  #                 maxDataPoints: 43200\n  #                 refId: A\n  #                 type: math\n  #           dashboardUid: my_dashboard\n  #           panelId: 123\n  #           noDataState: Alerting\n  #           for: 60s\n  #           annotations:\n  #             some_key: some_value\n  #           labels:\n  #             team: sre_team_1\n  #\n  # contactpoints.yaml:\n  #   secret:\n  #     apiVersion: 1\n  #     contactPoints:\n  #       - orgId: 1\n  #         name: cp_1\n  #         receivers:\n  #           - uid: first_uid\n  #             type: pagerduty\n  #             settings:\n  #               integrationKey: XXX\n  #               severity: critical\n  #               class: ping failure\n  #               component: Grafana\n  #               group: app-stack\n  #               summary: |\n  #                 {{ `{{ include \"default.message\" . }}` }}\n  #\n  # templates.yaml:\n  #   apiVersion: 1\n  #   templates:\n  #     - orgId: 1\n  #       name: my_first_template\n  #       template: |\n  #         {{ `\n  #         {{ define \"my_first_template\" }}\n  #         Custom notification message\n  #         {{ end }}\n  #         ` }}\n  #\n  # mutetimes.yaml\n  #   apiVersion: 1\n  #   muteTimes:\n  #     - orgId: 1\n  #       name: mti_1\n  #       # refer to https://prometheus.io/docs/alerting/latest/configuration/#time_interval-0\n  #       time_intervals: {}\n\n## Configure notifiers\n## ref: http://docs.grafana.org/administration/provisioning/#alert-notification-channels\n##\nnotifiers: {}\n#  notifiers.yaml:\n#    notifiers:\n#    - name: email-notifier\n#      type: email\n#      uid: email1\n#      # either:\n#      org_id: 1\n#      # or\n#      org_name: Main Org.\n#      is_default: true\n#      settings:\n#        addresses: an_email_address@example.com\n#    delete_notifiers:\n\n## Configure grafana dashboard providers\n## ref: http://docs.grafana.org/administration/provisioning/#dashboards\n##\n## `path` must be /var/lib/grafana/dashboards/\u003cprovider_name\u003e\n##\ndashboardProviders: {}\n#  dashboardproviders.yaml:\n#    apiVersion: 1\n#    providers:\n#    - name: 'default'\n#      orgId: 1\n#      folder: ''\n#      type: file\n#      disableDeletion: false\n#      editable: true\n#      options:\n#        path: /var/lib/grafana/dashboards/default\n\n## Configure how curl fetches remote dashboards. The beginning dash is required.\n## NOTE: This sets the default short flags for all dashboards, but these\n##       defaults can be overridden individually for each dashboard by setting\n##       curlOptions. See the example dashboards section below.\n##\n## -s  - silent mode\n## -k  - allow insecure (eg: non-TLS) connections\n## -f  - fail fast\n## See the curl documentation for additional options\n##\ndefaultCurlOptions: \"-skf\"\n\n## Configure grafana dashboard to import\n## NOTE: To use dashboards you must also enable/configure dashboardProviders\n## ref: https://grafana.com/dashboards\n##\n## dashboards per provider, use provider name as key.\n##\ndashboards: {}\n  # default:\n  #   some-dashboard:\n  #     json: |\n  #       $RAW_JSON\n  #   custom-dashboard:\n  #     file: dashboards/custom-dashboard.json\n  #   prometheus-stats:\n  #     gnetId: 2\n  #     revision: 2\n  #     datasource: Prometheus\n  #   local-dashboard:\n  #     url: https://example.com/repository/test.json\n  #     curlOptions: \"-sLf\"\n  #     token: ''\n  #   local-dashboard-base64:\n  #     url: https://example.com/repository/test-b64.json\n  #     token: ''\n  #     b64content: true\n  #   local-dashboard-gitlab:\n  #     url: https://example.com/repository/test-gitlab.json\n  #     gitlabToken: ''\n  #   local-dashboard-bitbucket:\n  #     url: https://example.com/repository/test-bitbucket.json\n  #     bearerToken: ''\n  #   local-dashboard-azure:\n  #     url: https://example.com/repository/test-azure.json\n  #     basic: ''\n  #     acceptHeader: '*/*'\n\n## Reference to external ConfigMap per provider. Use provider name as key and ConfigMap name as value.\n## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both.\n## ConfigMap data example:\n##\n## data:\n##   example-dashboard.json: |\n##     RAW_JSON\n##\ndashboardsConfigMaps: {}\n#  default: \"\"\n\n## Grafana's primary configuration\n## NOTE: values in map will be converted to ini format\n## ref: http://docs.grafana.org/installation/configuration/\n##\ngrafana.ini:\n  paths:\n    data: /var/lib/grafana/\n    logs: /var/log/grafana\n    plugins: /var/lib/grafana/plugins\n    provisioning: /etc/grafana/provisioning\n  analytics:\n    check_for_updates: true\n  log:\n    mode: console\n  grafana_net:\n    url: https://grafana.net\n  server:\n    domain: \"{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ tpl (.Values.ingress.hosts | first) . }}{{ else }}''{{ end }}\"\n## grafana Authentication can be enabled with the following values on grafana.ini\n # server:\n      # The full public facing url you use in browser, used for redirects and emails\n #    root_url:\n # https://grafana.com/docs/grafana/latest/auth/github/#enable-github-in-grafana\n # auth.github:\n #    enabled: false\n #    allow_sign_up: false\n #    scopes: user:email,read:org\n #    auth_url: https://github.com/login/oauth/authorize\n #    token_url: https://github.com/login/oauth/access_token\n #    api_url: https://api.github.com/user\n #    team_ids:\n #    allowed_organizations:\n #    client_id:\n #    client_secret:\n## LDAP Authentication can be enabled with the following values on grafana.ini\n## NOTE: Grafana will fail to start if the value for ldap.toml is invalid\n  # auth.ldap:\n  #   enabled: true\n  #   allow_sign_up: true\n  #   config_file: /etc/grafana/ldap.toml\n## Grafana's alerting configuration\n  # unified_alerting:\n  #   enabled: true\n  #   rule_version_record_limit: \"5\"\n\n## Grafana's LDAP configuration\n## Templated by the template in _helpers.tpl\n## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled\n## ref: http://docs.grafana.org/installation/configuration/#auth-ldap\n## ref: http://docs.grafana.org/installation/ldap/#configuration\nldap:\n  enabled: false\n  # `existingSecret` is a reference to an existing secret containing the ldap configuration\n  # for Grafana in a key `ldap-toml`.\n  existingSecret: \"\"\n  # `config` is the content of `ldap.toml` that will be stored in the created secret\n  config: \"\"\n  # config: |-\n  #   verbose_logging = true\n\n  #   [[servers]]\n  #   host = \"my-ldap-server\"\n  #   port = 636\n  #   use_ssl = true\n  #   start_tls = false\n  #   ssl_skip_verify = false\n  #   bind_dn = \"uid=%s,ou=users,dc=myorg,dc=com\"\n\n# When process namespace sharing is enabled, processes in a container are visible to all other containers in the same pod\n# This parameter is added because the ldap reload api is not working https://grafana.com/docs/grafana/latest/developers/http_api/admin/#reload-ldap-configuration\n# To allow an extraContainer to restart the Grafana container\nshareProcessNamespace: false\n\n## Grafana's SMTP configuration\n## NOTE: To enable, grafana.ini must be configured with smtp.enabled\n## ref: http://docs.grafana.org/installation/configuration/#smtp\nsmtp:\n  # `existingSecret` is a reference to an existing secret containing the smtp configuration\n  # for Grafana.\n  existingSecret: \"\"\n  userKey: \"user\"\n  passwordKey: \"password\"\n\n## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders\n## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards\nsidecar:\n  image:\n    # -- The Docker registry\n    registry: quay.io\n    repository: kiwigrid/k8s-sidecar\n    tag: 1.30.10\n    sha: \"\"\n  imagePullPolicy: IfNotPresent\n  resources: {}\n#   limits:\n#     cpu: 100m\n#     memory: 100Mi\n#   requests:\n#     cpu: 50m\n#     memory: 50Mi\n  securityContext:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop:\n      - ALL\n    seccompProfile:\n      type: RuntimeDefault\n  # skipTlsVerify Set to true to skip tls verification for kube api calls\n  # skipTlsVerify: true\n  enableUniqueFilenames: false\n  readinessProbe: {}\n  livenessProbe: {}\n  # Log level default for all sidecars. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL. Defaults to INFO\n  # logLevel: INFO\n  alerts:\n    enabled: false\n    # Additional environment variables for the alerts sidecar\n    env: {}\n    ## \"valueFrom\" environment variable references that will be added to deployment pods. Name is templated.\n    ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core\n    ## Renders in container spec as:\n    ##   env:\n    ##     ...\n    ##     - name: \u003ckey\u003e\n    ##       valueFrom:\n    ##         \u003cvalue rendered as YAML\u003e\n    envValueFrom: {}\n    #  ENV_NAME:\n    #    configMapKeyRef:\n    #      name: configmap-name\n    #      key: value_key\n    # Do not reprocess already processed unchanged resources on k8s API reconnect.\n    # ignoreAlreadyProcessed: true\n    # label that the configmaps with alert are marked with (can be templated)\n    label: grafana_alert\n    # value of label that the configmaps with alert are set to (can be templated)\n    labelValue: \"\"\n    # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.\n    # logLevel: INFO\n    # If specified, the sidecar will search for alert config-maps inside this namespace.\n    # Otherwise the namespace in which the sidecar is running will be used.\n    # It's also possible to specify ALL to search in all namespaces\n    searchNamespace: null\n    # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.\n    watchMethod: WATCH\n    # search in configmap, secret or both\n    resource: both\n    #\n    # resourceName: comma separated list of resource names to be fetched/checked by this sidecar.\n    # per default all resources of the type defined in {{ .Values.sidecar.alerts.resource }} will be checked.\n    # This e.g. allows stricter RBAC rules which are limited to the resources meant for the sidecars.\n    # resourceName: \"secret/alerts-1,configmap/alerts-0\"\n    resourceName: \"\"\n    #\n    # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.\n    # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S\n    # watchServerTimeout: 3600\n    #\n    # watchClientTimeout: is a client-side timeout, configuring your local socket.\n    # If you have a network outage dropping all packets with no RST/FIN,\n    # this is how long your client waits before realizing \u0026 dropping the connection.\n    # defaults to 66sec (sic!)\n    # watchClientTimeout: 60\n    #\n    # maxTotalRetries: Total number of retries to allow for any http request.\n    # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry.\n    # maxTotalRetries: 5\n    #\n    # maxConnectRetries: How many connection-related errors to retry on for any http request.\n    # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxConnectRetries: 10\n    #\n    # maxReadRetries: How many times to retry on read errors for any http request\n    # These errors are raised after the request was sent to the server, so the request may have side-effects.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxReadRetries: 5\n    #\n    # Endpoint to send request to reload alerts\n    reloadURL: \"http://localhost:3000/api/admin/provisioning/alerting/reload\"\n    # Absolute path to a script to execute after a configmap got reloaded.\n    # It runs before calls to REQ_URI. If the file is not executable it will be passed to sh.\n    # Otherwise, it's executed as is. Shebangs known to work are #!/bin/sh and #!/usr/bin/env python\n    script: null\n    skipReload: false\n    # This is needed if skipReload is true, to load any alerts defined at startup time.\n    # Deploy the alert sidecar as an initContainer.\n    initAlerts: false\n    # Additional alerts sidecar volume mounts\n    extraMounts: []\n    # Sets the size limit of the alert sidecar emptyDir volume\n    sizeLimit: \"\"\n  dashboards:\n    enabled: false\n    # Additional environment variables for the dashboards sidecar\n    env: {}\n    ## \"valueFrom\" environment variable references that will be added to deployment pods. Name is templated.\n    ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core\n    ## Renders in container spec as:\n    ##   env:\n    ##     ...\n    ##     - name: \u003ckey\u003e\n    ##       valueFrom:\n    ##         \u003cvalue rendered as YAML\u003e\n    envValueFrom: {}\n    #  ENV_NAME:\n    #    configMapKeyRef:\n    #      name: configmap-name\n    #      key: value_key\n    # Do not reprocess already processed unchanged resources on k8s API reconnect.\n    # ignoreAlreadyProcessed: true\n    SCProvider: true\n    # label that the configmaps with dashboards are marked with (can be templated)\n    label: grafana_dashboard\n    # value of label that the configmaps with dashboards are set to (can be templated)\n    labelValue: \"\"\n    # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.\n    # logLevel: INFO\n    # folder in the pod that should hold the collected dashboards (unless `defaultFolderName` is set)\n    folder: /tmp/dashboards\n    # The default folder name, it will create a subfolder under the `folder` and put dashboards in there instead\n    defaultFolderName: null\n    # Namespaces list. If specified, the sidecar will search for config-maps/secrets inside these namespaces.\n    # Otherwise the namespace in which the sidecar is running will be used.\n    # It's also possible to specify ALL to search in all namespaces.\n    searchNamespace: null\n    # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.\n    watchMethod: WATCH\n    # search in configmap, secret or both\n    resource: both\n    # If specified, the sidecar will look for annotation with this name to create folder and put graph here.\n    # You can use this parameter together with `provider.foldersFromFilesStructure`to annotate configmaps and create folder structure.\n    folderAnnotation: null\n    #\n    # resourceName: comma separated list of resource names to be fetched/checked by this sidecar.\n    # per default all resources of the type defined in {{ .Values.sidecar.dashboards.resource }} will be checked.\n    # This e.g. allows stricter RBAC rules which are limited to the resources meant for the sidecars.\n    # resourceName: \"secret/dashboards-0,configmap/dashboards-1\"\n    resourceName: \"\"\n    #\n    # maxTotalRetries: Total number of retries to allow for any http request.\n    # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry.\n    # maxTotalRetries: 5\n    #\n    # maxConnectRetries: How many connection-related errors to retry on for any http request.\n    # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxConnectRetries: 10\n    #\n    # maxReadRetries: How many times to retry on read errors for any http request\n    # These errors are raised after the request was sent to the server, so the request may have side-effects.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxReadRetries: 5\n    #\n    # Endpoint to send request to reload alerts\n    reloadURL: \"http://localhost:3000/api/admin/provisioning/dashboards/reload\"\n    # Absolute path to a script to execute after a configmap got reloaded.\n    # It runs before calls to REQ_URI. If the file is not executable it will be passed to sh.\n    # Otherwise, it's executed as is. Shebangs known to work are #!/bin/sh and #!/usr/bin/env python\n    script: null\n    skipReload: false\n    # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.\n    # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S\n    # watchServerTimeout: 3600\n    #\n    # watchClientTimeout: is a client-side timeout, configuring your local socket.\n    # If you have a network outage dropping all packets with no RST/FIN,\n    # this is how long your client waits before realizing \u0026 dropping the connection.\n    # defaults to 66sec (sic!)\n    # watchClientTimeout: 60\n    #\n    # provider configuration that lets grafana manage the dashboards\n    provider:\n      # name of the provider, should be unique\n      name: sidecarProvider\n      # orgid as configured in grafana\n      orgid: 1\n      # folder in which the dashboards should be imported in grafana\n      folder: ''\n      # \u003cstring\u003e folder UID. will be automatically generated if not specified\n      folderUid: ''\n      # type of the provider\n      type: file\n      # disableDelete to activate a import-only behaviour\n      disableDelete: false\n      # allow updating provisioned dashboards from the UI\n      allowUiUpdates: false\n      # allow Grafana to replicate dashboard structure from filesystem\n      foldersFromFilesStructure: false\n    # Additional dashboards sidecar volume mounts\n    extraMounts: []\n    # Sets the size limit of the dashboard sidecar emptyDir volume\n    sizeLimit: \"\"\n  datasources:\n    enabled: false\n    # Additional environment variables for the datasourcessidecar\n    env: {}\n    ## \"valueFrom\" environment variable references that will be added to deployment pods. Name is templated.\n    ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core\n    ## Renders in container spec as:\n    ##   env:\n    ##     ...\n    ##     - name: \u003ckey\u003e\n    ##       valueFrom:\n    ##         \u003cvalue rendered as YAML\u003e\n    envValueFrom: {}\n    #  ENV_NAME:\n    #    configMapKeyRef:\n    #      name: configmap-name\n    #      key: value_key\n    # Do not reprocess already processed unchanged resources on k8s API reconnect.\n    # ignoreAlreadyProcessed: true\n    # label that the configmaps with datasources are marked with (can be templated)\n    label: grafana_datasource\n    # value of label that the configmaps with datasources are set to (can be templated)\n    labelValue: \"\"\n    # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.\n    # logLevel: INFO\n    # If specified, the sidecar will search for datasource config-maps inside this namespace.\n    # Otherwise the namespace in which the sidecar is running will be used.\n    # It's also possible to specify ALL to search in all namespaces\n    searchNamespace: null\n    # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.\n    watchMethod: WATCH\n    # search in configmap, secret or both\n    resource: both\n    #\n    # resourceName: comma separated list of resource names to be fetched/checked by this sidecar.\n    # per default all resources of the type defined in {{ .Values.sidecar.datasources.resource }} will be checked.\n    # This e.g. allows stricter RBAC rules which are limited to the resources meant for the sidecars.\n    # resourceName: \"secret/datasources-0,configmap/datasources-15\"\n    resourceName: \"\"\n    #\n    # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.\n    # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S\n    # watchServerTimeout: 3600\n    #\n    # watchClientTimeout: is a client-side timeout, configuring your local socket.\n    # If you have a network outage dropping all packets with no RST/FIN,\n    # this is how long your client waits before realizing \u0026 dropping the connection.\n    # defaults to 66sec (sic!)\n    # watchClientTimeout: 60\n    #\n    # maxTotalRetries: Total number of retries to allow for any http request.\n    # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry.\n    # maxTotalRetries: 5\n    #\n    # maxConnectRetries: How many connection-related errors to retry on for any http request.\n    # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxConnectRetries: 10\n    #\n    # maxReadRetries: How many times to retry on read errors for any http request\n    # These errors are raised after the request was sent to the server, so the request may have side-effects.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxReadRetries: 5\n    #\n    # Endpoint to send request to reload datasources\n    reloadURL: \"http://localhost:3000/api/admin/provisioning/datasources/reload\"\n    # Absolute path to a script to execute after a configmap got reloaded.\n    # It runs before calls to REQ_URI. If the file is not executable it will be passed to sh.\n    # Otherwise, it's executed as is. Shebangs known to work are #!/bin/sh and #!/usr/bin/env python\n    script: null\n    skipReload: false\n    # This is needed if skipReload is true, to load any datasources defined at startup time.\n    # Deploy the datasources sidecar as an initContainer.\n    initDatasources: false\n    # Additional datasources sidecar volume mounts\n    extraMounts: []\n    # Sets the size limit of the datasource sidecar emptyDir volume\n    sizeLimit: \"\"\n  plugins:\n    enabled: false\n    # Additional environment variables for the plugins sidecar\n    env: {}\n    # Do not reprocess already processed unchanged resources on k8s API reconnect.\n    # ignoreAlreadyProcessed: true\n    # label that the configmaps with plugins are marked with (can be templated)\n    label: grafana_plugin\n    # value of label that the configmaps with plugins are set to (can be templated)\n    labelValue: \"\"\n    # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.\n    # logLevel: INFO\n    # If specified, the sidecar will search for plugin config-maps inside this namespace.\n    # Otherwise the namespace in which the sidecar is running will be used.\n    # It's also possible to specify ALL to search in all namespaces\n    searchNamespace: null\n    # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.\n    watchMethod: WATCH\n    # search in configmap, secret or both\n    resource: both\n    #\n    # resourceName: comma separated list of resource names to be fetched/checked by this sidecar.\n    # per default all resources of the type defined in {{ .Values.sidecar.plugins.resource }} will be checked.\n    # This e.g. allows stricter RBAC rules which are limited to the resources meant for the sidecars.\n    # resourceName: \"secret/plugins-0,configmap/plugins-1\"\n    resourceName: \"\"\n    #\n    # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.\n    # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S\n    # watchServerTimeout: 3600\n    #\n    # watchClientTimeout: is a client-side timeout, configuring your local socket.\n    # If you have a network outage dropping all packets with no RST/FIN,\n    # this is how long your client waits before realizing \u0026 dropping the connection.\n    # defaults to 66sec (sic!)\n    # watchClientTimeout: 60\n    #\n    # maxTotalRetries: Total number of retries to allow for any http request.\n    # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry.\n    # maxTotalRetries: 5\n    #\n    # maxConnectRetries: How many connection-related errors to retry on for any http request.\n    # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxConnectRetries: 10\n    #\n    # maxReadRetries: How many times to retry on read errors for any http request\n    # These errors are raised after the request was sent to the server, so the request may have side-effects.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxReadRetries: 5\n    #\n    # Endpoint to send request to reload plugins\n    reloadURL: \"http://localhost:3000/api/admin/provisioning/plugins/reload\"\n    # Absolute path to a script to execute after a configmap got reloaded.\n    # It runs before calls to REQ_URI. If the file is not executable it will be passed to sh.\n    # Otherwise, it's executed as is. Shebangs known to work are #!/bin/sh and #!/usr/bin/env python\n    script: null\n    skipReload: false\n    # Deploy the datasource sidecar as an initContainer in addition to a container.\n    # This is needed if skipReload is true, to load any plugins defined at startup time.\n    initPlugins: false\n    # Additional plugins sidecar volume mounts\n    extraMounts: []\n    # Sets the size limit of the plugin sidecar emptyDir volume\n    sizeLimit: \"\"\n  notifiers:\n    enabled: false\n    # Additional environment variables for the notifierssidecar\n    env: {}\n    # Do not reprocess already processed unchanged resources on k8s API reconnect.\n    # ignoreAlreadyProcessed: true\n    # label that the configmaps with notifiers are marked with (can be templated)\n    label: grafana_notifier\n    # value of label that the configmaps with notifiers are set to (can be templated)\n    labelValue: \"\"\n    # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.\n    # logLevel: INFO\n    # If specified, the sidecar will search for notifier config-maps inside this namespace.\n    # Otherwise the namespace in which the sidecar is running will be used.\n    # It's also possible to specify ALL to search in all namespaces\n    searchNamespace: null\n    # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.\n    watchMethod: WATCH\n    # search in configmap, secret or both\n    resource: both\n    #\n    # resourceName: comma separated list of resource names to be fetched/checked by this sidecar.\n    # per default all resources of the type defined in {{ .Values.sidecar.notifiers.resource }} will be checked.\n    # This e.g. allows stricter RBAC rules which are limited to the resources meant for the sidecars.\n    # resourceName: \"secret/notifiers-2,configmap/notifiers-1\"\n    resourceName: \"\"\n    #\n    # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.\n    # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S\n    # watchServerTimeout: 3600\n    #\n    # watchClientTimeout: is a client-side timeout, configuring your local socket.\n    # If you have a network outage dropping all packets with no RST/FIN,\n    # this is how long your client waits before realizing \u0026 dropping the connection.\n    # defaults to 66sec (sic!)\n    # watchClientTimeout: 60\n    #\n    # maxTotalRetries: Total number of retries to allow for any http request.\n    # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry.\n    # maxTotalRetries: 5\n    #\n    # maxConnectRetries: How many connection-related errors to retry on for any http request.\n    # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxConnectRetries: 10\n    #\n    # maxReadRetries: How many times to retry on read errors for any http request\n    # These errors are raised after the request was sent to the server, so the request may have side-effects.\n    # Applies to all requests to reloadURL and k8s api requests.\n    # Set to 0 to fail on the first retry of this type.\n    # maxReadRetries: 5\n    #\n    # Endpoint to send request to reload notifiers\n    reloadURL: \"http://localhost:3000/api/admin/provisioning/notifications/reload\"\n    # Absolute path to a script to execute after a configmap got reloaded.\n    # It runs before calls to REQ_URI. If the file is not executable it will be passed to sh.\n    # Otherwise, it's executed as is. Shebangs known to work are #!/bin/sh and #!/usr/bin/env python\n    script: null\n    skipReload: false\n    # Deploy the notifier sidecar as an initContainer in addition to a container.\n    # This is needed if skipReload is true, to load any notifiers defined at startup time.\n    initNotifiers: false\n    # Additional notifiers sidecar volume mounts\n    extraMounts: []\n    # Sets the size limit of the notifier sidecar emptyDir volume\n    sizeLimit: \"\"\n\n## Override the deployment namespace\n##\nnamespaceOverride: \"\"\n\n## Number of old ReplicaSets to retain\n##\nrevisionHistoryLimit: 10\n\n## Add a seperate remote image renderer deployment/service\nimageRenderer:\n  deploymentStrategy: {}\n  # Enable the image-renderer deployment \u0026 service\n  enabled: false\n  replicas: 1\n  autoscaling:\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 5\n    targetCPU: \"60\"\n    targetMemory: \"\"\n    behavior: {}\n  # The url of remote image renderer if it is not in the same namespace with the grafana instance\n  serverURL: \"\"\n  # The callback url of grafana instances if it is not in the same namespace with the remote image renderer\n  renderingCallbackURL: \"\"\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # image-renderer Image repository\n    repository: grafana/grafana-image-renderer\n    # image-renderer Image tag\n    tag: latest\n    # image-renderer Image sha (optional)\n    sha: \"\"\n    # image-renderer Image pull secrets (optional)\n    pullSecrets: []\n    # image-renderer ImagePullPolicy\n    pullPolicy: Always\n  # extra environment variables\n  env:\n    HTTP_HOST: \"0.0.0.0\"\n    # Fixes \"Error: Failed to launch the browser process!\\nchrome_crashpad_handler: --database is required\"\n    XDG_CONFIG_HOME: /tmp/.chromium\n    XDG_CACHE_HOME: /tmp/.chromium\n    # RENDERING_ARGS: --no-sandbox,--disable-gpu,--window-size=1280x758\n    # RENDERING_MODE: clustered\n    # IGNORE_HTTPS_ERRORS: true\n\n  ## \"valueFrom\" environment variable references that will be added to deployment pods. Name is templated.\n  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core\n  ## Renders in container spec as:\n  ##   env:\n  ##     ...\n  ##     - name: \u003ckey\u003e\n  ##       valueFrom:\n  ##         \u003cvalue rendered as YAML\u003e\n  envValueFrom: {}\n    #  ENV_NAME:\n    #    configMapKeyRef:\n    #      name: configmap-name\n    #      key: value_key\n\n  # image-renderer deployment serviceAccount\n  serviceAccountName: \"\"\n  automountServiceAccountToken: false\n  # image-renderer deployment securityContext\n  securityContext: {}\n  # image-renderer deployment container securityContext\n  containerSecurityContext:\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop: ['ALL']\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n  ## image-renderer pod annotation\n  podAnnotations: {}\n  # image-renderer deployment Host Aliases\n  hostAliases: []\n  # image-renderer deployment priority class\n  priorityClassName: ''\n  service:\n    # Enable the image-renderer service\n    enabled: true\n    # image-renderer service port name\n    portName: 'http'\n    # image-renderer service port used by both service and deployment\n    port: 8081\n    targetPort: 8081\n    # Adds the appProtocol field to the image-renderer service. This allows to work with istio protocol selection. Ex: \"http\" or \"tcp\"\n    appProtocol: \"\"\n  serviceMonitor:\n    ## If true, a ServiceMonitor CRD is created for a prometheus operator\n    ## https://github.com/coreos/prometheus-operator\n    ##\n    enabled: false\n    path: /metrics\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n    labels: {}\n    interval: 1m\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n    relabelings: []\n    # See: https://doc.crds.dev/github.com/prometheus-operator/kube-prometheus/monitoring.coreos.com/ServiceMonitor/v1@v0.11.0#spec-targetLabels\n    targetLabels: []\n      # - targetLabel1\n      # - targetLabel2\n  # If https is enabled in Grafana, this needs to be set as 'https' to correctly configure the callback used in Grafana\n  grafanaProtocol: http\n  # In case a sub_path is used this needs to be added to the image renderer callback\n  grafanaSubPath: \"\"\n  # name of the image-renderer port on the pod\n  podPortName: http\n  # number of image-renderer replica sets to keep\n  revisionHistoryLimit: 10\n  networkPolicy:\n    # Enable a NetworkPolicy to limit inbound traffic to only the created grafana pods\n    limitIngress: true\n    # Enable a NetworkPolicy to limit outbound traffic to only the created grafana pods\n    limitEgress: false\n    # Allow additional services to access image-renderer (eg. Prometheus operator when ServiceMonitor is enabled)\n    extraIngressSelectors: []\n  resources: {}\n#   limits:\n#     cpu: 100m\n#     memory: 100Mi\n#   requests:\n#     cpu: 50m\n#     memory: 50Mi\n  ## Node labels for pod assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  #\n  nodeSelector: {}\n\n  ## Tolerations for pod assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n\n  ## Affinity for pod assignment (evaluated as template)\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ##\n  affinity: {}\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName: \"default-scheduler\"\n\n  # Extra configmaps to mount in image-renderer pods\n  extraConfigmapMounts: []\n\n  # Extra secrets to mount in image-renderer pods\n  extraSecretMounts: []\n\n  # Extra volumes to mount in image-renderer pods\n  extraVolumeMounts: []\n\n  # Extra volumes for image-renderer pods\n  extraVolumes: []\n\nnetworkPolicy:\n  ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources. Only Ingress traffic is filtered for now.\n  ##\n  enabled: false\n  ## @param networkPolicy.allowExternal Don't require client label for connections\n  ## The Policy model to apply. When set to false, only pods with the correct\n  ## client label will have network access to  grafana port defined.\n  ## When true, grafana will accept connections from any source\n  ## (with the correct destination port).\n  ##\n  ingress: true\n  ## @param networkPolicy.ingress When true enables the creation\n  ## an ingress network policy\n  ##\n  allowExternal: true\n  ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed\n  ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace\n  ## and that match other criteria, the ones that have the good label, can reach the grafana.\n  ## But sometimes, we want the grafana to be accessible to clients from other namespaces, in this case, we can use this\n  ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.\n  ##\n  ## Example:\n  ## explicitNamespacesSelector:\n  ##   matchLabels:\n  ##     role: frontend\n  ##   matchExpressions:\n  ##    - {key: role, operator: In, values: [frontend]}\n  ##\n  explicitNamespacesSelector: {}\n  ##\n  ##\n  ##\n  ##\n  ##\n  ##\n  egress:\n    ## @param networkPolicy.egress.enabled When enabled, an egress network policy will be\n    ## created allowing grafana to connect to external data sources from kubernetes cluster.\n    enabled: false\n    ##\n    ## @param networkPolicy.egress.blockDNSResolution When enabled, DNS resolution will be blocked\n    ## for all pods in the grafana namespace.\n    blockDNSResolution: false\n    ##\n    ## @param networkPolicy.egress.ports Add individual ports to be allowed by the egress\n    ports: []\n    ## Add ports to the egress by specifying - port: \u003cport number\u003e\n    ## E.X.\n    ## - port: 80\n    ## - port: 443\n    ##\n    ## @param networkPolicy.egress.to Allow egress traffic to specific destinations\n    to: []\n    ## Add destinations to the egress by specifying - ipBlock: \u003cCIDR\u003e\n    ## E.X.\n    ## to:\n    ##  - namespaceSelector:\n    ##    matchExpressions:\n  ##    - {key: role, operator: In, values: [grafana]}\n  ##\n  ##\n  ##\n  ##\n  ##\n\n# Enable backward compatibility of kubernetes where version below 1.13 doesn't have the enableServiceLinks option\nenableKubeBackwardCompatibility: false\nuseStatefulSet: false\n\n# extraObjects could be utilized to add dynamic manifests via values\nextraObjects: []\n# Examples:\n# extraObjects:\n# - apiVersion: kubernetes-client.io/v1\n#   kind: ExternalSecret\n#   metadata:\n#     name: grafana-secrets-{{ .Release.Name }}\n#   spec:\n#     backendType: gcpSecretsManager\n#     data:\n#     - key: grafana-admin-password\n#       name: adminPassword\n# Alternatively, you can use strings, which lets you use additional templating features:\n# extraObjects:\n# - |\n#   apiVersion: kubernetes-client.io/v1\n#   kind: ExternalSecret\n#   metadata:\n#     name: grafana-secrets-{{ .Release.Name }}\n#   spec:\n#     backendType: gcpSecretsManager\n#     data:\n#     - key: grafana-admin-password\n#       name: {{ include \"some-other-template\" }}\n\n# assertNoLeakedSecrets is a helper function defined in _helpers.tpl that checks if secret\n# values are not exposed in the rendered grafana.ini configmap. It is enabled by default.\n#\n# To pass values into grafana.ini without exposing them in a configmap, use variable expansion:\n# https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/#variable-expansion\n#\n# Alternatively, if you wish to allow secret values to be exposed in the rendered grafana.ini configmap,\n# you can disable this check by setting assertNoLeakedSecrets to false.\nassertNoLeakedSecrets: true\n"
            ],
            "verify": false,
            "version": "10.1.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "jenkins",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "jenkins",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "jenkins",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.528.1",
                "chart": "jenkins",
                "first_deployed": 1761548838,
                "last_deployed": 1761548838,
                "name": "jenkins",
                "namespace": "ci-cd",
                "notes": "1. Get your 'admin' user password by running:\n  kubectl exec --namespace ci-cd -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/additional/chart-admin-password \u0026\u0026 echo\n2. Get the Jenkins URL to visit by running these commands in the same shell:\n  export NODE_PORT=$(kubectl get --namespace ci-cd -o jsonpath=\"{.spec.ports[0].nodePort}\" services jenkins)\n  export NODE_IP=$(kubectl get nodes --namespace ci-cd -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n  echo http://$NODE_IP:$NODE_PORT\n\n3. Login with the password from step 1 and the username: admin\n4. Configure security realm and authorization strategy\n5. Use Jenkins Configuration as Code by specifying configScripts in your values.yaml file, see documentation: http://$NODE_IP:$NODE_PORT/configuration-as-code and examples: https://github.com/jenkinsci/configuration-as-code-plugin/tree/master/demos\n\nFor more information on running Jenkins on Kubernetes, visit:\nhttps://cloud.google.com/solutions/jenkins-on-container-engine\n\nFor more information about Jenkins Configuration as Code, visit:\nhttps://jenkins.io/projects/jcasc/\n\n\nNOTE: Consider using a custom image with pre-installed plugins\n",
                "revision": 1,
                "values": "{\"additionalAgents\":{},\"additionalClouds\":{},\"agent\":{\"TTYEnabled\":false,\"additionalContainers\":[],\"alwaysPullImage\":false,\"annotations\":{},\"args\":\"${computer.jnlpmac} ${computer.name}\",\"command\":null,\"componentName\":\"jenkins-agent\",\"connectTimeout\":100,\"containerCap\":10,\"customJenkinsLabels\":[],\"defaultsProviderTemplate\":\"\",\"directConnection\":false,\"disableDefaultAgent\":false,\"enabled\":true,\"envVars\":[],\"garbageCollection\":{\"enabled\":false,\"namespaces\":\"\",\"timeout\":300},\"hostNetworking\":false,\"idleMinutes\":0,\"image\":{\"registry\":\"\",\"repository\":\"jenkins/inbound-agent\",\"tag\":\"3345.v03dee9b_f88fc-2\"},\"imagePullSecretName\":null,\"inheritYamlMergeStrategy\":false,\"instanceCap\":2147483647,\"jenkinsTunnel\":null,\"jenkinsUrl\":null,\"jnlpregistry\":null,\"kubernetesConnectTimeout\":5,\"kubernetesReadTimeout\":15,\"livenessProbe\":{},\"maxRequestsPerHostStr\":\"32\",\"namespace\":null,\"nodeSelector\":{},\"nodeUsageMode\":\"NORMAL\",\"podLabels\":{},\"podName\":\"default\",\"podRetention\":\"Never\",\"podTemplates\":{},\"privileged\":false,\"resources\":{\"limits\":{\"cpu\":\"512m\",\"memory\":\"512Mi\"},\"requests\":{\"cpu\":\"512m\",\"memory\":\"512Mi\"}},\"restrictedPssSecurityContext\":false,\"retentionTimeout\":5,\"runAsGroup\":null,\"runAsUser\":null,\"secretEnvVars\":[],\"serviceAccount\":null,\"showRawYaml\":true,\"sideContainerName\":\"jnlp\",\"skipTlsVerify\":false,\"usageRestricted\":false,\"useDefaultServiceAccount\":true,\"volumes\":[],\"waitForPodSec\":600,\"websocket\":false,\"workingDir\":\"/home/jenkins/agent\",\"workspaceVolume\":{},\"yamlMergeStrategy\":\"override\",\"yamlTemplate\":\"\"},\"awsSecurityGroupPolicies\":{\"enabled\":false,\"policies\":[{\"name\":\"\",\"podSelector\":{},\"securityGroupIds\":[]}]},\"checkDeprecation\":true,\"clusterZone\":\"cluster.local\",\"controller\":{\"JCasC\":{\"authorizationStrategy\":\"loggedInUsersCanDoAnything:\\n  allowAnonymousRead: false\",\"configMapAnnotations\":{},\"configScripts\":{},\"configUrls\":[],\"defaultConfig\":true,\"overwriteConfiguration\":false,\"security\":{\"apiToken\":{\"creationOfLegacyTokenEnabled\":false,\"tokenGenerationOnCreationEnabled\":false,\"usageStatisticsEnabled\":true}},\"securityRealm\":\"local:\\n  allowsSignup: false\\n  enableCaptcha: false\\n  users:\\n  - id: \\\"${chart-admin-username}\\\"\\n    name: \\\"Jenkins Admin\\\"\\n    password: \\\"${chart-admin-password}\\\"\"},\"additionalExistingSecrets\":[],\"additionalPlugins\":[],\"additionalSecrets\":[],\"admin\":{\"createSecret\":true,\"existingSecret\":\"\",\"password\":\"admin\",\"passwordKey\":\"jenkins-admin-password\",\"userKey\":\"jenkins-admin-user\",\"username\":\"admin\"},\"affinity\":{},\"agentListenerEnabled\":true,\"agentListenerExternalTrafficPolicy\":null,\"agentListenerHostPort\":null,\"agentListenerLoadBalancerIP\":null,\"agentListenerLoadBalancerSourceRanges\":[\"0.0.0.0/0\"],\"agentListenerNodePort\":null,\"agentListenerPort\":50000,\"agentListenerServiceAnnotations\":{},\"agentListenerServiceType\":\"ClusterIP\",\"backendconfig\":{\"annotations\":{},\"apiVersion\":\"extensions/v1beta1\",\"enabled\":false,\"labels\":{},\"name\":null,\"spec\":{}},\"cloudName\":\"kubernetes\",\"clusterIp\":null,\"componentName\":\"jenkins-controller\",\"containerEnv\":[],\"containerEnvFrom\":[],\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"readOnlyRootFilesystem\":true,\"runAsGroup\":1000,\"runAsUser\":1000},\"csrf\":{\"defaultCrumbIssuer\":{\"enabled\":true,\"proxyCompatability\":true}},\"customInitContainers\":[],\"customJenkinsLabels\":[],\"disableRememberMe\":false,\"disabledAgentProtocols\":[\"JNLP-connect\",\"JNLP2-connect\"],\"dnsConfig\":{},\"enableRawHtmlMarkupFormatter\":false,\"enableServiceLinks\":false,\"executorMode\":\"NORMAL\",\"existingSecret\":null,\"extraPorts\":[],\"fsGroup\":1000,\"googlePodMonitor\":{\"enabled\":false,\"scrapeEndpoint\":\"/prometheus\",\"scrapeInterval\":\"60s\"},\"healthProbes\":true,\"hostAliases\":[],\"hostNetworking\":false,\"httpsKeyStore\":{\"disableSecretMount\":false,\"enable\":false,\"fileName\":\"keystore.jks\",\"httpPort\":8081,\"jenkinsHttpsJksPasswordSecretKey\":\"https-jks-password\",\"jenkinsHttpsJksPasswordSecretName\":\"\",\"jenkinsHttpsJksSecretKey\":\"jenkins-jks-file\",\"jenkinsHttpsJksSecretName\":\"\",\"jenkinsKeyStoreBase64Encoded\":null,\"password\":\"password\",\"path\":\"/var/jenkins_keystore\"},\"image\":{\"pullPolicy\":\"Always\",\"registry\":\"docker.io\",\"repository\":\"jenkins/jenkins\",\"tag\":null,\"tagLabel\":\"jdk21\"},\"imagePullSecretName\":null,\"ingress\":{\"annotations\":{},\"apiVersion\":\"extensions/v1beta1\",\"enabled\":false,\"hostName\":null,\"labels\":{},\"path\":null,\"paths\":[],\"resourceRootUrl\":null,\"tls\":[]},\"initConfigMap\":null,\"initContainerEnv\":[],\"initContainerEnvFrom\":[],\"initContainerResources\":{},\"initScripts\":{},\"initializeOnce\":false,\"installLatestPlugins\":true,\"installLatestSpecifiedPlugins\":false,\"installPlugins\":[\"kubernetes:4384.v1b_6367f393d9\",\"workflow-aggregator:608.v67378e9d3db_1\",\"git:5.8.0\",\"configuration-as-code:2006.v001a_2ca_6b_574\"],\"javaOpts\":null,\"jenkinsAdminEmail\":null,\"jenkinsHome\":\"/var/jenkins_home\",\"jenkinsOpts\":null,\"jenkinsRef\":\"/usr/share/jenkins/ref\",\"jenkinsUriPrefix\":null,\"jenkinsUrl\":null,\"jenkinsUrlProtocol\":null,\"jenkinsWar\":\"/usr/share/jenkins/jenkins.war\",\"jmxPort\":null,\"legacyRemotingSecurityEnabled\":false,\"lifecycle\":{},\"loadBalancerIP\":null,\"loadBalancerSourceRanges\":[\"0.0.0.0/0\"],\"markupFormatter\":\"plainText\",\"nodePort\":null,\"nodeSelector\":{},\"numExecutors\":0,\"overwritePlugins\":false,\"overwritePluginsFromImage\":true,\"podAnnotations\":{},\"podDisruptionBudget\":{\"annotations\":{},\"apiVersion\":\"policy/v1beta1\",\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"0\"},\"podLabels\":{},\"podSecurityContextOverride\":null,\"priorityClassName\":null,\"probes\":{\"livenessProbe\":{\"failureThreshold\":5,\"httpGet\":{\"path\":\"{{ default \\\"\\\" .Values.controller.jenkinsUriPrefix }}/login\",\"port\":\"http\"},\"initialDelaySeconds\":null,\"periodSeconds\":10,\"timeoutSeconds\":5},\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"{{ default \\\"\\\" .Values.controller.jenkinsUriPrefix }}/login\",\"port\":\"http\"},\"initialDelaySeconds\":null,\"periodSeconds\":10,\"timeoutSeconds\":5},\"startupProbe\":{\"failureThreshold\":12,\"httpGet\":{\"path\":\"{{ default \\\"\\\" .Values.controller.jenkinsUriPrefix }}/login\",\"port\":\"http\"},\"periodSeconds\":10,\"timeoutSeconds\":5}},\"projectNamingStrategy\":\"standard\",\"prometheus\":{\"alertingRulesAdditionalLabels\":{},\"alertingrules\":[],\"enabled\":false,\"metricRelabelings\":[],\"prometheusRuleNamespace\":\"\",\"relabelings\":[],\"scrapeEndpoint\":\"/prometheus\",\"scrapeInterval\":\"60s\",\"serviceMonitorAdditionalLabels\":{},\"serviceMonitorNamespace\":null},\"publishNotReadyAddresses\":null,\"resources\":{\"limits\":{\"cpu\":\"2000m\",\"memory\":\"4096Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"256Mi\"}},\"route\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"path\":null},\"runAsUser\":1000,\"schedulerName\":\"\",\"scriptApproval\":[],\"secondaryingress\":{\"annotations\":{},\"apiVersion\":\"extensions/v1beta1\",\"enabled\":false,\"hostName\":null,\"labels\":{},\"paths\":[],\"tls\":null},\"secretClaims\":[],\"securityContextCapabilities\":{},\"serviceAnnotations\":{},\"serviceEnabled\":true,\"serviceExternalTrafficPolicy\":null,\"serviceLabels\":{},\"servicePort\":8080,\"serviceType\":\"NodePort\",\"shareProcessNamespace\":false,\"sidecars\":{\"additionalSidecarContainers\":[],\"configAutoReload\":{\"additionalVolumeMounts\":[],\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"readOnlyRootFilesystem\":true},\"enabled\":true,\"env\":[],\"envFrom\":[],\"folder\":\"/var/jenkins_home/casc_configs\",\"image\":{\"registry\":\"docker.io\",\"repository\":\"kiwigrid/k8s-sidecar\",\"tag\":\"1.30.7\"},\"imagePullPolicy\":\"IfNotPresent\",\"logging\":{\"configuration\":{\"backupCount\":3,\"formatter\":\"JSON\",\"logLevel\":\"INFO\",\"logToConsole\":true,\"logToFile\":false,\"maxBytes\":1024,\"override\":false}},\"reqRetryConnect\":10,\"resources\":{},\"scheme\":\"http\",\"skipTlsVerify\":false,\"sleepTime\":null,\"sshTcpPort\":1044}},\"statefulSetAnnotations\":{},\"statefulSetLabels\":{},\"targetPort\":8080,\"terminationGracePeriodSeconds\":null,\"terminationMessagePath\":null,\"terminationMessagePolicy\":null,\"testEnabled\":true,\"tolerations\":[],\"topologySpreadConstraints\":{},\"updateStrategy\":{},\"usePodSecurityContext\":true},\"credentialsId\":null,\"extraLabels\":{},\"fullnameOverride\":null,\"helmtest\":{\"bats\":{\"image\":{\"registry\":\"docker.io\",\"repository\":\"bats/bats\",\"tag\":\"1.12.0\"}}},\"kubernetesURL\":\"https://kubernetes.default\",\"nameOverride\":null,\"namespaceOverride\":null,\"networkPolicy\":{\"apiVersion\":\"networking.k8s.io/v1\",\"enabled\":false,\"externalAgents\":{\"except\":[],\"ipCIDR\":null},\"internalAgents\":{\"allowed\":true,\"namespaceLabels\":{},\"podLabels\":{}}},\"persistence\":{\"accessMode\":\"ReadWriteOnce\",\"annotations\":{},\"dataSource\":{},\"enabled\":true,\"existingClaim\":null,\"labels\":{},\"mounts\":[],\"size\":\"8Gi\",\"storageClass\":null,\"subPath\":null,\"volumes\":[]},\"rbac\":{\"create\":true,\"readSecrets\":false,\"useOpenShiftNonRootSCC\":false},\"renderHelmLabels\":true,\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"extraLabels\":{},\"imagePullSecretName\":null,\"name\":null},\"serviceAccountAgent\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":false,\"extraLabels\":{},\"imagePullSecretName\":null,\"name\":null}}",
                "version": "5.8.104"
              }
            ],
            "name": "jenkins",
            "namespace": "ci-cd",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# Default values for jenkins.\n# This is a YAML-formatted file.\n# Declare name/value pairs to be passed into your templates.\n# name: value\n\n## Overrides for generated resource names\n# See templates/_helpers.tpl\n# -- Override the resource name prefix\n# @default -- `Chart.Name`\nnameOverride:\n# -- Override the full resource names\n# @default -- `jenkins-(release-name)` or `jenkins` if the release-name is `jenkins`\nfullnameOverride:\n# -- Override the deployment namespace\n# @default -- `Release.Namespace`\nnamespaceOverride:\n\n# For FQDN resolving of the controller service. Change this value to match your existing configuration.\n# ref: https://github.com/kubernetes/dns/blob/master/docs/specification.md\n# -- Override the cluster name for FQDN resolving\nclusterZone: \"cluster.local\"\n\n# -- The URL of the Kubernetes API server\nkubernetesURL: \"https://kubernetes.default\"\n\n# -- The Jenkins credentials to access the Kubernetes API server. For the default cluster it is not needed.\ncredentialsId:\n\n# -- Enables rendering of the helm.sh/chart label to the annotations\nrenderHelmLabels: true\n\n# -- Configures extra labels for the agent all objects\nextraLabels: {}\n\ncontroller:\n  # -- Used for label app.kubernetes.io/component\n  componentName: \"jenkins-controller\"\n  image:\n    # -- Controller image registry\n    registry: \"docker.io\"\n    # -- Controller image repository\n    repository: \"jenkins/jenkins\"\n\n    # -- Controller image tag override; i.e., tag: \"2.440.1-jdk21\"\n    tag:\n\n    # -- Controller image tag label\n    tagLabel: jdk21\n    # -- Controller image pull policy\n    pullPolicy: \"Always\"\n  # -- Controller image pull secret\n  imagePullSecretName:\n  # -- Lifecycle specification for controller-container\n  lifecycle: {}\n  #  postStart:\n  #    exec:\n  #      command:\n  #      - \"uname\"\n  #      - \"-a\"\n\n  # -- Disable use of remember me\n  disableRememberMe: false\n\n  # -- Set Number of executors\n  numExecutors: 0\n\n  # -- Sets the executor mode of the Jenkins node. Possible values are \"NORMAL\" or \"EXCLUSIVE\"\n  executorMode: \"NORMAL\"\n\n  # -- Append Jenkins labels to the controller\n  customJenkinsLabels: []\n\n  hostNetworking: false\n\n  # When enabling LDAP or another non-Jenkins identity source, the built-in admin account will no longer exist.\n  # If you disable the non-Jenkins identity store and instead use the Jenkins internal one,\n  # you should revert controller.admin.username to your preferred admin user:\n  admin:\n    # -- Admin username created as a secret if `controller.admin.createSecret` is true\n    username: \"admin\"\n    # -- Admin password created as a secret if `controller.admin.createSecret` is true\n    # @default -- \u003crandom password\u003e\n    password: \"admin\"\n\n    # -- The key in the existing admin secret containing the username\n    userKey: jenkins-admin-user\n    # -- The key in the existing admin secret containing the password\n    passwordKey: jenkins-admin-password\n\n    # The default configuration uses this secret to configure an admin user\n    # If you don't need that user or use a different security realm, then you can disable it\n    # -- Create secret for admin user\n    createSecret: true\n\n    # -- The name of an existing secret containing the admin credentials\n    existingSecret: \"\"\n  # -- Email address for the administrator of the Jenkins instance\n  jenkinsAdminEmail:\n\n  # This value should not be changed unless you use your custom image of jenkins or any derived from.\n  # If you want to use Cloudbees Jenkins Distribution docker, you should set jenkinsHome: \"/var/cloudbees-jenkins-distribution\"\n  # -- Custom Jenkins home path\n  jenkinsHome: \"/var/jenkins_home\"\n\n  # This value should not be changed unless you use your custom image of jenkins or any derived from.\n  # If you want to use Cloudbees Jenkins Distribution docker, you should set jenkinsRef: \"/usr/share/cloudbees-jenkins-distribution/ref\"\n  # -- Custom Jenkins reference path\n  jenkinsRef: \"/usr/share/jenkins/ref\"\n\n  # Path to the jenkins war file which is used by jenkins-plugin-cli.\n  jenkinsWar: \"/usr/share/jenkins/jenkins.war\"\n  # Override the default arguments passed to the war\n  # overrideArgs:\n  #   - --httpPort=8080\n\n  # -- Resource allocation (Requests and Limits)\n  resources:\n    requests:\n      cpu: \"50m\"\n      memory: \"256Mi\"\n    limits:\n      cpu: \"2000m\"\n      memory: \"4096Mi\"\n\n  # Share process namespace to allow sidecar containers to interact with processes in other containers in the same pod\n  shareProcessNamespace: false\n\n  # Service links might cause issue if running in a namespace with a large amount of services\n  # that might cause a slow startup when plugins are copied from ref to volume\n  # Set to true to keep previous behavior\n  # See https://github.com/kubernetes/kubernetes/issues/121787\n  enableServiceLinks: false\n\n  # Overrides the init container default values\n  # -- Resources allocation (Requests and Limits) for Init Container\n  initContainerResources: {}\n  # initContainerResources:\n  #   requests:\n  #     cpu: \"50m\"\n  #     memory: \"256Mi\"\n  #   limits:\n  #     cpu: \"2000m\"\n  #     memory: \"4096Mi\"\n  # -- Environment variable sources for Init Container\n  initContainerEnvFrom: []\n\n  # useful for i.e., http_proxy\n  # -- Environment variables for Init Container\n  initContainerEnv: []\n  # initContainerEnv:\n  #   - name: http_proxy\n  #     value: \"http://192.168.64.1:3128\"\n\n  # -- Environment variable sources for Jenkins Container\n  containerEnvFrom: []\n\n  # -- Environment variables for Jenkins Container\n  containerEnv: []\n  #   - name: http_proxy\n  #     value: \"http://192.168.64.1:3128\"\n\n  # Set min/max heap here if needed with \"-Xms512m -Xmx512m\"\n  # -- Append to `JAVA_OPTS` env var\n  javaOpts:\n  # -- Append to `JENKINS_OPTS` env var\n  jenkinsOpts:\n\n  # If you are using the ingress definitions provided by this chart via the `controller.ingress` block,\n  # the configured hostname will be the ingress hostname starting with `https://`\n  # or `http://` depending on the `tls` configuration.\n  # The Protocol can be overwritten by specifying `controller.jenkinsUrlProtocol`.\n  # -- Set protocol for Jenkins URL; `https` if `controller.ingress.tls`, `http` otherwise\n  jenkinsUrlProtocol:\n\n  # -- Set Jenkins URL if you are not using the ingress definitions provided by the chart\n  jenkinsUrl:\n\n  # If you set this prefix and use ingress controller, then you might want to set the ingress path below\n  # I.e., \"/jenkins\"\n  # -- Root URI Jenkins will be served on\n  jenkinsUriPrefix:\n\n  # -- Enable pod security context (must be `true` if podSecurityContextOverride, runAsUser or fsGroup are set)\n  usePodSecurityContext: true\n\n  # Note that `runAsUser`, `fsGroup`, and `securityContextCapabilities` are\n  # being deprecated and replaced by `podSecurityContextOverride`.\n  # Set runAsUser to 1000 to let Jenkins run as non-root user 'jenkins', which exists in 'jenkins/jenkins' docker image.\n  # When configuring runAsUser to a different value than 0 also set fsGroup to the same value:\n  # -- Deprecated in favor of `controller.podSecurityContextOverride`. uid that jenkins runs with.\n  runAsUser: 1000\n\n  # -- Deprecated in favor of `controller.podSecurityContextOverride`. uid that will be used for persistent volume.\n  fsGroup: 1000\n\n  # If you have PodSecurityPolicies that require dropping of capabilities as suggested by CIS K8s benchmark, put them here\n  # securityContextCapabilities:\n  #  drop:\n  #    - NET_RAW\n  securityContextCapabilities: {}\n\n  # In the case of mounting an ext4 filesystem, it might be desirable to use `supplementalGroups` instead of `fsGroup` in\n  # the `securityContext` block: https://github.com/kubernetes/kubernetes/issues/67014#issuecomment-589915496\n  # podSecurityContextOverride:\n  #   runAsUser: 1000\n  #   runAsNonRoot: true\n  #   supplementalGroups: [1000]\n  #   capabilities: {}\n  # -- Completely overwrites the contents of the pod security context, ignoring the values provided for `runAsUser`, `fsGroup`, and `securityContextCapabilities`\n  podSecurityContextOverride: ~\n\n  # -- Allow controlling the securityContext for the jenkins container\n  containerSecurityContext:\n    runAsUser: 1000\n    runAsGroup: 1000\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n\n  # -- enable or disable the controller k8s service\n  serviceEnabled: true\n\n  # For minikube, set this to NodePort, elsewhere uses LoadBalancer\n  # Use ClusterIP if your setup includes ingress controller\n  # -- k8s service type\n  serviceType: NodePort\n\n  # -- k8s service clusterIP. Only used if serviceType is ClusterIP\n  clusterIp:\n  # -- k8s service port\n  servicePort: 8080\n  # -- k8s target port\n  targetPort: 8080\n  # -- k8s node port. Only used if serviceType is NodePort\n  nodePort:\n\n  # Use Local to preserve the client source IP and avoids a second hop for LoadBalancer and NodePort type services,\n  # but risks potentially imbalanced traffic spreading.\n  serviceExternalTrafficPolicy:\n\n  # If enabled, the controller is available through its service before its pods reports ready. Makes startup screen and\n  # auto-reload on restart feature possible.\n  publishNotReadyAddresses:\n\n  # -- Jenkins controller service annotations\n  serviceAnnotations: {}\n  # -- Jenkins controller custom labels for the StatefulSet\n  statefulSetLabels: {}\n  #   foo: bar\n  #   bar: foo\n  # -- Labels for the Jenkins controller-service\n  serviceLabels: {}\n  #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: https\n\n  # Put labels on Jenkins controller pod\n  # -- Custom Pod labels (an object with `label-key: label-value` pairs)\n  podLabels: {}\n\n  # Enable Kubernetes Startup, Liveness and Readiness Probes\n  # if Startup Probe is supported, enable it too\n  # ~ 2 minutes to allow Jenkins to restart when upgrading plugins. Set ReadinessTimeout to be shorter than LivenessTimeout.\n  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes\n  # -- Enable Kubernetes Probes configuration configured in `controller.probes`\n  healthProbes: true\n\n  probes:\n    startupProbe:\n      # -- Set the failure threshold for the startup probe\n      failureThreshold: 12\n      httpGet:\n        # -- Set the Pod's HTTP path for the startup probe\n        path: '{{ default \"\" .Values.controller.jenkinsUriPrefix }}/login'\n        # -- Set the Pod's HTTP port to use for the startup probe\n        port: http\n      # -- Set the time interval between two startup probes executions in seconds\n      periodSeconds: 10\n      # -- Set the timeout for the startup probe in seconds\n      timeoutSeconds: 5\n\n    livenessProbe:\n      # -- Set the failure threshold for the liveness probe\n      failureThreshold: 5\n      httpGet:\n        # -- Set the Pod's HTTP path for the liveness probe\n        path: '{{ default \"\" .Values.controller.jenkinsUriPrefix }}/login'\n        # -- Set the Pod's HTTP port to use for the liveness probe\n        port: http\n      # -- Set the time interval between two liveness probes executions in seconds\n      periodSeconds: 10\n      # -- Set the timeout for the liveness probe in seconds\n      timeoutSeconds: 5\n\n      # If Startup Probe is not supported on your Kubernetes cluster, you might want to use \"initialDelaySeconds\" instead.\n      # It delays the initial liveness probe while Jenkins is starting\n      # -- Set the initial delay for the liveness probe in seconds\n      initialDelaySeconds:\n\n    readinessProbe:\n      # -- Set the failure threshold for the readiness probe\n      failureThreshold: 3\n      httpGet:\n        # -- Set the Pod's HTTP path for the liveness probe\n        path: '{{ default \"\" .Values.controller.jenkinsUriPrefix }}/login'\n        # -- Set the Pod's HTTP port to use for the readiness probe\n        port: http\n      # -- Set the time interval between two readiness probes executions in seconds\n      periodSeconds: 10\n      # -- Set the timeout for the readiness probe in seconds\n      timeoutSeconds: 5\n\n      # If Startup Probe is not supported on your Kubernetes cluster, you might want to use \"initialDelaySeconds\" instead.\n      # It delays the initial readiness probe while Jenkins is starting\n      # -- Set the initial delay for the readiness probe in seconds\n      initialDelaySeconds:\n\n  # PodDisruptionBudget config\n  podDisruptionBudget:\n    # ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n\n    # -- Enable Kubernetes Pod Disruption Budget configuration\n    enabled: false\n\n    # For Kubernetes v1.5+, use 'policy/v1beta1'\n    # For Kubernetes v1.21+, use 'policy/v1'\n    # -- Policy API version\n    apiVersion: \"policy/v1beta1\"\n\n    annotations: {}\n    labels: {}\n    # -- Number of pods that can be unavailable. Either an absolute number or a percentage\n    maxUnavailable: \"0\"\n\n  # -- Create Agent listener service\n  agentListenerEnabled: true\n  # -- Listening port for agents\n  agentListenerPort: 50000\n  # -- Host port to listen for agents\n  agentListenerHostPort:\n  # -- Node port to listen for agents\n  agentListenerNodePort:\n\n  # ref: https://kubernetes.io/docs/concepts/services-networking/service/#traffic-policies\n  # -- Traffic Policy of for the agentListener service\n  agentListenerExternalTrafficPolicy:\n  # -- Allowed inbound IP for the agentListener service\n  agentListenerLoadBalancerSourceRanges:\n    - 0.0.0.0/0\n  # -- Disabled agent protocols\n  disabledAgentProtocols:\n    - JNLP-connect\n    - JNLP2-connect\n  csrf:\n    defaultCrumbIssuer:\n      # -- Enable the default CSRF Crumb issuer\n      enabled: true\n      # -- Enable proxy compatibility\n      proxyCompatability: true\n\n  # Kubernetes service type for the JNLP agent service\n  # agentListenerServiceType is the Kubernetes Service type for the JNLP agent service,\n  # either 'LoadBalancer', 'NodePort', or 'ClusterIP'\n  # Note if you set this to 'LoadBalancer', you *must* define annotations to secure it. By default,\n  # this will be an external load balancer and allowing inbound 0.0.0.0/0, a HUGE\n  # security risk: https://github.com/kubernetes/charts/issues/1341\n  # -- Defines how to expose the agentListener service\n  agentListenerServiceType: \"ClusterIP\"\n\n  # -- Annotations for the agentListener service\n  agentListenerServiceAnnotations: {}\n\n  # Optionally, assign an IP to the LoadBalancer agentListenerService LoadBalancer\n  # GKE users: only regional static IPs will work for Service Load balancer.\n  # -- Static IP for the agentListener LoadBalancer\n  agentListenerLoadBalancerIP:\n\n  # -- Whether legacy remoting security should be enabled\n  legacyRemotingSecurityEnabled: false\n\n  # Example of a 'LoadBalancer'-type agent listener with annotations securing it\n  # agentListenerServiceType: LoadBalancer\n  # agentListenerServiceAnnotations:\n  #   service.beta.kubernetes.io/aws-load-balancer-internal: \"True\"\n  #   service.beta.kubernetes.io/load-balancer-source-ranges: \"172.0.0.0/8, 10.0.0.0/8\"\n\n  # LoadBalancerSourcesRange is a list of allowed CIDR values, which are combined with ServicePort to\n  # set allowed inbound rules on the security group assigned to the controller load balancer\n  # -- Allowed inbound IP addresses\n  loadBalancerSourceRanges:\n    - 0.0.0.0/0\n\n  # -- Optionally assign a known public LB IP\n  loadBalancerIP:\n\n  # Optionally configure a JMX port. This requires additional javaOpts, for example,\n  # javaOpts: \u003e\n  #   -Dcom.sun.management.jmxremote.port=4000\n  #   -Dcom.sun.management.jmxremote.authenticate=false\n  #   -Dcom.sun.management.jmxremote.ssl=false\n  # jmxPort: 4000\n  # -- Open a port, for JMX stats\n  jmxPort:\n\n  # -- Optionally configure other ports to expose in the controller container\n  extraPorts: []\n  # - name: BuildInfoProxy\n  #   port: 9000\n  #   targetPort: 9010 (Optional: Use to explicitly set targetPort if different from port)\n\n  # Plugins will be installed during Jenkins controller start\n  # -- List of Jenkins plugins to install. If you don't want to install plugins, set it to `false`\n  installPlugins:\n    - kubernetes:4384.v1b_6367f393d9\n    - workflow-aggregator:608.v67378e9d3db_1\n    - git:5.8.0\n    - configuration-as-code:2006.v001a_2ca_6b_574\n\n  # If set to false, Jenkins will download the minimum required version of all dependencies.\n  # -- Download the minimum required version or latest version of all dependencies\n  installLatestPlugins: true\n\n  # -- Set to true to download the latest version of any plugin that is requested to have the latest version\n  installLatestSpecifiedPlugins: false\n\n  # -- List of plugins to install in addition to those listed in controller.installPlugins\n  additionalPlugins: []\n\n  # Without this; whenever the controller gets restarted (Evicted, etc.) it will fetch plugin updates that have the potential to cause breakage.\n  # Note that for this to work, `persistence.enabled` needs to be set to `true`\n  # -- Initialize only on first installation. Ensures plugins do not get updated inadvertently. Requires `persistence.enabled` to be set to `true`\n  initializeOnce: false\n\n  # Enable to always override the installed plugins with the values of 'controller.installPlugins' on upgrade or redeployment.\n  # -- Overwrite installed plugins on start\n  overwritePlugins: false\n\n  # Configures if plugins bundled with `controller.image` should be overwritten with the values of 'controller.installPlugins' on upgrade or redeployment.\n  # -- Overwrite plugins that are already installed in the controller image\n  overwritePluginsFromImage: true\n\n  # Configures the restrictions for naming projects. Set this key to null or empty to skip it in the default config.\n  projectNamingStrategy: standard\n\n  # Useful with ghprb plugin. The OWASP plugin is not installed by default, please update controller.installPlugins.\n  # -- Enable HTML parsing using OWASP Markup Formatter Plugin (antisamy-markup-formatter)\n  enableRawHtmlMarkupFormatter: false\n\n  # This is ignored if enableRawHtmlMarkupFormatter is true\n  # -- Yaml of the markup formatter to use\n  markupFormatter: plainText\n\n  # Used to approve a list of groovy functions in pipelines used the script-security plugin. Can be viewed under /scriptApproval\n  # -- List of groovy functions to approve\n  scriptApproval: []\n  #  - \"method groovy.json.JsonSlurperClassic parseText java.lang.String\"\n  #  - \"new groovy.json.JsonSlurperClassic\"\n\n  # -- Map of groovy init scripts to be executed during Jenkins controller start\n  initScripts: {}\n  #  test: |-\n  #    print 'adding global pipeline libraries, register properties, bootstrap jobs...'\n  # -- Name of the existing ConfigMap that contains init scripts\n  initConfigMap:\n\n  # 'name' is a name of an existing secret in the same namespace as jenkins,\n  # 'keyName' is the name of one of the keys inside the current secret.\n  # the 'name' and 'keyName' are concatenated with a '-' in between, so for example:\n  # an existing secret \"secret-credentials\" and a key inside it named \"github-password\" should be used in JCasC as ${secret-credentials-github-password}\n  # 'name' and 'keyName' must be lowercase RFC 1123 label must consist of lower case alphanumeric characters or '-',\n  # and must start and end with an alphanumeric character (e.g. 'my-name', or '123-abc')\n  # existingSecret existing secret \"secret-credentials\" and a key inside it named \"github-username\" should be used in JCasC as ${github-username}\n  # When using existingSecret no need to specify the keyName under additionalExistingSecrets.\n  existingSecret:\n\n  # -- List of additional existing secrets to mount\n  additionalExistingSecrets: []\n  # ref: https://github.com/jenkinsci/configuration-as-code-plugin/blob/master/docs/features/secrets.adoc#kubernetes-secrets\n  # additionalExistingSecrets:\n  #  - name: secret-name-1\n  #    keyName: username\n  #  - name: secret-name-1\n  #    keyName: password\n\n  # -- List of additional secrets to create and mount\n  additionalSecrets: []\n  # ref: https://github.com/jenkinsci/configuration-as-code-plugin/blob/master/docs/features/secrets.adoc#kubernetes-secrets\n  # additionalSecrets:\n  #  - name: nameOfSecret\n  #    value: secretText\n\n  # Generate SecretClaim resources to create Kubernetes secrets from HashiCorp Vault using kube-vault-controller.\n  # 'name' is the name of the secret that will be created in Kubernetes. The Jenkins fullname is prepended to this value.\n  # 'path' is the fully qualified path to the secret in Vault\n  # 'type' is an optional Kubernetes secret type. The default is 'Opaque'\n  # 'renew' is an optional secret renewal time in seconds\n  # -- List of `SecretClaim` resources to create\n  secretClaims: []\n  # - name: secretName        # required\n  #   path: testPath          # required\n  #   type: kubernetes.io/tls # optional\n  #   renew: 60               # optional\n\n  # -- Name of default cloud configuration.\n  cloudName: \"kubernetes\"\n\n  # Below is the implementation of Jenkins Configuration as Code. Add a key under configScripts for each configuration area,\n  # where each corresponds to a plugin or section of the UI. Each key (prior to | character) is just a label, and can be any value.\n  # Keys are only used to give the section a meaningful name. The only restriction is they may only contain RFC 1123 \\ DNS label\n  # characters: lowercase letters, numbers, and hyphens. The keys become the name of a configuration yaml file on the controller in\n  # /var/jenkins_home/casc_configs (by default) and will be processed by the Configuration as Code Plugin. The lines after each |\n  # become the content of the configuration yaml file. The first line after this is a JCasC root element, e.g., jenkins, credentials,\n  # etc. Best reference is https://\u003cjenkins_url\u003e/configuration-as-code/reference. The example below creates a welcome message:\n  JCasC:\n    # -- Enables default Jenkins configuration via configuration as code plugin\n    defaultConfig: true\n\n    # If true, the init container deletes all the plugin config files and Jenkins Config as Code overwrites any existing configuration\n    # -- Whether Jenkins Config as Code should overwrite any existing configuration\n    overwriteConfiguration: false\n    # -- Remote URLs for configuration files.\n    configUrls: []\n    # - https://acme.org/jenkins.yaml\n    # -- List of Jenkins Config as Code scripts\n    configScripts: {}\n    #  welcome-message: |\n    #    jenkins:\n    #      systemMessage: Welcome to our CI\\CD server. This Jenkins is configured and managed 'as code'.\n\n    # Allows adding to the top-level security JCasC section. For legacy purposes, by default, the chart includes apiToken configurations\n    # -- Jenkins Config as Code security-section\n    security:\n      apiToken:\n        creationOfLegacyTokenEnabled: false\n        tokenGenerationOnCreationEnabled: false\n        usageStatisticsEnabled: true\n\n    # Ignored if securityRealm is defined in controller.JCasC.configScripts\n    # -- Jenkins Config as Code Security Realm-section\n    securityRealm: |-\n      local:\n        allowsSignup: false\n        enableCaptcha: false\n        users:\n        - id: \"${chart-admin-username}\"\n          name: \"Jenkins Admin\"\n          password: \"${chart-admin-password}\"\n\n    # Ignored if authorizationStrategy is defined in controller.JCasC.configScripts\n    # -- Jenkins Config as Code Authorization Strategy-section\n    authorizationStrategy: |-\n      loggedInUsersCanDoAnything:\n        allowAnonymousRead: false\n\n    # -- Annotations for the JCasC ConfigMap\n    configMapAnnotations: {}\n\n  # -- Custom init-container specification in raw-yaml format\n  customInitContainers: []\n  # - name: custom-init\n  #   image: \"alpine:3\"\n  #   imagePullPolicy: Always\n  #   command: [ \"uname\", \"-a\" ]\n\n  sidecars:\n    configAutoReload:\n      # If enabled: true, Jenkins Configuration as Code will be reloaded on-the-fly without a reboot.\n      # If false or not-specified, JCasC changes will cause a reboot and will only be applied at the subsequent start-up.\n      # Auto-reload uses the http://\u003cjenkins_url\u003e/reload-configuration-as-code endpoint to reapply config when changes to\n      # the configScripts are detected.\n      # -- Enable Jenkins Config as Code auto-reload\n      enabled: true\n      image:\n        # -- Registry for the image that triggers the reload\n        registry: docker.io\n        # -- Repository of the image that triggers the reload\n        repository: kiwigrid/k8s-sidecar\n        # -- Tag for the image that triggers the reload\n        tag: 1.30.7\n      imagePullPolicy: IfNotPresent\n      resources:\n        {}\n        #   limits:\n        #     cpu: 100m\n        #     memory: 100Mi\n        #   requests:\n        #     cpu: 50m\n        #     memory: 50Mi\n      # -- Enables additional volume mounts for the config auto-reload container\n      additionalVolumeMounts:\n        []\n        #   - name: auto-reload-config\n        #     mountPath: /var/config/logger\n        #   - name: auto-reload-logs\n        #     mountPath: /var/log/auto_reload\n      # -- Config auto-reload logging settings\n      logging:\n        # See default settings https://github.com/kiwigrid/k8s-sidecar/blob/master/src/logger.py\n        configuration:\n          # -- Enables custom log config utilizing using the settings below.\n          override: false\n          logLevel: INFO\n          formatter: JSON\n          logToConsole: true\n          logToFile: false\n          maxBytes: 1024\n          backupCount: 3\n\n      # -- The scheme to use when connecting to the Jenkins configuration as code endpoint\n      scheme: http\n      # -- Skip TLS verification when connecting to the Jenkins configuration as code endpoint\n      skipTlsVerify: false\n\n      # -- How many connection-related errors to retry on\n      reqRetryConnect: 10\n      # -- How many seconds to wait before updating config-maps/secrets (sets METHOD=SLEEP on the sidecar)\n      sleepTime:\n\n      # -- Environment variable sources for the Jenkins Config as Code auto-reload container\n      envFrom: []\n      # -- Environment variables for the Jenkins Config as Code auto-reload container\n      env: []\n      #   - name: REQ_TIMEOUT\n      #     value: \"30\"\n\n      # SSH port value can be set to any unused TCP port. The default, 1044, is a non-standard SSH port that has been chosen at random.\n      # This is only used to reload JCasC config from the sidecar container running in the Jenkins controller pod.\n      # This TCP port will not be open in the pod (unless you specifically configure this), so Jenkins will not be\n      # accessible via SSH from outside the pod. Note if you use non-root pod privileges (runAsUser \u0026 fsGroup),\n      # this must be \u003e 1024:\n      sshTcpPort: 1044\n      # folder in the pod that should hold the collected dashboards:\n      folder: \"/var/jenkins_home/casc_configs\"\n\n      # If specified, the sidecar will search for JCasC config-maps inside this namespace.\n      # Otherwise, the namespace in which the sidecar is running will be used.\n      # It's also possible to specify ALL to search in all namespaces:\n      # searchNamespace:\n      # -- Enable container security context\n      containerSecurityContext:\n        readOnlyRootFilesystem: true\n        allowPrivilegeEscalation: false\n\n    # -- Configures additional sidecar container(s) for the Jenkins controller\n    additionalSidecarContainers: []\n    ## The example below runs the client for https://smee.io as sidecar container next to Jenkins,\n    ## that allows triggering build behind a secure firewall.\n    ## https://jenkins.io/blog/2019/01/07/webhook-firewalls/#triggering-builds-with-webhooks-behind-a-secure-firewall\n    ##\n    ## Note: To use it you should go to https://smee.io/new and update the url to the generated one.\n    # - name: smee\n    #   image: docker.io/twalter/smee-client:1.0.2\n    #   args: [\"--port\", \"{{ .Values.controller.servicePort }}\", \"--path\", \"/github-webhook/\", \"--url\", \"https://smee.io/new\"]\n    #   resources:\n    #     limits:\n    #       cpu: 50m\n    #       memory: 128Mi\n    #     requests:\n    #       cpu: 10m\n    #       memory: 32Mi\n\n  # -- Name of the Kubernetes scheduler to use\n  schedulerName: \"\"\n\n  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  # -- Node labels for pod assignment\n  nodeSelector: {}\n\n  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature\n  # -- Toleration labels for pod assignment\n  tolerations: []\n  # -- Set TerminationGracePeriodSeconds\n  terminationGracePeriodSeconds:\n  # -- Set the termination message path\n  terminationMessagePath:\n  # -- Set the termination message policy\n  terminationMessagePolicy:\n\n  # -- Affinity settings\n  affinity: {}\n\n  # Leverage a priorityClass to ensure your pods survive resource shortages\n  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n  # -- The name of a `priorityClass` to apply to the controller pod\n  priorityClassName:\n\n  # -- Annotations for controller pod\n  podAnnotations: {}\n  # -- Annotations for controller StatefulSet\n  statefulSetAnnotations: {}\n\n  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  # -- Update strategy for StatefulSet\n  updateStrategy: {}\n\n  # -- Topology spread constraints\n  topologySpreadConstraints: {}\n\n  # -- DNS config for the pod\n  dnsConfig: {}\n\n  ingress:\n    # -- Enables ingress\n    enabled: false\n\n    # Override for the default paths that map requests to the backend\n    # -- Override for the default Ingress paths\n    paths: []\n    # - backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n    # - backend:\n    #     serviceName: \u003e-\n    #       {{ template \"jenkins.fullname\" . }}\n    #     # Don't use string here, use only integer value!\n    #     servicePort: 8080\n\n    # For Kubernetes v1.14+, use 'networking.k8s.io/v1beta1'\n    # For Kubernetes v1.19+, use 'networking.k8s.io/v1'\n    # -- Ingress API version\n    apiVersion: \"extensions/v1beta1\"\n    # -- Ingress labels\n    labels: {}\n    # -- Ingress annotations\n    annotations:\n      {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    # Set this path to jenkinsUriPrefix above or use annotations to rewrite path\n    # -- Ingress path\n    path:\n\n    # configures the hostname e.g. jenkins.example.com\n    # -- Ingress hostname\n    hostName:\n    # -- Hostname to serve assets from\n    resourceRootUrl:\n    # -- Ingress TLS configuration\n    tls: []\n    # - secretName: jenkins.cluster.local\n    #   hosts:\n    #     - jenkins.cluster.local\n\n  # often you want to have your controller all locked down and private,\n  # but you still want to get webhooks from your SCM\n  # A secondary ingress will let you expose different urls\n  # with a different configuration\n  secondaryingress:\n    enabled: false\n    # paths you want forwarded to the backend\n    # ex /github-webhook\n    paths: []\n    # For Kubernetes v1.14+, use 'networking.k8s.io/v1beta1'\n    # For Kubernetes v1.19+, use 'networking.k8s.io/v1'\n    apiVersion: \"extensions/v1beta1\"\n    labels: {}\n    annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n    # configures the hostname e.g., jenkins-external.example.com\n    hostName:\n    tls:\n    # - secretName: jenkins-external.example.com\n    #   hosts:\n    #     - jenkins-external.example.com\n\n  # If you're running on GKE and need to configure a backendconfig\n  # to finish ingress setup, use the following values.\n  # Docs: https://cloud.google.com/kubernetes-engine/docs/concepts/backendconfig\n  backendconfig:\n    # -- Enables backendconfig\n    enabled: false\n    # -- backendconfig API version\n    apiVersion: \"extensions/v1beta1\"\n    # -- backendconfig name\n    name:\n    # -- backendconfig labels\n    labels: {}\n    # -- backendconfig annotations\n    annotations: {}\n    # -- backendconfig spec\n    spec: {}\n\n  # Openshift route\n  route:\n    # -- Enables openshift route\n    enabled: false\n    # -- Route labels\n    labels: {}\n    # -- Route annotations\n    annotations: {}\n    # -- Route path\n    path:\n\n  # -- Allows for adding entries to Pod /etc/hosts\n  hostAliases: []\n  # ref: https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  # hostAliases:\n  # - ip: 192.168.50.50\n  #   hostnames:\n  #     - something.local\n  # - ip: 10.0.50.50\n  #   hostnames:\n  #     - other.local\n\n  # Expose Prometheus metrics\n  prometheus:\n    # If enabled, add the prometheus plugin to the list of plugins to install\n    # https://plugins.jenkins.io/prometheus\n\n    # -- Enables prometheus service monitor\n    enabled: false\n    # -- Additional labels to add to the service monitor object\n    serviceMonitorAdditionalLabels: {}\n    # -- Set a custom namespace where to deploy ServiceMonitor resource\n    serviceMonitorNamespace:\n    # -- How often prometheus should scrape metrics\n    scrapeInterval: 60s\n\n    # Defaults to the default endpoint used by the prometheus plugin\n    # -- The endpoint prometheus should get metrics from\n    scrapeEndpoint: /prometheus\n\n    # See here: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n    # The `groups` root object is added by default, add the rule entries\n    # -- Array of prometheus alerting rules\n    alertingrules: []\n    # -- Additional labels to add to the PrometheusRule object\n    alertingRulesAdditionalLabels: {}\n    # -- Set a custom namespace where to deploy PrometheusRule resource\n    prometheusRuleNamespace: \"\"\n\n    # RelabelConfigs to apply to samples before scraping. Prometheus Operator automatically adds\n    # relabelings for a few standard Kubernetes fields. The original scrape jobs name\n    # is available via the __tmp_prometheus_job_name label.\n    # More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config\n    relabelings: []\n    # MetricRelabelConfigs to apply to samples before ingestion.\n    metricRelabelings: []\n\n  googlePodMonitor:\n    # If enabled, It creates Google Managed Prometheus scraping config\n    enabled: false\n    # Set a custom namespace where to deploy PodMonitoring resource\n    # serviceMonitorNamespace: \"\"\n    scrapeInterval: 60s\n    # This is the default endpoint used by the prometheus plugin\n    scrapeEndpoint: /prometheus\n\n  # -- Can be used to disable rendering controller test resources when using helm template\n  testEnabled: true\n\n  httpsKeyStore:\n    # -- Enables HTTPS keystore on jenkins controller\n    enable: false\n    # -- Name of the secret that already has SSL keystore\n    jenkinsHttpsJksSecretName: \"\"\n    # -- Name of the key in the secret that already has SSL keystore\n    jenkinsHttpsJksSecretKey: \"jenkins-jks-file\"\n    # -- Name of the secret that contains the JKS password, if it is not in the same secret as the JKS file\n    jenkinsHttpsJksPasswordSecretName: \"\"\n    # -- Name of the key in the secret that contains the JKS password\n    jenkinsHttpsJksPasswordSecretKey: \"https-jks-password\"\n    disableSecretMount: false\n\n    # When HTTPS keystore is enabled, servicePort and targetPort will be used as HTTPS port\n    # -- HTTP Port that Jenkins should listen to along with HTTPS, it also serves as the liveness and readiness probes port.\n    httpPort: 8081\n    # -- Path of HTTPS keystore file\n    path: \"/var/jenkins_keystore\"\n    # -- Jenkins keystore filename which will appear under controller.httpsKeyStore.path\n    fileName: \"keystore.jks\"\n    # -- Jenkins keystore password\n    password: \"password\"\n\n    # -- Base64 encoded Keystore content. Keystore must be converted to base64 then being pasted here\n    jenkinsKeyStoreBase64Encoded:\n    # Convert keystore.jks files content to base64 \u003e $ cat keystore.jks | base64\n#        /u3+7QAAAAIAAAABAAAAAQANamVua2luc2NpLmNvbQAAAW2r/b1ZAAAFATCCBP0wDgYKKwYBBAEq\n#        AhEBAQUABIIE6QbCqasvoHS0pSwYqSvdydMCB9t+VNfwhFIiiuAelJfO5sSe2SebJbtwHgLcRz1Z\n#        gMtWgOSFdl3bWSzA7vrW2LED52h+jXLYSWvZzuDuh8hYO85m10ikF6QR+dTi4jra0whIFDvq3pxe\n#        TnESxEsN+DvbZM3jA3qsjQJSeISNpDjO099dqQvHpnCn18lyk7J4TWJ8sOQQb1EM2zDAfAOSqA/x\n#        QuPEFl74DlY+5DIk6EBvpmWhaMSvXzWZACGA0sYqa157dq7O0AqmuLG/EI5EkHETO4CrtBW+yLcy\n#        2dUCXOMA+j+NjM1BjrQkYE5vtSfNO6lFZcISyKo5pTFlcA7ut0Fx2nZ8GhHTn32CpeWwNcZBn1gR\n#        pZVt6DxVVkhTAkMLhR4rL2wGIi/1WRs23ZOLGKtyDNvDHnQyDiQEoJGy9nAthA8aNHa3cfdF10vB\n#        Drb19vtpFHmpvKEEhpk2EBRF4fTi644Fuhu2Ied6118AlaPvEea+n6G4vBz+8RWuVCmZjLU+7h8l\n#        Hy3/WdUPoIL5eW7Kz+hS+sRTFzfu9C48dMkQH3a6f3wSY+mufizNF9U298r98TnYy+PfDJK0bstG\n#        Ph6yPWx8DGXKQBwrhWJWXI6JwZDeC5Ny+l8p1SypTmAjpIaSW3ge+KgcL6Wtt1R5hUV1ajVwVSUi\n#        HF/FachKqPqyLJFZTGjNrxnmNYpt8P1d5JTvJfmfr55Su/P9n7kcyWp7zMcb2Q5nlXt4tWogOHLI\n#        OzEWKCacbFfVHE+PpdrcvCVZMDzFogIq5EqGTOZe2poPpBVE+1y9mf5+TXBegy5HToLWvmfmJNTO\n#        NCDuBjgLs2tdw2yMPm4YEr57PnMX5gGTC3f2ZihXCIJDCRCdQ9sVBOjIQbOCzxFXkVITo0BAZhCi\n#        Yz61wt3Ud8e//zhXWCkCsSV+IZCxxPzhEFd+RFVjW0Nm9hsb2FgAhkXCjsGROgoleYgaZJWvQaAg\n#        UyBzMmKDPKTllBHyE3Gy1ehBNGPgEBChf17/9M+j8pcm1OmlM434ctWQ4qW7RU56//yq1soFY0Te\n#        fu2ei03a6m68fYuW6s7XEEK58QisJWRAvEbpwu/eyqfs7PsQ+zSgJHyk2rO95IxdMtEESb2GRuoi\n#        Bs+AHNdYFTAi+GBWw9dvEgqQ0Mpv0//6bBE/Fb4d7b7f56uUNnnE7mFnjGmGQN+MvC62pfwfvJTT\n#        EkT1iZ9kjM9FprTFWXT4UmO3XTvesGeE50sV9YPm71X4DCQwc4KE8vyuwj0s6oMNAUACW2ClU9QQ\n#        y0tRpaF1tzs4N42Q5zl0TzWxbCCjAtC3u6xf+c8MCGrr7DzNhm42LOQiHTa4MwX4x96q7235oiAU\n#        iQqSI/hyF5yLpWw4etyUvsx2/0/0wkuTU1FozbLoCWJEWcPS7QadMrRRISxHf0YobIeQyz34regl\n#        t1qSQ3dCU9D6AHLgX6kqllx4X0fnFq7LtfN7fA2itW26v+kAT2QFZ3qZhINGfofCja/pITC1uNAZ\n#        gsJaTMcQ600krj/ynoxnjT+n1gmeqThac6/Mi3YlVeRtaxI2InL82ZuD+w/dfY9OpPssQjy3xiQa\n#        jPuaMWXRxz/sS9syOoGVH7XBwKrWpQcpchozWJt40QV5DslJkclcr8aC2AGlzuJMTdEgz1eqV0+H\n#        bAXG9HRHN/0eJTn1/QAAAAEABVguNTA5AAADjzCCA4swggJzAhRGqVxH4HTLYPGO4rzHcCPeGDKn\n#        xTANBgkqhkiG9w0BAQsFADCBgTELMAkGA1UEBhMCY2ExEDAOBgNVBAgMB29udGFyaW8xEDAOBgNV\n#        BAcMB3Rvcm9udG8xFDASBgNVBAoMC2plbmtpbnN0ZXN0MRkwFwYDVQQDDBBqZW5raW5zdGVzdC5p\n#        bmZvMR0wGwYJKoZIhvcNAQkBFg50ZXN0QHRlc3QuaW5mbzAeFw0xOTEwMDgxNTI5NTVaFw0xOTEx\n#        MDcxNTI5NTVaMIGBMQswCQYDVQQGEwJjYTEQMA4GA1UECAwHb250YXJpbzEQMA4GA1UEBwwHdG9y\n#        b250bzEUMBIGA1UECgwLamVua2luc3Rlc3QxGTAXBgNVBAMMEGplbmtpbnN0ZXN0LmluZm8xHTAb\n#        BgkqhkiG9w0BCQEWDnRlc3RAdGVzdC5pbmZvMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC\n#        AQEA02q352JTHGvROMBhSHvSv+vnoOTDKSTz2aLQn0tYrIRqRo+8bfmMjXuhkwZPSnCpvUGNAJ+w\n#        Jrt/dqMoYUjCBkjylD/qHmnXN5EwS1cMg1Djh65gi5JJLFJ7eNcoSsr/0AJ+TweIal1jJSP3t3PF\n#        9Uv21gm6xdm7HnNK66WpUUXLDTKaIs/jtagVY1bLOo9oEVeLN4nT2CYWztpMvdCyEDUzgEdDbmrP\n#        F5nKUPK5hrFqo1Dc5rUI4ZshL3Lpv398aMxv6n2adQvuL++URMEbXXBhxOrT6rCtYzbcR5fkwS9i\n#        d3Br45CoWOQro02JAepoU0MQKY5+xQ4Bq9Q7tB9BAwIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQAe\n#        4xc+mSvKkrKBHg9/zpkWgZUiOp4ENJCi8H4tea/PCM439v6y/kfjT/okOokFvX8N5aa1OSz2Vsrl\n#        m8kjIc6hiA7bKzT6lb0EyjUShFFZ5jmGVP4S7/hviDvgB5yEQxOPpumkdRP513YnEGj/o9Pazi5h\n#        /MwpRxxazoda9r45kqQpyG+XoM4pB+Fd3JzMc4FUGxfVPxJU4jLawnJJiZ3vqiSyaB0YyUL+Er1Q\n#        6NnqtR4gEBF0ZVlQmkycFvD4EC2boP943dLqNUvop+4R3SM1QMM6P5u8iTXtHd/VN4MwMyy1wtog\n#        hYAzODo1Jt59pcqqKJEas0C/lFJEB3frw4ImNx5fNlJYOpx+ijfQs9m39CevDq0=\n\nagent:\n  # -- Enable Kubernetes plugin jnlp-agent podTemplate\n  enabled: true\n  # -- The name of the pod template to use for providing default values\n  defaultsProviderTemplate: \"\"\n\n  # Useful for not including a serviceAccount in the template if `false`\n  # -- Use `serviceAccountAgent.name` as the default value for defaults template `serviceAccount`\n  useDefaultServiceAccount: true\n\n  # -- Override the default service account\n  # @default -- `serviceAccountAgent.name` if `agent.useDefaultServiceAccount` is `true`\n  serviceAccount:\n\n  # For connecting to the Jenkins controller\n  # -- Overrides the Kubernetes Jenkins URL\n  jenkinsUrl:\n\n  # connects to the specified host and port, instead of connecting directly to the Jenkins controller\n  # -- Overrides the Kubernetes Jenkins tunnel\n  jenkinsTunnel:\n  # -- Disables the verification of the controller certificate on remote connection. This flag correspond to the \"Disable https certificate check\" flag in kubernetes plugin UI\n  skipTlsVerify: false\n  # -- Enable the possibility to restrict the usage of this agent to specific folder. This flag correspond to the \"Restrict pipeline support to authorized folders\" flag in kubernetes plugin UI\n  usageRestricted: false\n  # -- The connection timeout in seconds for connections to Kubernetes API. The minimum value is 5\n  kubernetesConnectTimeout: 5\n  # -- The read timeout in seconds for connections to Kubernetes API. The minimum value is 15\n  kubernetesReadTimeout: 15\n  # -- The maximum concurrent connections to Kubernetes API\n  maxRequestsPerHostStr: \"32\"\n  # -- Time in minutes after which the Kubernetes cloud plugin will clean up an idle worker that has not already terminated\n  retentionTimeout: 5\n  # -- Seconds to wait for pod to be running\n  waitForPodSec: 600\n  # -- Namespace in which the Kubernetes agents should be launched\n  namespace:\n  # -- Custom Pod labels (an object with `label-key: label-value` pairs)\n  podLabels: {}\n  # -- Custom registry used to pull the agent jnlp image from\n  jnlpregistry:\n  image:\n    # -- Registry to pull the agent jnlp image from\n    registry: \"\"\n    # -- Repository to pull the agent jnlp image from\n    repository: \"jenkins/inbound-agent\"\n    # -- Tag of the image to pull\n    tag: \"3345.v03dee9b_f88fc-2\"\n  # -- Configure working directory for default agent\n  workingDir: \"/home/jenkins/agent\"\n  nodeUsageMode: \"NORMAL\"\n  # -- Append Jenkins labels to the agent\n  customJenkinsLabels: []\n  # -- Name of the secret to be used to pull the image\n  imagePullSecretName:\n  componentName: \"jenkins-agent\"\n  # -- Enables agent communication via websockets\n  websocket: false\n  directConnection: false\n  # -- Agent privileged container\n  privileged: false\n  # -- Configure container user\n  runAsUser:\n  # -- Configure container group\n  runAsGroup:\n  # -- Enables the agent to use the host network\n  hostNetworking: false\n  # -- Resources allocation (Requests and Limits)\n  resources:\n    requests:\n      cpu: \"512m\"\n      memory: \"512Mi\"\n      # ephemeralStorage:\n    limits:\n      cpu: \"512m\"\n      memory: \"512Mi\"\n      # ephemeralStorage:\n  livenessProbe: {}\n  #  execArgs: \"cat /tmp/healthy\"\n  #  failureThreshold: 3\n  #  initialDelaySeconds: 0\n  #  periodSeconds: 10\n  #  successThreshold: 1\n  #  timeoutSeconds: 1\n\n  # You may want to change this to true while testing a new image\n  # -- Always pull agent container image before build\n  alwaysPullImage: false\n  # When using Pod Security Admission in the Agents namespace with the restricted Pod Security Standard,\n  # the jnlp container cannot be scheduled without overriding its container definition with a securityContext.\n  # This option allows to automatically inject in the jnlp container a securityContext\n  # that is suitable for the use of the restricted Pod Security Standard.\n  # -- Set a restricted securityContext on jnlp containers\n  restrictedPssSecurityContext: false\n  # Controls how agent pods are retained after the Jenkins build completes\n  # Possible values: Always, Never, OnFailure\n  podRetention: \"Never\"\n  # Disable if you do not want the Yaml the agent pod template to show up\n  # in the job Console Output. This can be helpful for either security reasons\n  # or simply to clean up the output to make it easier to read.\n  showRawYaml: true\n\n  # You can define the volumes that you want to mount for this container\n  # Allowed types are: ConfigMap, EmptyDir, EphemeralVolume, HostPath, Nfs, PVC, Secret\n  # Configure the attributes as they appear in the corresponding Java class for that type\n  # https://github.com/jenkinsci/kubernetes-plugin/tree/master/src/main/java/org/csanchez/jenkins/plugins/kubernetes/volumes\n  # -- Additional volumes\n  volumes: []\n  # - type: ConfigMap\n  #   configMapName: myconfigmap\n  #   mountPath: /var/myapp/myconfigmap\n  # - type: EmptyDir\n  #   mountPath: /var/myapp/myemptydir\n  #   memory: false\n  # - type: EphemeralVolume\n  #   mountPath: /var/myapp/myephemeralvolume\n  #   accessModes: ReadWriteOnce\n  #   requestsSize: 10Gi\n  #   storageClassName: mystorageclass\n  # - type: HostPath\n  #   hostPath: /var/lib/containers\n  #   mountPath: /var/myapp/myhostpath\n  # - type: Nfs\n  #   mountPath: /var/myapp/mynfs\n  #   readOnly: false\n  #   serverAddress: \"192.0.2.0\"\n  #   serverPath: /var/lib/containers\n  # - type: PVC\n  #   claimName: mypvc\n  #   mountPath: /var/myapp/mypvc\n  #   readOnly: false\n  # - type: Secret\n  #   defaultMode: \"600\"\n  #   mountPath: /var/myapp/mysecret\n  #   secretName: mysecret\n  # Pod-wide environment, these vars are visible to any container in the agent pod\n\n  # You can define the workspaceVolume that you want to mount for this container\n  # Allowed types are: DynamicPVC, EmptyDir, EphemeralVolume, HostPath, Nfs, PVC\n  # Configure the attributes as they appear in the corresponding Java class for that type\n  # https://github.com/jenkinsci/kubernetes-plugin/tree/master/src/main/java/org/csanchez/jenkins/plugins/kubernetes/volumes/workspace\n  # -- Workspace volume (defaults to EmptyDir)\n  workspaceVolume: {}\n  ## DynamicPVC example\n  # - type: DynamicPVC\n  #   configMapName: myconfigmap\n  ## EmptyDir example\n  # - type: EmptyDir\n  #   memory: false\n  ## EphemeralVolume example\n  # - type: EphemeralVolume\n  #   accessModes: ReadWriteOnce\n  #   requestsSize: 10Gi\n  #   storageClassName: mystorageclass\n  ## HostPath example\n  # - type: HostPath\n  #   hostPath: /var/lib/containers\n  ## NFS example\n  # - type: Nfs\n  #   readOnly: false\n  #   serverAddress: \"192.0.2.0\"\n  #   serverPath: /var/lib/containers\n  ## PVC example\n  # - type: PVC\n  #   claimName: mypvc\n  #   readOnly: false\n\n  # Pod-wide environment, these vars are visible to any container in the agent pod\n  # -- Environment variables for the agent Pod\n  envVars: []\n  # - name: PATH\n  #   value: /usr/local/bin\n  # -- Mount a secret as environment variable\n  secretEnvVars: []\n  # - key: PATH\n  #   optional: false # default: false\n  #   secretKey: MY-K8S-PATH\n  #   secretName: my-k8s-secret\n\n  # -- Node labels for pod assignment\n  nodeSelector: {}\n  # Key Value selectors. Ex:\n  # nodeSelector\n  #   jenkins-agent: v1\n\n  # -- Command to execute when side container starts\n  command:\n  # -- Arguments passed to command to execute\n  args: \"${computer.jnlpmac} ${computer.name}\"\n  # -- Side container name\n  sideContainerName: \"jnlp\"\n\n  # Doesn't allocate pseudo TTY by default\n  # -- Allocate pseudo tty to the side container\n  TTYEnabled: false\n  # -- Max number of agents to launch for a whole cluster.\n  containerCap: 10\n  # -- Max number of agents to launch for this type of agent\n  instanceCap: 2147483647\n  # -- Agent Pod base name\n  podName: \"default\"\n\n  # Enables garbage collection of orphan pods for this Kubernetes cloud. (beta)\n  garbageCollection:\n    # -- When enabled, Jenkins will periodically check for orphan pods that have not been touched for the given timeout period and delete them.\n    enabled: false\n    # -- Namespaces to look at for garbage collection, in addition to the default namespace defined for the cloud. One namespace per line.\n    namespaces: \"\"\n    # namespaces: |-\n    #   namespaceOne\n    #   namespaceTwo\n    # -- Timeout value for orphaned pods\n    timeout: 300\n\n  # -- Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it\n  idleMinutes: 0\n\n  # The raw yaml of a Pod API Object, for example, this allows usage of toleration for agent pods.\n  # https://github.com/jenkinsci/kubernetes-plugin#using-yaml-to-define-pod-templates\n  # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  # -- The raw yaml of a Pod API Object to merge into the agent spec\n  yamlTemplate: \"\"\n  # yamlTemplate: |-\n  #   apiVersion: v1\n  #   kind: Pod\n  #   spec:\n  #     tolerations:\n  #     - key: \"key\"\n  #       operator: \"Equal\"\n  #       value: \"value\"\n\n  # -- Defines how the raw yaml field gets merged with yaml definitions from inherited pod templates. Possible values: \"merge\" or \"override\"\n  yamlMergeStrategy: \"override\"\n  # -- Controls whether the defined yaml merge strategy will be inherited if another defined pod template is configured to inherit from the current one\n  inheritYamlMergeStrategy: false\n  # -- Timeout in seconds for an agent to be online\n  connectTimeout: 100\n  # -- Annotations to apply to the pod\n  annotations: {}\n\n  # Containers specified here are added to all agents. Set key empty to remove container from additional agents.\n  # -- Add additional containers to the agents\n  additionalContainers: []\n  #  - sideContainerName: dind\n  #    image:\n  #      repository: docker\n  #      tag: dind\n  #    command: dockerd-entrypoint.sh\n  #    args: \"\"\n  #    privileged: true\n  #    resources:\n  #      requests:\n  #        cpu: 500m\n  #        memory: 1Gi\n  #      limits:\n  #        cpu: 1\n  #        memory: 2Gi\n\n  # Useful when configuring agents only with the podTemplates value, since the default podTemplate populated by values mentioned above will be excluded in the rendered template.\n  # -- Disable the default Jenkins Agent configuration\n  disableDefaultAgent: false\n\n  # Below is the implementation of custom pod templates for the default configured kubernetes cloud.\n  # Add a key under podTemplates for each pod template. Each key (prior to | character) is just a label, and can be any value.\n  # Keys are only used to give the pod template a meaningful name. The only restriction is they may only contain RFC 1123 \\ DNS label\n  # characters: lowercase letters, numbers, and hyphens. Each pod template can contain multiple containers.\n  # For this pod templates configuration to be loaded, the following values must be set:\n  # controller.JCasC.defaultConfig: true\n  # Best reference is https://\u003cjenkins_url\u003e/configuration-as-code/reference#Cloud-kubernetes. The example below creates a python pod template.\n  # -- Configures extra pod templates for the default kubernetes cloud\n  podTemplates: {}\n  #  python: |\n  #    - name: python\n  #      label: jenkins-python\n  #      serviceAccount: jenkins\n  #      containers:\n  #        - name: python\n  #          image: python:3\n  #          command: \"/bin/sh -c\"\n  #          args: \"cat\"\n  #          ttyEnabled: true\n  #          privileged: true\n  #          resourceRequestCpu: \"400m\"\n  #          resourceRequestMemory: \"512Mi\"\n  #          resourceLimitCpu: \"1\"\n  #          resourceLimitMemory: \"1024Mi\"\n\n# Inherits all values from `agent` so you only need to specify values which differ\n# -- Configure additional\nadditionalAgents: {}\n#  maven:\n#    podName: maven\n#    customJenkinsLabels: maven\n#    # An example of overriding the jnlp container\n#    # sideContainerName: jnlp\n#    image:\n#      repository: jenkins/jnlp-agent-maven\n#      tag: latest\n#  python:\n#    podName: python\n#    customJenkinsLabels: python\n#    sideContainerName: python\n#    image:\n#      repository: python\n#      tag: \"3\"\n#    command: \"/bin/sh -c\"\n#    args: \"cat\"\n#    TTYEnabled: true\n\n# Here you can add additional clouds\n# They inherit all values from the default cloud (including the main agent), so\n# you only need to specify values which differ. If you want to override\n# default additionalAgents with the additionalClouds.additionalAgents set\n# additionalAgentsOverride to `true`.\nadditionalClouds: {}\n#  remote-cloud-1:\n#    kubernetesURL: https://api.remote-cloud.com\n#    additionalAgentsOverride: true\n#    additionalAgents:\n#      maven-2:\n#        podName: maven-2\n#        customJenkinsLabels: maven\n#        # An example of overriding the jnlp container\n#        # sideContainerName: jnlp\n#        image:\n#          repository: jenkins/jnlp-agent-maven\n#          tag: latest\n#        namespace: my-other-maven-namespace\n#  remote-cloud-2:\n#    kubernetesURL: https://api.remote-cloud.com\n\npersistence:\n  # -- Enable the use of a Jenkins PVC\n  enabled: true\n\n  # A manually managed Persistent Volume and Claim\n  # Requires persistence.enabled: true\n  # If defined, PVC must be created manually before volume will be bound\n  # -- Provide the name of a PVC\n  existingClaim:\n\n  # jenkins data Persistent Volume Storage Class\n  # If defined, storageClassName: \u003cstorageClass\u003e\n  # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  # If undefined (the default) or set to null, no storageClassName spec is\n  #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS \u0026 OpenStack)\n  # -- Storage class for the PVC\n  storageClass:\n  # -- Annotations for the PVC\n  annotations: {}\n  # -- Labels for the PVC\n  labels: {}\n  # -- The PVC access mode\n  accessMode: \"ReadWriteOnce\"\n  # -- The size of the PVC\n  size: \"8Gi\"\n\n  # ref: https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/\n  # -- Existing data source to clone PVC from\n  dataSource: {}\n  #   name: PVC-NAME\n  #   kind: PersistentVolumeClaim\n\n  # -- SubPath for jenkins-home mount\n  subPath:\n  # -- Additional volumes\n  volumes: []\n  #  - name: nothing\n  #    emptyDir: {}\n\n  # -- Additional mounts\n  mounts: []\n  #  - mountPath: /var/nothing\n  #    name: nothing\n  #    readOnly: true\n\nnetworkPolicy:\n  # -- Enable the creation of NetworkPolicy resources\n  enabled: false\n\n  # For Kubernetes v1.4, v1.5 and v1.6, use 'extensions/v1beta1'\n  # For Kubernetes v1.7, use 'networking.k8s.io/v1'\n  # -- NetworkPolicy ApiVersion\n  apiVersion: networking.k8s.io/v1\n  # You can allow agents to connect from both within the cluster (from within specific/all namespaces) AND/OR from a given external IP range\n  internalAgents:\n    # -- Allow internal agents (from the same cluster) to connect to controller. Agent pods will be filtered based on PodLabels\n    allowed: true\n    # -- A map of labels (keys/values) that agent pods must have to be able to connect to controller\n    podLabels: {}\n    # -- A map of labels (keys/values) that agents namespaces must have to be able to connect to controller\n    namespaceLabels:\n      {}\n      # project: myproject\n  externalAgents:\n    # -- The IP range from which external agents are allowed to connect to controller, i.e., 172.17.0.0/16\n    ipCIDR:\n    # -- A list of IP sub-ranges to be excluded from the allowlisted IP range\n    except:\n      []\n      # - 172.17.1.0/24\n\n## Install Default RBAC roles and bindings\nrbac:\n  # -- Whether RBAC resources are created\n  create: true\n  # -- Whether the Jenkins service account should be able to read Kubernetes secrets\n  readSecrets: false\n  # -- Whether the Jenkins service account should be able to use the OpenShift \"nonroot\" Security Context Constraints\n  useOpenShiftNonRootSCC: false\n\nserviceAccount:\n  # -- Configures if a ServiceAccount with this name should be created\n  create: true\n\n  # The name of the ServiceAccount is autogenerated by default\n  #  -- The name of the ServiceAccount to be used by access-controlled resources\n  name:\n  # -- Configures annotations for the ServiceAccount\n  annotations: {}\n  # -- Configures extra labels for the ServiceAccount\n  extraLabels: {}\n  # -- Controller ServiceAccount image pull secret\n  imagePullSecretName:\n  # -- Auto-mount ServiceAccount token\n  automountServiceAccountToken: true\n\nserviceAccountAgent:\n  # -- Configures if an agent ServiceAccount should be created\n  create: false\n\n  # If not set and create is true, a name is generated using the fullname template\n  # -- The name of the agent ServiceAccount to be used by access-controlled resources\n  name:\n  # -- Configures annotations for the agent ServiceAccount\n  annotations: {}\n  # -- Configures extra labels for the agent ServiceAccount\n  extraLabels: {}\n  # -- Agent ServiceAccount image pull secret\n  imagePullSecretName:\n  # -- Auto-mount ServiceAccount token\n  automountServiceAccountToken: true\n\n# -- Checks if any deprecated values are used\ncheckDeprecation: true\n\nawsSecurityGroupPolicies:\n  enabled: false\n  policies:\n    - name: \"\"\n      securityGroupIds: []\n      podSelector: {}\n\n# Here you can configure unit tests values when executing the helm unittest in the CONTRIBUTING.md\nhelmtest:\n  # A testing framework for bash\n  bats:\n    # Bash Automated Testing System (BATS)\n    image:\n      # -- Registry of the image used to test the framework\n      registry: \"docker.io\"\n      # -- Repository of the image used to test the framework\n      repository: \"bats/bats\"\n      # -- Tag of the image to test the framework\n      tag: \"1.12.0\"\n"
            ],
            "verify": false,
            "version": "5.8.104",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "loki",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "loki",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "loki",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "3.5.5",
                "chart": "loki",
                "first_deployed": 1761172593,
                "last_deployed": 1761172593,
                "name": "loki",
                "namespace": "monitoring",
                "notes": "***********************************************************************\n Welcome to Grafana Loki\n Chart version: 6.43.0\n Chart Name: loki\n Loki version: 3.5.5\n***********************************************************************\n\n** Please be patient while the chart is being deployed **\n\nTip:\n\n  Watch the deployment status using the command: kubectl get pods -w --namespace monitoring\n\nIf pods are taking too long to schedule make sure pod affinity can be fulfilled in the current cluster.\n\n***********************************************************************\nInstalled components:\n***********************************************************************\n* loki\n\nLoki has been deployed as a single binary.\nThis means a single pod is handling reads and writes. You can scale that pod vertically by adding more CPU and memory resources.\n\n\n***********************************************************************\nSending logs to Loki\n***********************************************************************\n\nLoki has been configured with a gateway (nginx) to support reads and writes from a single component.\n\nYou can send logs from inside the cluster using the cluster DNS:\n\nhttp://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push\n\nYou can test to send data from outside the cluster by port-forwarding the gateway to your local machine:\n\n  kubectl port-forward --namespace monitoring svc/loki-gateway 3100:80 \u0026\n\nAnd then using http://127.0.0.1:3100/loki/api/v1/push URL as shown below:\n\n```\ncurl -H \"Content-Type: application/json\" -XPOST -s \"http://127.0.0.1:3100/loki/api/v1/push\"  \\\n--data-raw \"{\\\"streams\\\": [{\\\"stream\\\": {\\\"job\\\": \\\"test\\\"}, \\\"values\\\": [[\\\"$(date +%s)000000000\\\", \\\"fizzbuzz\\\"]]}]}\"\n```\n\nThen verify that Loki did receive the data using the following command:\n\n```\ncurl \"http://127.0.0.1:3100/loki/api/v1/query_range\" --data-urlencode 'query={job=\"test\"}' | jq .data.result\n```\n\n***********************************************************************\nConnecting Grafana to Loki\n***********************************************************************\n\nIf Grafana operates within the cluster, you'll set up a new Loki datasource by utilizing the following URL:\n\nhttp://loki-gateway.monitoring.svc.cluster.local/\n",
                "revision": 1,
                "values": "{\"adminApi\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"dnsConfig\":{},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"initContainers\":[],\"labels\":{},\"livenessProbe\":{},\"nodeSelector\":{},\"podSecurityContext\":{\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"startupProbe\":{},\"strategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":60,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"backend\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"backend\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"annotations\":{},\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":0,\"minReplicas\":0,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"dataVolumeParameters\":{\"emptyDir\":{}},\"enableStatefulSetAutoDeletePVC\":true,\"labels\":{},\"selector\":null,\"size\":\"10Gi\",\"storageClass\":null,\"volumeClaimsEnabled\":true},\"podAnnotations\":{},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"selectorLabels\":{},\"service\":{\"annotations\":{},\"labels\":{},\"type\":\"ClusterIP\"},\"targetModule\":\"backend\",\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"bloomBuilder\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"bloom-builder\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"autoscaling\":{\"behavior\":{\"enabled\":false,\"scaleDown\":{},\"scaleUp\":{}},\"customMetrics\":[],\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"maxUnavailable\":null,\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"bloomGateway\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"bloom-gateway\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"livenessProbe\":{},\"nodeSelector\":{},\"persistence\":{\"annotations\":{},\"claims\":[{\"accessModes\":[\"ReadWriteOnce\"],\"name\":\"data\",\"size\":\"10Gi\",\"storageClass\":null}],\"enableStatefulSetAutoDeletePVC\":false,\"enabled\":false,\"labels\":{},\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{},\"replicas\":0,\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":false,\"imagePullSecrets\":[],\"name\":null},\"serviceAnnotations\":{},\"serviceLabels\":{},\"startupProbe\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"bloomPlanner\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"bloom-planner\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"livenessProbe\":{},\"nodeSelector\":{},\"persistence\":{\"claims\":[{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"labels\":{},\"name\":\"data\",\"size\":\"10Gi\",\"storageClass\":null}],\"enableStatefulSetAutoDeletePVC\":false,\"enabled\":false,\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{},\"replicas\":0,\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":false,\"imagePullSecrets\":[],\"name\":null},\"serviceAnnotations\":{},\"serviceLabels\":{},\"startupProbe\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"chunksCache\":{\"addresses\":\"dnssrvnoa+_memcached-client._tcp.{{ include \\\"loki.resourceName\\\" (dict \\\"ctx\\\" $ \\\"component\\\" \\\"chunks-cache\\\" \\\"suffix\\\" $.Values.chunksCache.suffix ) }}.{{ include \\\"loki.namespace\\\" $ }}.svc\",\"affinity\":{},\"allocatedMemory\":8192,\"annotations\":{},\"batchSize\":4,\"connectionLimit\":16384,\"defaultValidity\":\"0s\",\"dnsConfig\":{},\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"extraExtendedOptions\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"initContainers\":[],\"l2\":{\"addresses\":\"dnssrvnoa+_memcached-client._tcp.{{ include \\\"loki.resourceName\\\" (dict \\\"ctx\\\" $ \\\"component\\\" \\\"chunks-cache\\\" \\\"suffix\\\" $.Values.chunksCache.l2.suffix ) }}.{{ include \\\"loki.namespace\\\" $ }}.svc\",\"affinity\":{},\"allocatedMemory\":8192,\"annotations\":{},\"batchSize\":4,\"connectionLimit\":16384,\"defaultValidity\":\"0s\",\"dnsConfig\":{},\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"extraExtendedOptions\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"initContainers\":[],\"l2ChunkCacheHandoff\":\"345600s\",\"maxItemMemory\":5,\"maxUnavailable\":1,\"nodeSelector\":{},\"parallelism\":5,\"persistence\":{\"enabled\":false,\"labels\":{},\"mountPath\":\"/data\",\"storageClass\":null,\"storageSize\":\"10G\"},\"podAnnotations\":{},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"suffix\":\"l2\",\"terminationGracePeriodSeconds\":60,\"timeout\":\"2000ms\",\"tolerations\":[],\"topologySpreadConstraints\":[],\"writebackBuffer\":500000,\"writebackParallelism\":1,\"writebackSizeLimit\":\"500MB\"},\"maxItemMemory\":5,\"maxUnavailable\":1,\"nodeSelector\":{},\"parallelism\":5,\"persistence\":{\"enabled\":false,\"labels\":{},\"mountPath\":\"/data\",\"storageClass\":null,\"storageSize\":\"10G\"},\"podAnnotations\":{},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"suffix\":\"\",\"terminationGracePeriodSeconds\":60,\"timeout\":\"2000ms\",\"tolerations\":[],\"topologySpreadConstraints\":[],\"writebackBuffer\":500000,\"writebackParallelism\":1,\"writebackSizeLimit\":\"500MB\"},\"clusterLabelOverride\":null,\"compactor\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"compactor\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"livenessProbe\":{},\"nodeSelector\":{},\"persistence\":{\"claims\":[{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"labels\":{},\"name\":\"data\",\"size\":\"10Gi\",\"storageClass\":null}],\"enableStatefulSetAutoDeletePVC\":false,\"enabled\":false,\"size\":\"10Gi\",\"storageClass\":null,\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{},\"replicas\":0,\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":false,\"imagePullSecrets\":[],\"name\":null},\"serviceAnnotations\":{},\"serviceLabels\":{},\"serviceType\":\"ClusterIP\",\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"deploymentMode\":\"SingleBinary\",\"distributor\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"distributor\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"autoscaling\":{\"behavior\":{\"enabled\":false,\"scaleDown\":{},\"scaleUp\":{}},\"customMetrics\":[],\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"maxSurge\":0,\"maxUnavailable\":null,\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"serviceType\":\"ClusterIP\",\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"trafficDistribution\":\"\"},\"enterprise\":{\"adminApi\":{\"enabled\":true},\"adminToken\":{\"secret\":null},\"canarySecret\":null,\"cluster_name\":null,\"config\":\"{{- if .Values.enterprise.adminApi.enabled }}\\nadmin_client:\\n  {{ include \\\"enterprise-logs.adminAPIStorageConfig\\\" . | nindent 2 }}\\n{{ end }}\\nauth:\\n  type: {{ .Values.enterprise.adminApi.enabled | ternary \\\"enterprise\\\" \\\"trust\\\" }}\\nauth_enabled: {{ .Values.loki.auth_enabled }}\\ncluster_name: {{ include \\\"loki.clusterName\\\" . }}\\nlicense:\\n  path: /etc/loki/license/license.jwt\\n\",\"enabled\":false,\"externalConfigName\":\"\",\"externalLicenseName\":null,\"gelGateway\":true,\"image\":{\"digest\":null,\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/enterprise-logs\",\"tag\":\"3.5.5\"},\"license\":{\"contents\":\"NOTAVALIDLICENSE\"},\"provisioner\":{\"additionalTenants\":[],\"affinity\":{},\"annotations\":{},\"apiUrl\":\"{{ include \\\"loki.address\\\" . }}\",\"enabled\":true,\"env\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hookType\":\"post-install\",\"hostUsers\":\"nil\",\"image\":{\"digest\":null,\"pullPolicy\":\"IfNotPresent\",\"registry\":\"us-docker.pkg.dev\",\"repository\":\"grafanalabs-global/docker-enterprise-provisioner-prod/enterprise-provisioner\",\"tag\":\"latest\"},\"labels\":{},\"nodeSelector\":{},\"priorityClassName\":null,\"provisionedSecretPrefix\":null,\"securityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001},\"tolerations\":[]},\"useExternalLicense\":false,\"version\":\"3.5.2\"},\"enterpriseGateway\":{\"affinity\":{},\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"env\":[],\"extraArgs\":{},\"extraContainers\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"initContainers\":[],\"labels\":{},\"nodeSelector\":{},\"podSecurityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"labels\":{},\"type\":\"ClusterIP\"},\"strategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":60,\"tolerations\":[],\"topologySpreadConstraints\":[],\"useDefaultProxyURLs\":true},\"extraObjects\":null,\"fullnameOverride\":null,\"gateway\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"gateway\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"annotations\":{},\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"basicAuth\":{\"enabled\":false,\"existingSecret\":null,\"htpasswd\":\"{{- with $tenants := .Values.loki.tenants }}\\n  {{- range $t := $tenants }}\\n    {{- $username := required \\\"All tenants must have a 'name' set\\\" $t.name }}\\n    {{- if $passwordHash := $t.passwordHash }}\\n      {{- printf \\\"%s:%s\\\\n\\\" $username $passwordHash }}\\n    {{- else if $password := $t.password }}\\n      {{- printf \\\"%s\\\\n\\\" (htpasswd $username $password) }}\\n    {{- else }}\\n      {{- fail \\\"All tenants must have a 'password' or 'passwordHash' set\\\" }}\\n    {{- end }}\\n  {{- end }}\\n{{- else }}\\n  {{- printf \\\"%s\\\\n\\\" (htpasswd (required \\\"'gateway.basicAuth.username' is required\\\" .Values.gateway.basicAuth.username) (required \\\"'gateway.basicAuth.password' is required\\\" .Values.gateway.basicAuth.password)) }}\\n{{- end }}\\n\",\"password\":null,\"username\":null},\"containerPort\":8080,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"deploymentStrategy\":{\"type\":\"RollingUpdate\"},\"dnsConfig\":{},\"enabled\":true,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"image\":{\"digest\":null,\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"nginxinc/nginx-unprivileged\",\"tag\":\"1.29-alpine\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[{\"host\":\"gateway.loki.example.com\",\"paths\":[{\"path\":\"/\"}]}],\"ingressClassName\":\"\",\"labels\":{},\"tls\":[{\"hosts\":[\"gateway.loki.example.com\"],\"secretName\":\"loki-gateway-tls\"}]},\"lifecycle\":{},\"livenessProbe\":{},\"nginxConfig\":{\"clientMaxBodySize\":\"4M\",\"customBackendUrl\":null,\"customReadUrl\":null,\"customWriteUrl\":null,\"enableIPv6\":true,\"file\":\"{{- include \\\"loki.nginxFile\\\" . -}}\\n\",\"httpSnippet\":\"\",\"locationSnippet\":\"{{ if .Values.loki.tenants }}proxy_set_header X-Scope-OrgID $remote_user;{{ end }}\",\"logFormat\":\"main '$remote_addr - $remote_user [$time_local]  $status '\\n        '\\\"$request\\\" $body_bytes_sent \\\"$http_referer\\\" '\\n        '\\\"$http_user_agent\\\" \\\"$http_x_forwarded_for\\\"';\",\"resolver\":\"\",\"schema\":\"http\",\"serverSnippet\":\"\",\"ssl\":false},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"fsGroup\":101,\"runAsGroup\":101,\"runAsNonRoot\":true,\"runAsUser\":101},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":15,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"clusterIP\":null,\"labels\":{},\"loadBalancerIP\":null,\"nodePort\":null,\"port\":80,\"type\":\"ClusterIP\"},\"startupProbe\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"verboseLogging\":true},\"global\":{\"clusterDomain\":\"cluster.local\",\"dnsNamespace\":\"kube-system\",\"dnsService\":\"kube-dns\",\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null},\"priorityClassName\":null},\"imagePullSecrets\":[],\"indexGateway\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"index-gateway\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"joinMemberlist\":true,\"maxUnavailable\":null,\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enableStatefulSetAutoDeletePVC\":false,\"enabled\":false,\"inMemory\":false,\"labels\":{},\"size\":\"10Gi\",\"storageClass\":null,\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"serviceType\":\"ClusterIP\",\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[],\"updateStrategy\":{\"type\":\"RollingUpdate\"}},\"ingester\":{\"addIngesterNamePrefix\":false,\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"ingester\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"autoscaling\":{\"behavior\":{\"enabled\":false,\"scaleDown\":{},\"scaleUp\":{}},\"customMetrics\":[],\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"labels\":{},\"lifecycle\":{},\"livenessProbe\":{},\"maxUnavailable\":1,\"nodeSelector\":{},\"persistence\":{\"claims\":[{\"accessModes\":[\"ReadWriteOnce\"],\"name\":\"data\",\"size\":\"10Gi\",\"storageClass\":null}],\"enableStatefulSetAutoDeletePVC\":false,\"enabled\":false,\"inMemory\":false,\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{},\"replicas\":0,\"resources\":{},\"rolloutGroupPrefix\":null,\"serviceAnnotations\":{},\"serviceLabels\":{},\"serviceType\":\"ClusterIP\",\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"ingester\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}],\"updateStrategy\":{\"type\":\"RollingUpdate\"},\"zoneAwareReplication\":{\"enabled\":true,\"maxUnavailablePct\":33,\"migration\":{\"enabled\":false,\"excludeDefaultZone\":false,\"readPath\":false,\"writePath\":false},\"zoneA\":{\"annotations\":{},\"extraAffinity\":{},\"nodeSelector\":null,\"podAnnotations\":{}},\"zoneB\":{\"annotations\":{},\"extraAffinity\":{},\"nodeSelector\":null,\"podAnnotations\":{}},\"zoneC\":{\"annotations\":{},\"extraAffinity\":{},\"nodeSelector\":null,\"podAnnotations\":{}}}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[\"loki.example.com\"],\"ingressClassName\":\"\",\"labels\":{},\"paths\":{\"compactor\":[\"/loki/api/v1/delete\"],\"distributor\":[\"/api/prom/push\",\"/loki/api/v1/push\",\"/otlp/v1/logs\",\"/ui\"],\"queryFrontend\":[\"/api/prom/query\",\"/api/prom/label\",\"/api/prom/series\",\"/api/prom/tail\",\"/loki/api/v1/query\",\"/loki/api/v1/query_range\",\"/loki/api/v1/tail\",\"/loki/api/v1/label\",\"/loki/api/v1/labels\",\"/loki/api/v1/series\",\"/loki/api/v1/index/stats\",\"/loki/api/v1/index/volume\",\"/loki/api/v1/index/volume_range\",\"/loki/api/v1/format_query\",\"/loki/api/v1/detected_field\",\"/loki/api/v1/detected_fields\",\"/loki/api/v1/detected_labels\",\"/loki/api/v1/patterns\"],\"ruler\":[\"/api/prom/rules\",\"/api/prom/api/v1/rules\",\"/api/prom/api/v1/alerts\",\"/loki/api/v1/rules\",\"/prometheus/api/v1/rules\",\"/prometheus/api/v1/alerts\"]},\"tls\":[]},\"kubeVersionOverride\":null,\"loki\":{\"analytics\":{},\"annotations\":{},\"auth_enabled\":false,\"block_builder\":{},\"bloom_build\":{\"builder\":{\"planner_address\":\"{{ include \\\"loki.bloomPlannerAddress\\\" . }}\"},\"enabled\":false},\"bloom_gateway\":{\"client\":{\"addresses\":\"{{ include \\\"loki.bloomGatewayAddresses\\\" . }}\"},\"enabled\":false},\"commonConfig\":{\"compactor_grpc_address\":\"{{ include \\\"loki.compactorAddress\\\" . }}\",\"path_prefix\":\"/var/loki\",\"replication_factor\":1},\"compactor\":{},\"compactor_grpc_client\":{},\"config\":\"{{- if .Values.enterprise.enabled}}\\n{{- tpl .Values.enterprise.config . }}\\n{{- else }}\\nauth_enabled: {{ .Values.loki.auth_enabled }}\\n{{- end }}\\n\\n{{- with .Values.loki.server }}\\nserver:\\n  {{- toYaml . | nindent 2}}\\n{{- end}}\\n\\n{{- with .Values.loki.pattern_ingester }}\\npattern_ingester:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\nmemberlist:\\n{{- if .Values.loki.memberlistConfig }}\\n  {{- toYaml .Values.loki.memberlistConfig | nindent 2 }}\\n{{- else }}\\n{{- if .Values.loki.extraMemberlistConfig}}\\n{{- toYaml .Values.loki.extraMemberlistConfig | nindent 2}}\\n{{- end }}\\n  join_members:\\n    - {{ include \\\"loki.memberlist\\\" . }}\\n    {{- with .Values.migrate.fromDistributed }}\\n    {{- if .enabled }}\\n    - {{ .memberlistService }}\\n    {{- end }}\\n    {{- end }}\\n{{- end }}\\n\\n{{- with .Values.loki.ingester }}\\ningester:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.ingester_client }}\\ningester_client:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.block_builder }}\\nblock_builder:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- if .Values.loki.commonConfig}}\\ncommon:\\n{{- toYaml .Values.loki.commonConfig | nindent 2}}\\n  storage:\\n  {{- include \\\"loki.commonStorageConfig\\\" . | nindent 4}}\\n{{- end}}\\n\\n{{- with .Values.loki.limits_config }}\\nlimits_config:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\nruntime_config:\\n  file: /etc/loki/runtime-config/runtime-config.yaml\\n\\n{{- if .Values.chunksCache.enabled }}\\n{{- with .Values.chunksCache }}\\nchunk_store_config:\\n  chunk_cache_config:\\n    default_validity: {{ .defaultValidity }}\\n    background:\\n      writeback_goroutines: {{ .writebackParallelism }}\\n      writeback_buffer: {{ .writebackBuffer }}\\n      writeback_size_limit: {{ .writebackSizeLimit }}\\n    memcached:\\n      batch_size: {{ .batchSize }}\\n      parallelism: {{ .parallelism }}\\n    memcached_client:\\n      addresses: {{ .addresses }}\\n      consistent_hash: true\\n      timeout: {{ .timeout }}\\n      max_idle_conns: 72\\n  {{- end }}\\n  {{- with .Values.chunksCache.l2 }}\\n  {{- if .enabled }}\\n  l2_chunk_cache_handoff: {{ .l2ChunkCacheHandoff }}\\n  chunk_cache_config_l2:\\n    default_validity: {{ .defaultValidity }}\\n    background:\\n      writeback_goroutines: {{ .writebackParallelism }}\\n      writeback_buffer: {{ .writebackBuffer }}\\n      writeback_size_limit: {{ .writebackSizeLimit }}\\n    memcached:\\n      batch_size: {{ .batchSize }}\\n      parallelism: {{ .parallelism }}\\n    memcached_client:\\n      addresses: {{ .addresses }}\\n      consistent_hash: true\\n      timeout: {{ .timeout }}\\n      max_idle_conns: 72\\n  {{- end }}\\n  {{- end }}\\n{{- end }}\\n\\n{{- if .Values.loki.schemaConfig }}\\nschema_config:\\n{{- toYaml .Values.loki.schemaConfig | nindent 2}}\\n{{- end }}\\n\\n{{- if .Values.loki.useTestSchema }}\\nschema_config:\\n{{- toYaml .Values.loki.testSchemaConfig | nindent 2}}\\n{{- end }}\\n\\n{{- if .Values.ruler.enabled }}\\n{{ include \\\"loki.rulerConfig\\\" . }}\\n{{- end }}\\n\\n{{- if and .Values.loki.storage.use_thanos_objstore .Values.ruler.enabled}}\\nruler_storage:\\n  {{- include \\\"loki.rulerThanosStorageConfig\\\" . | nindent 2 }}\\n{{- end }}\\n\\n{{- if or .Values.tableManager.retention_deletes_enabled .Values.tableManager.retention_period }}\\ntable_manager:\\n  retention_deletes_enabled: {{ .Values.tableManager.retention_deletes_enabled }}\\n  retention_period: {{ .Values.tableManager.retention_period }}\\n{{- end }}\\n\\nquery_range:\\n  align_queries_with_step: true\\n  {{- with .Values.loki.query_range }}\\n  {{- tpl (. | toYaml) $ | nindent 2 }}\\n  {{- end }}\\n  {{- if .Values.resultsCache.enabled }}\\n  {{- with .Values.resultsCache }}\\n  cache_results: true\\n  results_cache:\\n    cache:\\n      default_validity: {{ .defaultValidity }}\\n      background:\\n        writeback_goroutines: {{ .writebackParallelism }}\\n        writeback_buffer: {{ .writebackBuffer }}\\n        writeback_size_limit: {{ .writebackSizeLimit }}\\n      memcached_client:\\n        addresses: {{ .addresses }}\\n        consistent_hash: true\\n        timeout: {{ .timeout }}\\n        update_interval: 1m\\n  {{- end }}\\n  {{- end }}\\n\\n{{- with .Values.loki.storage_config }}\\nstorage_config:\\n    {{- if not (hasKey $.Values.loki.storage_config \\\"use_thanos_objstore\\\") }}\\n    use_thanos_objstore: {{ $.Values.loki.storage.use_thanos_objstore }}\\n    {{- end }}\\n    {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.query_scheduler }}\\nquery_scheduler:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.compactor }}\\ncompactor:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.compactor_grpc_client }}\\ncompactor_grpc_client:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.analytics }}\\nanalytics:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- if .Values.loki.ui.enabled }}\\nui:\\n  enabled: true\\n{{- end }}\\n{{- with .Values.loki.querier }}\\nquerier:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.index_gateway }}\\nindex_gateway:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.frontend }}\\nfrontend:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.frontend_worker }}\\nfrontend_worker:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.distributor }}\\ndistributor:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\ntracing:\\n  enabled: {{ .Values.loki.tracing.enabled }}\\n\\n{{- with .Values.loki.bloom_build }}\\nbloom_build:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.bloom_gateway }}\\nbloom_gateway:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.operational_config }}\\noperational_config:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\",\"configObjectName\":\"{{ include \\\"loki.name\\\" . }}\",\"configStorageType\":\"ConfigMap\",\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"distributor\":{},\"dnsConfig\":{},\"enableServiceLinks\":true,\"extraMemberlistConfig\":{},\"frontend\":{\"scheduler_address\":\"{{ include \\\"loki.querySchedulerAddress\\\" . }}\",\"tail_proxy_url\":\"{{ include \\\"loki.querierAddress\\\" . }}\"},\"frontend_worker\":{\"scheduler_address\":\"{{ include \\\"loki.querySchedulerAddress\\\" . }}\"},\"generatedConfigObjectName\":\"{{ include \\\"loki.name\\\" . }}\",\"image\":{\"digest\":null,\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/loki\",\"tag\":\"3.5.5\"},\"index_gateway\":{\"mode\":\"simple\"},\"ingester\":{},\"ingester_client\":{},\"limits_config\":{\"max_cache_freshness_per_query\":\"10m\",\"query_timeout\":\"300s\",\"reject_old_samples\":true,\"reject_old_samples_max_age\":\"168h\",\"split_queries_by_interval\":\"15m\",\"volume_enabled\":true},\"livenessProbe\":{},\"memberlistConfig\":{},\"operational_config\":{},\"pattern_ingester\":{\"enabled\":false},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001},\"querier\":{},\"query_range\":{},\"query_scheduler\":{},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":30,\"timeoutSeconds\":1},\"revisionHistoryLimit\":10,\"rulerConfig\":{\"wal\":{\"dir\":\"/var/loki/ruler-wal\"}},\"runtimeConfig\":{},\"schemaConfig\":{},\"server\":{\"grpc_listen_port\":9095,\"http_listen_port\":3100,\"http_server_read_timeout\":\"600s\",\"http_server_write_timeout\":\"600s\"},\"serviceAnnotations\":{},\"serviceLabels\":{},\"startupProbe\":{},\"storage\":{\"azure\":{\"accountKey\":null,\"accountName\":null,\"chunkDelimiter\":null,\"connectionString\":null,\"endpointSuffix\":null,\"requestTimeout\":null,\"useFederatedToken\":false,\"useManagedIdentity\":false,\"userAssignedId\":null},\"filesystem\":{\"chunks_directory\":\"/var/loki/chunks\",\"rules_directory\":\"/var/loki/rules\"},\"gcs\":{\"chunkBufferSize\":0,\"enableHttp2\":true,\"requestTimeout\":\"0s\"},\"object_store\":{\"azure\":{\"account_key\":null,\"account_name\":null},\"gcs\":{\"bucket_name\":null,\"service_account\":null},\"s3\":{\"access_key_id\":null,\"endpoint\":null,\"http\":{},\"insecure\":false,\"region\":null,\"secret_access_key\":null,\"sse\":{}},\"storage_prefix\":null,\"type\":\"s3\"},\"s3\":{\"accessKeyId\":null,\"backoff_config\":{},\"disable_dualstack\":false,\"endpoint\":null,\"http_config\":{},\"insecure\":false,\"region\":null,\"s3\":null,\"s3ForcePathStyle\":false,\"secretAccessKey\":null,\"signatureVersion\":null},\"swift\":{\"auth_url\":null,\"auth_version\":null,\"connect_timeout\":null,\"container_name\":null,\"domain_id\":null,\"domain_name\":null,\"internal\":null,\"max_retries\":null,\"password\":null,\"project_domain_id\":null,\"project_domain_name\":null,\"project_id\":null,\"project_name\":null,\"region_name\":null,\"request_timeout\":null,\"user_domain_id\":null,\"user_domain_name\":null,\"user_id\":null,\"username\":null},\"type\":\"filesystem\",\"use_thanos_objstore\":false},\"storage_config\":{\"bloom_shipper\":{\"working_directory\":\"/var/loki/data/bloomshipper\"},\"boltdb_shipper\":{\"index_gateway_client\":{\"server_address\":\"{{ include \\\"loki.indexGatewayAddress\\\" . }}\"}},\"hedging\":{\"at\":\"250ms\",\"max_per_second\":20,\"up_to\":3},\"tsdb_shipper\":{\"index_gateway_client\":{\"server_address\":\"{{ include \\\"loki.indexGatewayAddress\\\" . }}\"}}},\"structuredConfig\":{},\"tenants\":[],\"testSchemaConfig\":{\"configs\":[{\"from\":\"2024-04-01\",\"index\":{\"period\":\"24h\",\"prefix\":\"index_\"},\"object_store\":\"{{ include \\\"loki.testSchemaObjectStore\\\" . }}\",\"schema\":\"v13\",\"store\":\"tsdb\"}]},\"tracing\":{\"enabled\":false},\"ui\":{\"enabled\":false,\"gateway\":{\"enabled\":true}},\"useTestSchema\":true},\"lokiCanary\":{\"affinity\":{},\"annotations\":{},\"dnsConfig\":{},\"enabled\":true,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"image\":{\"digest\":null,\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/loki-canary\",\"tag\":null},\"kind\":\"DaemonSet\",\"labelname\":\"pod\",\"lokiurl\":null,\"nodeSelector\":{},\"podLabels\":{},\"priorityClassName\":null,\"push\":true,\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"labels\":{}},\"tolerations\":[],\"updateStrategy\":{\"rollingUpdate\":{\"maxUnavailable\":1},\"type\":\"RollingUpdate\"}},\"memberlist\":{\"service\":{\"annotations\":{},\"publishNotReadyAddresses\":false}},\"memcached\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"memcached\",\"tag\":\"1.6.39-alpine\"},\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"tcpSocket\":{\"port\":\"client\"},\"timeoutSeconds\":5},\"podSecurityContext\":{\"fsGroup\":11211,\"runAsGroup\":11211,\"runAsNonRoot\":true,\"runAsUser\":11211},\"priorityClassName\":null,\"readinessProbe\":{\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":5,\"tcpSocket\":{\"port\":\"client\"},\"timeoutSeconds\":3},\"startupProbe\":{}},\"memcachedExporter\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":true,\"extraArgs\":{},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"prom/memcached-exporter\",\"tag\":\"v0.15.3\"},\"livenessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/metrics\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":30,\"periodSeconds\":10,\"timeoutSeconds\":5},\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/metrics\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":5,\"periodSeconds\":5,\"timeoutSeconds\":3},\"resources\":{\"limits\":{},\"requests\":{}},\"startupProbe\":{}},\"migrate\":{\"fromDistributed\":{\"enabled\":false,\"memberlistService\":\"\"}},\"minio\":{\"address\":null,\"buckets\":[{\"name\":\"chunks\",\"policy\":\"none\",\"purge\":false},{\"name\":\"ruler\",\"policy\":\"none\",\"purge\":false},{\"name\":\"admin\",\"policy\":\"none\",\"purge\":false}],\"drivesPerNode\":2,\"enabled\":false,\"persistence\":{\"annotations\":{},\"size\":\"5Gi\"},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}},\"rootPassword\":\"supersecretpassword\",\"rootUser\":\"root-user\",\"users\":[{\"accessKey\":\"logs-user\",\"policy\":\"readwrite\",\"secretKey\":\"supersecretpassword\"}]},\"monitoring\":{\"dashboards\":{\"annotations\":{},\"enabled\":false,\"labels\":{\"grafana_dashboard\":\"1\"},\"namespace\":null},\"rules\":{\"additionalGroups\":[],\"additionalRuleAnnotations\":{},\"additionalRuleLabels\":{},\"alerting\":true,\"annotations\":{},\"disabled\":{},\"enabled\":false,\"labels\":{},\"namespace\":null},\"selfMonitoring\":{\"enabled\":false,\"grafanaAgent\":{\"annotations\":{},\"enableConfigReadAPI\":false,\"installOperator\":false,\"labels\":{},\"priorityClassName\":null,\"resources\":{},\"tolerations\":[]},\"logsInstance\":{\"annotations\":{},\"clients\":null,\"labels\":{}},\"podLogs\":{\"additionalPipelineStages\":[],\"annotations\":{},\"apiVersion\":\"monitoring.grafana.com/v1alpha1\",\"labels\":{},\"relabelings\":[]},\"tenant\":{\"name\":\"self-monitoring\",\"password\":null,\"secretNamespace\":\"{{ include \\\"loki.namespace\\\" . }}\"}},\"serviceMonitor\":{\"annotations\":{},\"enabled\":false,\"interval\":\"15s\",\"labels\":{},\"metricRelabelings\":[],\"metricsInstance\":{\"annotations\":{},\"enabled\":true,\"labels\":{},\"remoteWrite\":null},\"namespaceSelector\":{},\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":null,\"tlsConfig\":null}},\"nameOverride\":null,\"namespaceOverride\":null,\"networkPolicy\":{\"alertmanager\":{\"namespaceSelector\":{},\"podSelector\":{},\"port\":9093},\"discovery\":{\"namespaceSelector\":{},\"podSelector\":{},\"port\":null},\"egressKubeApiserver\":{\"enabled\":false},\"egressWorld\":{\"enabled\":false},\"enabled\":false,\"externalStorage\":{\"cidrs\":[],\"ports\":[]},\"flavor\":\"kubernetes\",\"ingress\":{\"namespaceSelector\":{},\"podSelector\":{}},\"metrics\":{\"cidrs\":[],\"namespaceSelector\":{},\"podSelector\":{}}},\"overridesExporter\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"overrides-exporter\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"command\":null,\"dnsConfig\":{},\"enabled\":false,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"maxUnavailable\":null,\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"patternIngester\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"pattern-ingester\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"livenessProbe\":{},\"maxUnavailable\":null,\"nodeSelector\":{},\"persistence\":{\"claims\":[{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"labels\":{},\"name\":\"data\",\"size\":\"10Gi\",\"storageClass\":null}],\"enableStatefulSetAutoDeletePVC\":false,\"enabled\":false,\"size\":\"10Gi\",\"storageClass\":null,\"whenDeleted\":\"Retain\",\"whenScaled\":\"Retain\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{},\"replicas\":0,\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":false,\"imagePullSecrets\":[],\"name\":null},\"serviceAnnotations\":{},\"serviceLabels\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"querier\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"querier\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"autoscaling\":{\"behavior\":{\"enabled\":false,\"scaleDown\":{},\"scaleUp\":{}},\"customMetrics\":[],\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"command\":null,\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"maxSurge\":0,\"maxUnavailable\":null,\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"serviceType\":\"ClusterIP\",\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"querier\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"maxSkew\":1,\"topologyKey\":\"kubernetes.io/hostname\",\"whenUnsatisfiable\":\"ScheduleAnyway\"}]},\"queryFrontend\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"query-frontend\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"autoscaling\":{\"behavior\":{\"enabled\":false,\"scaleDown\":{},\"scaleUp\":{}},\"customMetrics\":[],\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"command\":null,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"maxUnavailable\":null,\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"serviceType\":\"ClusterIP\",\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"queryScheduler\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"query-scheduler\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"maxUnavailable\":1,\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"rbac\":{\"namespaced\":false,\"pspAnnotations\":{},\"pspEnabled\":false,\"sccAllowHostDirVolumePlugin\":false,\"sccEnabled\":false},\"read\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"read\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"annotations\":{},\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":6,\"minReplicas\":2,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"legacyReadTarget\":false,\"lifecycle\":{},\"livenessProbe\":{},\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enableStatefulSetAutoDeletePVC\":true,\"labels\":{},\"selector\":null,\"size\":\"10Gi\",\"storageClass\":null},\"podAnnotations\":{},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"selectorLabels\":{},\"service\":{\"annotations\":{},\"labels\":{},\"type\":\"ClusterIP\"},\"targetModule\":\"read\",\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"resultsCache\":{\"addresses\":\"dnssrvnoa+_memcached-client._tcp.{{ include \\\"loki.resourceName\\\" (dict \\\"ctx\\\" $ \\\"component\\\" \\\"results-cache\\\") }}.{{ include \\\"loki.namespace\\\" $ }}.svc\",\"affinity\":{},\"allocatedMemory\":1024,\"annotations\":{},\"connectionLimit\":16384,\"defaultValidity\":\"12h\",\"dnsConfig\":{},\"enabled\":false,\"extraArgs\":{},\"extraContainers\":[],\"extraExtendedOptions\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"initContainers\":[],\"maxItemMemory\":5,\"maxUnavailable\":1,\"nodeSelector\":{},\"persistence\":{\"enabled\":false,\"labels\":{},\"mountPath\":\"/data\",\"storageClass\":null,\"storageSize\":\"10G\"},\"podAnnotations\":{},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"port\":11211,\"priorityClassName\":null,\"replicas\":1,\"resources\":null,\"service\":{\"annotations\":{},\"labels\":{}},\"statefulStrategy\":{\"type\":\"RollingUpdate\"},\"terminationGracePeriodSeconds\":60,\"timeout\":\"500ms\",\"tolerations\":[],\"topologySpreadConstraints\":[],\"writebackBuffer\":500000,\"writebackParallelism\":1,\"writebackSizeLimit\":\"500MB\"},\"rollout_operator\":{\"enabled\":false,\"podSecurityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"ruler\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"ruler\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"appProtocol\":{\"grpc\":\"\"},\"command\":null,\"directories\":{},\"dnsConfig\":{},\"enabled\":true,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"maxUnavailable\":null,\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":false,\"labels\":{},\"size\":\"10Gi\",\"storageClass\":null},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"serviceAnnotations\":{},\"serviceLabels\":{},\"sidecar\":false,\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[]},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"imagePullSecrets\":[],\"labels\":{},\"name\":null},\"sidecar\":{\"enableUniqueFilenames\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"docker.io/kiwigrid/k8s-sidecar\",\"sha\":\"\",\"tag\":\"1.30.10\"},\"livenessProbe\":{},\"readinessProbe\":{},\"resources\":{},\"rules\":{\"enabled\":true,\"folder\":\"/rules\",\"folderAnnotation\":null,\"label\":\"loki_rule\",\"labelValue\":\"\",\"logLevel\":\"INFO\",\"resource\":\"both\",\"script\":null,\"searchNamespace\":null,\"watchClientTimeout\":60,\"watchMethod\":\"WATCH\",\"watchServerTimeout\":60},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"skipTlsVerify\":false,\"startupProbe\":{}},\"singleBinary\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"single-binary\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"annotations\":{},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[{\"emptyDir\":{},\"name\":\"loki-data\"}],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enableStatefulSetAutoDeletePVC\":true,\"enabled\":true,\"labels\":{},\"selector\":null,\"size\":\"10Gi\",\"storageClass\":null,\"whenDeleted\":\"Delete\",\"whenScaled\":\"Delete\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"selectorLabels\":{},\"service\":{\"annotations\":{},\"labels\":{},\"type\":\"ClusterIP\"},\"targetModule\":\"all\",\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"tableManager\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"table-manager\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"annotations\":{},\"command\":null,\"dnsConfig\":{},\"enabled\":false,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"resources\":{},\"retention_deletes_enabled\":false,\"retention_period\":0,\"service\":{\"annotations\":{},\"labels\":{}},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"test\":{\"annotations\":{},\"canaryServiceAddress\":\"http://loki-canary:3500/metrics\",\"enabled\":true,\"hostUsers\":\"nil\",\"image\":{\"digest\":null,\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/loki-helm-test\",\"tag\":\"latest\"},\"labels\":{},\"prometheusAddress\":\"\",\"timeout\":\"1m\"},\"write\":{\"affinity\":{\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchLabels\":{\"app.kubernetes.io/component\":\"write\",\"app.kubernetes.io/instance\":\"{{ .Release.Name }}\",\"app.kubernetes.io/name\":\"{{ include \\\"loki.name\\\" . }}\"}},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"annotations\":{},\"autoscaling\":{\"behavior\":{\"scaleDown\":{\"policies\":[{\"periodSeconds\":1800,\"type\":\"Pods\",\"value\":1}],\"stabilizationWindowSeconds\":3600},\"scaleUp\":{\"policies\":[{\"periodSeconds\":900,\"type\":\"Pods\",\"value\":1}]}},\"enabled\":false,\"maxReplicas\":6,\"minReplicas\":2,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"dnsConfig\":{},\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeClaimTemplates\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostUsers\":\"nil\",\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"initContainers\":[],\"lifecycle\":{},\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"dataVolumeParameters\":{\"emptyDir\":{}},\"enableStatefulSetAutoDeletePVC\":false,\"labels\":{},\"selector\":null,\"size\":\"10Gi\",\"storageClass\":null,\"volumeClaimsEnabled\":true},\"podAnnotations\":{},\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"priorityClassName\":null,\"replicas\":0,\"resources\":{},\"selectorLabels\":{},\"service\":{\"annotations\":{},\"labels\":{},\"type\":\"ClusterIP\"},\"targetModule\":\"write\",\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[]}}",
                "version": "6.43.0"
              }
            ],
            "name": "loki",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# -- Overrides the version used to determine compatibility of resources with the target Kubernetes cluster.\n# This is useful when using `helm template`, because then helm will use the client version of kubectl as the Kubernetes version,\n# which may or may not match your cluster's server version. Example: 'v1.24.4'. Set to null to use the version that helm\n# devises.\nkubeVersionOverride: null\n\nglobal:\n  image:\n    # -- Overrides the Docker registry globally for all images\n    registry: null\n  # -- Overrides the priorityClassName for all pods\n  priorityClassName: null\n  # -- configures cluster domain (\"cluster.local\" by default)\n  clusterDomain: \"cluster.local\"\n  # -- configures DNS service name\n  dnsService: \"kube-dns\"\n  # -- configures DNS service namespace\n  dnsNamespace: \"kube-system\"\n  # -- Common additional CLI arguments for all jobs (that is, -log.level debug, -config.expand-env=true or -log-config-reverse-order)\n  # scope: admin-api, backend, bloom-builder, bloom-gateway, bloom-planner, compactor, distributor, index-gateway, ingester, overrides-exporter, pattern-ingester, querier, query-frontend, query-scheduler, read, ruler, write.\n  extraArgs: []\n  # -- Common environment variables to add to all pods directly managed by this chart.\n  # scope: admin-api, backend, bloom-builder, bloom-gateway, bloom-planner, compactor, distributor, index-gateway, ingester, overrides-exporter, pattern-ingester, querier, query-frontend, query-scheduler, read, ruler, write.\n  extraEnv: []\n  # -- Common source of environment injections to add to all pods directly managed by this chart.\n  # scope: admin-api, backend, bloom-builder, bloom-gateway, bloom-planner, compactor, distributor, index-gateway, ingester, overrides-exporter, pattern-ingester, querier, query-frontend, query-scheduler, read, ruler, write.\n  # For example to inject values from a Secret, use:\n  # extraEnvFrom:\n  #   - secretRef:\n  #       name: mysecret\n  extraEnvFrom: []\n  # -- Common volumes to add to all pods directly managed by this chart.\n  # scope: admin-api, backend, bloom-builder, bloom-gateway, bloom-planner, compactor, distributor, index-gateway, ingester, overrides-exporter, pattern-ingester, querier, query-frontend, query-scheduler, read, ruler, write.\n  extraVolumes: []\n  # -- Common mount points to add to all pods directly managed by this chart.\n  # scope: admin-api, backend, bloom-builder, bloom-gateway, bloom-planner, compactor, distributor, index-gateway, ingester, overrides-exporter, pattern-ingester, querier, query-frontend, query-scheduler, read, ruler, write.\n  extraVolumeMounts: []\n# -- Overrides the chart's name\nnameOverride: null\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n# -- Overrides the chart's namespace\nnamespaceOverride: null\n# -- Overrides the chart's cluster label\nclusterLabelOverride: null\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n# -- Deployment mode lets you specify how to deploy Loki.\n# There are 3 options:\n# - SingleBinary: Loki is deployed as a single binary, useful for small installs typically without HA, up to a few tens of GB/day.\n# - SimpleScalable: Loki is deployed as 3 targets: read, write, and backend. Useful for medium installs easier to manage than distributed, up to a about 1TB/day.\n# - Distributed: Loki is deployed as individual microservices. The most complicated but most capable, useful for large installs, typically over 1TB/day.\n# There are also 2 additional modes used for migrating between deployment modes:\n# - SingleBinary\u003c-\u003eSimpleScalable: Migrate from SingleBinary to SimpleScalable (or vice versa)\n# - SimpleScalable\u003c-\u003eDistributed: Migrate from SimpleScalable to Distributed (or vice versa)\n# Note: SimpleScalable and Distributed REQUIRE the use of object storage.\ndeploymentMode: SingleBinary\n######################################################################################################################\n#\n# Base Loki Configs including kubernetes configurations and configurations for Loki itself,\n# see below for more specifics on Loki's configuration.\n#\n######################################################################################################################\n# -- Configuration for running Loki\n# @default -- See values.yaml\nloki:\n  # Configures the liveness probe for all of the Loki pods\n  livenessProbe: {}\n  # Configures the readiness probe for all of the Loki pods\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 30\n    timeoutSeconds: 1\n  # Configures the startup probe for all of the Loki pods\n  startupProbe: {}\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/loki\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: 3.5.5\n    # -- Overrides the image tag with an image digest\n    digest: null\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- Common annotations for all deployments/StatefulSets\n  annotations: {}\n  # -- Common annotations for all pods\n  podAnnotations: {}\n  # -- Common labels for all pods\n  podLabels: {}\n  # -- Common annotations for all services\n  serviceAnnotations: {}\n  # -- Common labels for all services\n  serviceLabels: {}\n  # -- The number of old ReplicaSets to retain to allow rollback\n  revisionHistoryLimit: 10\n  # -- The SecurityContext for Loki pods\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n  # -- The SecurityContext for Loki containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Should enableServiceLinks be enabled. Default to enable\n  enableServiceLinks: true\n  # -- DNS config for Loki pods\n  dnsConfig: {}\n  ######################################################################################################################\n  #\n  # Loki Configuration\n  #\n  # There are several ways to pass configuration to Loki, listing them here in order of our preference for how\n  # you should use this chart.\n  # 1. Use the templated value of loki.config below and the corresponding override sections which follow.\n  #    This allows us to set a lot of important Loki configurations and defaults and also allows us to maintain them\n  #    over time as Loki changes and evolves.\n  # 2. Use the loki.structuredConfig section.\n  #    This will completely override the templated value of loki.config, so you MUST provide the entire Loki config\n  #    including any configuration that we set in loki.config unless you explicitly are trying to change one of those\n  #    values and are not able to do so with the templated sections.\n  #    If you choose this approach the burden is on you to maintain any changes we make to the templated config.\n  # 3. Use an existing secret or configmap to provide the configuration.\n  #    This option is mostly provided for folks who have external processes which provide or modify the configuration.\n  #    When using this option you can specify a different name for loki.generatedConfigObjectName and configObjectName\n  #    if you have a process which takes the generated config and modifies it, or you can stop the chart from generating\n  #    a config entirely by setting loki.generatedConfigObjectName to\n  #\n  ######################################################################################################################\n\n  # -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.\n  # In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/loki/latest/configuration/#use-environment-variables-in-the-configuration).\n  # Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).\n  configStorageType: ConfigMap\n  # -- The name of the object which Loki will mount as a volume containing the config.\n  # If the configStorageType is Secret, this will be the name of the Secret, if it is ConfigMap, this will be the name of the ConfigMap.\n  # The value will be passed through tpl.\n  configObjectName: '{{ include \"loki.name\" . }}'\n  # -- The name of the Secret or ConfigMap that will be created by this chart.\n  # If empty, no configmap or secret will be created.\n  # The value will be passed through tpl.\n  generatedConfigObjectName: '{{ include \"loki.name\" . }}'\n  # -- Config file contents for Loki\n  # @default -- See values.yaml\n  config: |\n    {{- if .Values.enterprise.enabled}}\n    {{- tpl .Values.enterprise.config . }}\n    {{- else }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    {{- end }}\n\n    {{- with .Values.loki.server }}\n    server:\n      {{- toYaml . | nindent 2}}\n    {{- end}}\n\n    {{- with .Values.loki.pattern_ingester }}\n    pattern_ingester:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    memberlist:\n    {{- if .Values.loki.memberlistConfig }}\n      {{- toYaml .Values.loki.memberlistConfig | nindent 2 }}\n    {{- else }}\n    {{- if .Values.loki.extraMemberlistConfig}}\n    {{- toYaml .Values.loki.extraMemberlistConfig | nindent 2}}\n    {{- end }}\n      join_members:\n        - {{ include \"loki.memberlist\" . }}\n        {{- with .Values.migrate.fromDistributed }}\n        {{- if .enabled }}\n        - {{ .memberlistService }}\n        {{- end }}\n        {{- end }}\n    {{- end }}\n\n    {{- with .Values.loki.ingester }}\n    ingester:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.ingester_client }}\n    ingester_client:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.block_builder }}\n    block_builder:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- if .Values.loki.commonConfig}}\n    common:\n    {{- toYaml .Values.loki.commonConfig | nindent 2}}\n      storage:\n      {{- include \"loki.commonStorageConfig\" . | nindent 4}}\n    {{- end}}\n\n    {{- with .Values.loki.limits_config }}\n    limits_config:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    runtime_config:\n      file: /etc/loki/runtime-config/runtime-config.yaml\n\n    {{- if .Values.chunksCache.enabled }}\n    {{- with .Values.chunksCache }}\n    chunk_store_config:\n      chunk_cache_config:\n        default_validity: {{ .defaultValidity }}\n        background:\n          writeback_goroutines: {{ .writebackParallelism }}\n          writeback_buffer: {{ .writebackBuffer }}\n          writeback_size_limit: {{ .writebackSizeLimit }}\n        memcached:\n          batch_size: {{ .batchSize }}\n          parallelism: {{ .parallelism }}\n        memcached_client:\n          addresses: {{ .addresses }}\n          consistent_hash: true\n          timeout: {{ .timeout }}\n          max_idle_conns: 72\n      {{- end }}\n      {{- with .Values.chunksCache.l2 }}\n      {{- if .enabled }}\n      l2_chunk_cache_handoff: {{ .l2ChunkCacheHandoff }}\n      chunk_cache_config_l2:\n        default_validity: {{ .defaultValidity }}\n        background:\n          writeback_goroutines: {{ .writebackParallelism }}\n          writeback_buffer: {{ .writebackBuffer }}\n          writeback_size_limit: {{ .writebackSizeLimit }}\n        memcached:\n          batch_size: {{ .batchSize }}\n          parallelism: {{ .parallelism }}\n        memcached_client:\n          addresses: {{ .addresses }}\n          consistent_hash: true\n          timeout: {{ .timeout }}\n          max_idle_conns: 72\n      {{- end }}\n      {{- end }}\n    {{- end }}\n\n    {{- if .Values.loki.schemaConfig }}\n    schema_config:\n    {{- toYaml .Values.loki.schemaConfig | nindent 2}}\n    {{- end }}\n\n    {{- if .Values.loki.useTestSchema }}\n    schema_config:\n    {{- toYaml .Values.loki.testSchemaConfig | nindent 2}}\n    {{- end }}\n\n    {{- if .Values.ruler.enabled }}\n    {{ include \"loki.rulerConfig\" . }}\n    {{- end }}\n\n    {{- if and .Values.loki.storage.use_thanos_objstore .Values.ruler.enabled}}\n    ruler_storage:\n      {{- include \"loki.rulerThanosStorageConfig\" . | nindent 2 }}\n    {{- end }}\n\n    {{- if or .Values.tableManager.retention_deletes_enabled .Values.tableManager.retention_period }}\n    table_manager:\n      retention_deletes_enabled: {{ .Values.tableManager.retention_deletes_enabled }}\n      retention_period: {{ .Values.tableManager.retention_period }}\n    {{- end }}\n\n    query_range:\n      align_queries_with_step: true\n      {{- with .Values.loki.query_range }}\n      {{- tpl (. | toYaml) $ | nindent 2 }}\n      {{- end }}\n      {{- if .Values.resultsCache.enabled }}\n      {{- with .Values.resultsCache }}\n      cache_results: true\n      results_cache:\n        cache:\n          default_validity: {{ .defaultValidity }}\n          background:\n            writeback_goroutines: {{ .writebackParallelism }}\n            writeback_buffer: {{ .writebackBuffer }}\n            writeback_size_limit: {{ .writebackSizeLimit }}\n          memcached_client:\n            addresses: {{ .addresses }}\n            consistent_hash: true\n            timeout: {{ .timeout }}\n            update_interval: 1m\n      {{- end }}\n      {{- end }}\n\n    {{- with .Values.loki.storage_config }}\n    storage_config:\n        {{- if not (hasKey $.Values.loki.storage_config \"use_thanos_objstore\") }}\n        use_thanos_objstore: {{ $.Values.loki.storage.use_thanos_objstore }}\n        {{- end }}\n        {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.query_scheduler }}\n    query_scheduler:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.compactor }}\n    compactor:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.compactor_grpc_client }}\n    compactor_grpc_client:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.analytics }}\n    analytics:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- if .Values.loki.ui.enabled }}\n    ui:\n      enabled: true\n    {{- end }}\n    {{- with .Values.loki.querier }}\n    querier:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.index_gateway }}\n    index_gateway:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.frontend }}\n    frontend:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.frontend_worker }}\n    frontend_worker:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.distributor }}\n    distributor:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    tracing:\n      enabled: {{ .Values.loki.tracing.enabled }}\n\n    {{- with .Values.loki.bloom_build }}\n    bloom_build:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.bloom_gateway }}\n    bloom_gateway:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.operational_config }}\n    operational_config:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n  # Should authentication be enabled\n  auth_enabled: false\n  # -- memberlist configuration (overrides embedded default)\n  memberlistConfig: {}\n  # -- Extra memberlist configuration\n  extraMemberlistConfig: {}\n  # -- Tenants list to be created on nginx htpasswd file, with name and password or passwordHash keys\u003cbr\u003e\u003cbr\u003e\n  # Example:\n  # \u003cpre\u003e\n  # tenants:\u003cbr\u003e\n  #   - name: \"test-user-1\"\u003cbr\u003e\n  #     password: \"test-password-1\"\u003cbr\u003e\n  #   - name: \"test-user-2\"\u003cbr\u003e\n  #     passwordHash: \"$2y$10$7O40CaY1yz7fu9O24k2/u.ct/wELYHRBsn25v/7AyuQ8E8hrLqpva\" # generated using `htpasswd -nbBC10 test-user-2 test-password-2`\n  # \u003c/pre\u003e\n  tenants: []\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#server for more info on the server configuration.\n  server:\n    http_listen_port: 3100\n    grpc_listen_port: 9095\n    http_server_read_timeout: 600s\n    http_server_write_timeout: 600s\n  # -- Limits config\n  limits_config:\n    reject_old_samples: true\n    reject_old_samples_max_age: 168h\n    max_cache_freshness_per_query: 10m\n    split_queries_by_interval: 15m\n    query_timeout: 300s\n    volume_enabled: true\n  # -- Provides a reloadable runtime configuration file for some specific configuration\n  runtimeConfig: {}\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#common_config for more info on how to provide a common configuration\n  commonConfig:\n    path_prefix: /var/loki\n    replication_factor: 1\n    # -- The gRPC address of the compactor. The use of compactor_grpc_address is prefered over compactor_address.\n    # If a customized compactor_address is set, compactor_grpc_address should be set to an empty string.\n    compactor_grpc_address: '{{ include \"loki.compactorAddress\" . }}'\n  # -- Storage config. Providing this will automatically populate all necessary storage configs in the templated config.\n  # -- In case of using thanos storage, enable use_thanos_objstore and the configuration should be done inside the object_store section.\n  storage:\n    # Loki requires a bucket for chunks and the ruler. GEL requires a third bucket for the admin API.\n    # Please provide these values if you are using object storage.\n    # bucketNames:\n    #   chunks: FIXME\n    #   ruler: FIXME\n    #   admin: FIXME\n    type: filesystem\n    s3:\n      s3: null\n      endpoint: null\n      region: null\n      secretAccessKey: null\n      accessKeyId: null\n      signatureVersion: null\n      s3ForcePathStyle: false\n      insecure: false\n      http_config: {}\n      # -- Check https://grafana.com/docs/loki/latest/configure/#s3_storage_config for more info on how to provide a backoff_config\n      backoff_config: {}\n      disable_dualstack: false\n    gcs:\n      chunkBufferSize: 0\n      requestTimeout: \"0s\"\n      enableHttp2: true\n    azure:\n      accountName: null\n      accountKey: null\n      connectionString: null\n      useManagedIdentity: false\n      useFederatedToken: false\n      userAssignedId: null\n      requestTimeout: null\n      endpointSuffix: null\n      chunkDelimiter: null\n    swift:\n      auth_version: null\n      auth_url: null\n      internal: null\n      username: null\n      user_domain_name: null\n      user_domain_id: null\n      user_id: null\n      password: null\n      domain_id: null\n      domain_name: null\n      project_id: null\n      project_name: null\n      project_domain_id: null\n      project_domain_name: null\n      region_name: null\n      container_name: null\n      max_retries: null\n      connect_timeout: null\n      request_timeout: null\n    filesystem:\n      chunks_directory: /var/loki/chunks\n      rules_directory: /var/loki/rules\n\n    # Loki now supports using thanos storage clients for connecting to object storage backend.\n    # This will become the default way to configure storage in a future releases.\n    use_thanos_objstore: false\n\n    object_store:\n      # Type of object store. Valid options are: s3, gcs, azure\n      type: s3\n      # Optional prefix for storage keys\n      storage_prefix: null\n      # S3 configuration (when type is \"s3\")\n      s3:\n        # S3 endpoint URL\n        endpoint: null\n        # Optional region\n        region: null\n        # Optional access key\n        access_key_id: null\n        # Optional secret key\n        secret_access_key: null\n        # Optional. Enable if using self-signed TLS\n        insecure: false\n        # Optional server-side encryption configuration\n        sse: {}\n        # Optional HTTP client configuration\n        http: {}\n\n      # GCS configuration (when type is \"gcs\")\n      gcs:\n        # Name of the bucket\n        bucket_name: null\n        # Optional service account JSON\n        service_account: null\n\n      # Azure configuration (when type is \"azure\")\n      azure:\n        # Storage account name\n        account_name: null\n        # Optional storage account key\n        account_key: null\n\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas\n  schemaConfig: {}\n  # -- a real Loki install requires a proper schemaConfig defined above this, however for testing or playing around\n  # you can enable useTestSchema\n  useTestSchema: true\n  testSchemaConfig:\n    configs:\n      - from: 2024-04-01\n        store: tsdb\n        object_store: '{{ include \"loki.testSchemaObjectStore\" . }}'\n        schema: v13\n        index:\n          prefix: index_\n          period: 24h\n  ## A separate loki ruler storage configuration can be provided via rulerStorage.storage section:\n  ##  rulerConfig:\n  ##    storage:\n  ##      type: local\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler\n  rulerConfig:\n    wal:\n      dir: /var/loki/ruler-wal\n    # -- Storage for the ruler. If defining rules in `ruler.directories`, this must be configured to use local storage as shown below.\n    # storage:\n    #   type: local\n    #   local:\n    #     directory: /etc/loki/rules\n  # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig`\n  structuredConfig: {}\n  # -- Additional query scheduler config\n  query_scheduler: {}\n  # -- Additional storage config\n  storage_config:\n    boltdb_shipper:\n      index_gateway_client:\n        server_address: '{{ include \"loki.indexGatewayAddress\" . }}'\n    tsdb_shipper:\n      index_gateway_client:\n        server_address: '{{ include \"loki.indexGatewayAddress\" . }}'\n    bloom_shipper:\n      working_directory: /var/loki/data/bloomshipper\n    hedging:\n      at: \"250ms\"\n      max_per_second: 20\n      up_to: 3\n  # --  Optional compactor configuration\n  compactor: {}\n  # --  Optional compactor grpc client configuration\n  compactor_grpc_client: {}\n  # --  Optional pattern ingester configuration\n  pattern_ingester:\n    enabled: false\n  # --  Optional analytics configuration\n  analytics: {}\n  # --  Optional Loki UI: Provides access to a operators UI for Loki distributed. When enabled UI will be available at /ui/ of loki-gateway\n  ui:\n    # Disabled by default for backwards compatibility. Enable to use the Loki UI.\n    enabled: false\n    gateway:\n      # enable gateway proxying to UI under /ui\n      enabled: true\n  # --  Optional querier configuration\n  query_range: {}\n  # --  Optional querier configuration\n  querier: {}\n  # --  Optional ingester configuration\n  ingester: {}\n  # --  Optional ingester client configuration\n  ingester_client: {}\n  # --  Optional block builder configuration\n  block_builder: {}\n  # --  Optional index gateway configuration\n  index_gateway:\n    mode: simple\n  frontend:\n    scheduler_address: '{{ include \"loki.querySchedulerAddress\" . }}'\n    tail_proxy_url: '{{ include \"loki.querierAddress\" . }}'\n  frontend_worker:\n    scheduler_address: '{{ include \"loki.querySchedulerAddress\" . }}'\n  # -- Optional distributor configuration\n  distributor: {}\n  # -- Enable tracing\n  tracing:\n    enabled: false\n  bloom_build:\n    enabled: false\n    builder:\n      planner_address: '{{ include \"loki.bloomPlannerAddress\" . }}'\n  bloom_gateway:\n    enabled: false\n    client:\n      addresses: '{{ include \"loki.bloomGatewayAddresses\" . }}'\n  # -- Optional operational configuration\n  operational_config: {}\n######################################################################################################################\n#\n# Enterprise Loki Configs\n#\n######################################################################################################################\n\n# -- Configuration for running Enterprise Loki\nenterprise:\n  # Enable enterprise features, license must be provided\n  enabled: false\n  # Default verion of GEL to deploy\n  version: 3.5.2\n  # -- Optional name of the GEL cluster, otherwise will use .Release.Name\n  # The cluster name must match what is in your GEL license\n  cluster_name: null\n  # -- Grafana Enterprise Logs license\n  # In order to use Grafana Enterprise Logs features, you will need to provide\n  # the contents of your Grafana Enterprise Logs license, either by providing the\n  # contents of the license.jwt, or the name Kubernetes Secret that contains your\n  # license.jwt.\n  # To set the license contents, use the flag `--set-file 'enterprise.license.contents=./license.jwt'`\n  license:\n    contents: \"NOTAVALIDLICENSE\"\n  # -- Set to true when providing an external license\n  useExternalLicense: false\n  # -- Name of external license secret to use\n  externalLicenseName: null\n  # -- Name of the external config secret to use\n  externalConfigName: \"\"\n  # -- Use GEL gateway, if false will use the default nginx gateway\n  gelGateway: true\n  # -- If enabled, the correct admin_client storage will be configured. If disabled while running enterprise,\n  # make sure auth is set to `type: trust`, or that `auth_enabled` is set to `false`.\n  adminApi:\n    enabled: true\n  # enterprise specific sections of the config.yaml file\n  config: |\n    {{- if .Values.enterprise.adminApi.enabled }}\n    admin_client:\n      {{ include \"enterprise-logs.adminAPIStorageConfig\" . | nindent 2 }}\n    {{ end }}\n    auth:\n      type: {{ .Values.enterprise.adminApi.enabled | ternary \"enterprise\" \"trust\" }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    cluster_name: {{ include \"loki.clusterName\" . }}\n    license:\n      path: /etc/loki/license/license.jwt\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/enterprise-logs\n    # -- Docker image tag\n    tag: 3.5.5\n    # -- Overrides the image tag with an image digest\n    digest: null\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  adminToken:\n    # -- Name of external secret containing the admin token for enterprise provisioner\n    # This secret must exist before deploying and must contain a key named 'token'\n    secret: null\n  # -- Alternative name of the secret to store token for the canary\n  canarySecret: null\n  # -- Configuration for `provisioner` target\n  # Note: Uses enterprise.adminToken.secret value to mount the admin token used to call the admin api.\n  provisioner:\n    # -- Whether the job should be part of the deployment\n    enabled: true\n    # -- Name of the secret to store provisioned tokens in\n    provisionedSecretPrefix: null\n    # -- Hook type(s) to customize when the job runs.  defaults to post-install\n    hookType: \"post-install\"\n    # -- url of the admin api to use for the provisioner\n    apiUrl: '{{ include \"loki.address\" . }}'\n    # -- Additional tenants to be created. Each tenant will get a read and write policy\n    # and associated token. Tenant must have a name and a namespace for the secret containting\n    # the token to be created in. For example\n    # additionalTenants:\n    #   - name: loki\n    #     secretNamespace: grafana\n    additionalTenants: []\n    # -- Additional Kubernetes environment\n    env: []\n    # -- Additional labels for the `provisioner` Job\n    labels: {}\n    # -- Additional annotations for the `provisioner` Job\n    annotations: {}\n    # -- Affinity for provisioner Pods\n    # The value will be passed through tpl.\n    affinity: {}\n    # -- Node selector for provisioner Pods\n    nodeSelector: {}\n    # -- Tolerations for provisioner Pods\n    tolerations: []\n    # -- The name of the PriorityClass for provisioner Job\n    priorityClassName: null\n    # -- Use the host's user namespace in provisioner pods\n    hostUsers: nil\n    # -- Run containers as user `enterprise-logs(uid=10001)`\n    securityContext:\n      runAsNonRoot: true\n      runAsGroup: 10001\n      runAsUser: 10001\n      fsGroup: 10001\n    # -- Provisioner image to Utilize\n    image:\n      # -- The Docker registry\n      registry: us-docker.pkg.dev\n      # -- Docker image repository\n      repository: grafanalabs-global/docker-enterprise-provisioner-prod/enterprise-provisioner\n      # -- Overrides the image tag whose default is the chart's appVersion\n      tag: latest\n      # -- Overrides the image tag with an image digest\n      digest: null\n      # -- Docker image pull policy\n      pullPolicy: IfNotPresent\n    # -- Volume mounts to add to the provisioner pods\n    extraVolumeMounts: []\n    # -- Additional volumes for Pods\n    extraVolumes: []\n######################################################################################################################\n#\n# Chart Testing\n#\n######################################################################################################################\n\n# -- Section for configuring optional Helm test\ntest:\n  enabled: true\n  # -- Used to directly query the metrics endpoint of the canary for testing, this approach avoids needing prometheus for testing.\n  # This in a newer approach to using prometheusAddress such that tests do not have a dependency on prometheus\n  canaryServiceAddress: \"http://loki-canary:3500/metrics\"\n  # -- Address of the prometheus server to query for the test. This overrides any value set for canaryServiceAddress.\n  # This is kept for backward compatibility and may be removed in future releases. Previous value was 'http://prometheus:9090'\n  prometheusAddress: \"\"\n  # -- Number of times to retry the test before failing\n  timeout: 1m\n  # -- Additional labels for the test pods\n  labels: {}\n  # -- Additional annotations for test pods\n  annotations: {}\n  # -- Image to use for loki canary\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/loki-helm-test\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: \"latest\"\n    # -- Overrides the image tag with an image digest\n    digest: null\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- Use the host's user namespace in test pods\n  hostUsers: nil\n# The Loki canary pushes logs to and queries from this loki installation to test\n# that it's working correctly\nlokiCanary:\n  enabled: true\n  # -- The type of the loki canary k8s rollout. This can be a DaemonSet or Deployment.\n  kind: DaemonSet\n  # -- If true, the canary will send directly to Loki via the address configured for verification --\n  # -- If false, it will write to stdout and an Agent will be needed to scrape and send the logs --\n  push: true\n  # -- If set overwrites the default value set by loki.host helper function. Use this if gateway not enabled.\n  lokiurl: null\n  # -- The name of the label to look for at loki when doing the checks.\n  labelname: pod\n  # -- Additional annotations for the `loki-canary` Daemonset\n  annotations: {}\n  # -- Additional labels for each `loki-canary` pod\n  podLabels: {}\n  service:\n    # -- Annotations for loki-canary Service\n    annotations: {}\n    # -- Additional labels for loki-canary Service\n    labels: {}\n  # -- Additional CLI arguments for the `loki-canary' command\n  extraArgs: []\n  # -- Environment variables to add to the canary pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the canary pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the canary pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the canary pods\n  extraVolumes: []\n  # -- Resource requests and limits for the canary\n  resources: {}\n  # -- DNS config for canary pods\n  dnsConfig: {}\n  # -- Node selector for canary pods\n  nodeSelector: {}\n  # -- Tolerations for canary pods\n  tolerations: []\n  # -- Affinity for canary pods\n  affinity: {}\n  # -- The name of the PriorityClass for loki-canary pods\n  priorityClassName: null\n  # -- Use the host's user namespace in loki-canary pods\n  hostUsers: nil\n  # -- Image to use for loki canary\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/loki-canary\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: null\n    # -- Overrides the image tag with an image digest\n    digest: null\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- Update strategy for the `loki-canary` Daemonset pods\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  # -- Replicas for `loki-canary` when using a Deployment\n  replicas: 1\n######################################################################################################################\n#\n# Service Accounts and Kubernetes RBAC\n#\n######################################################################################################################\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n  # -- Labels for the service account\n  labels: {}\n  # -- Set this toggle to false to opt out of automounting API credentials for the service account\n  automountServiceAccountToken: true\n# RBAC configuration\nrbac:\n  # -- If pspEnabled true, a PodSecurityPolicy is created for K8s that use psp.\n  pspEnabled: false\n  # -- For OpenShift set pspEnabled to 'false' and sccEnabled to 'true' to use the SecurityContextConstraints.\n  sccEnabled: false\n  # -- Toggle this to true to allow the use of hostPath volumes on OpenShift\n  sccAllowHostDirVolumePlugin: false\n  # -- Specify PSP annotations\n  # Ref: https://kubernetes.io/docs/reference/access-authn-authz/psp-to-pod-security-standards/#podsecuritypolicy-annotations\n  pspAnnotations: {}\n  # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n  # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n  # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n  # -- Whether to install RBAC in the namespace only or cluster-wide. Useful if you want to watch ConfigMap globally.\n  namespaced: false\n######################################################################################################################\n#\n# Network Policy configuration\n#\n######################################################################################################################\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  # -- Specifies whether the policies created will be standard Network Policies (flavor: kubernetes)\n  # or Cilium Network Policies (flavor: cilium)\n  flavor: kubernetes\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  ingress:\n    # -- Specifies the Pods which are allowed to access the http port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the http port\n    namespaceSelector: {}\n  alertmanager:\n    # -- Specify the alertmanager port used for alerting\n    port: 9093\n    # -- Specifies the alertmanager Pods.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the alertmanager is running in\n    namespaceSelector: {}\n  externalStorage:\n    # -- Specify the port used for external storage, e.g. AWS S3\n    ports: []\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n  discovery:\n    # -- (int) Specify the port used for discovery\n    port: null\n    # -- Specifies the Pods labels used for discovery.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the discovery Pods are running in\n    namespaceSelector: {}\n  egressWorld:\n    # -- Enable additional cilium egress rules to external world for write, read and backend.\n    enabled: false\n  egressKubeApiserver:\n    # -- Enable additional cilium egress rules to kube-apiserver for backend.\n    enabled: false\n######################################################################################################################\n#\n# Global memberlist configuration\n#\n######################################################################################################################\n\n# Configuration for the memberlist service\nmemberlist:\n  service:\n    publishNotReadyAddresses: false\n    annotations: {}\n######################################################################################################################\n#\n# adminAPI configuration, enterprise only.\n#\n######################################################################################################################\n\n# -- Configuration for the `admin-api` target\nadminApi:\n  # -- Define the amount of instances\n  replicas: 1\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Additional CLI arguments for the `admin-api` target\n  extraArgs: {}\n  # -- Environment variables to add to the admin-api pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the admin-api pods\n  extraEnvFrom: []\n  # -- Additional labels for the `admin-api` Deployment\n  labels: {}\n  # -- Additional annotations for the `admin-api` Deployment\n  annotations: {}\n  # -- DNSConfig for `admin-api` pods\n  dnsConfig: {}\n  # -- Additional labels and annotations for the `admin-api` Service\n  service:\n    labels: {}\n    annotations: {}\n  # -- Run container as user `enterprise-logs(uid=10001)`\n  # `fsGroup` must not be specified, because these security options are applied\n  # on container level not on Pod level.\n  podSecurityContext:\n    runAsNonRoot: true\n    runAsGroup: 10001\n    runAsUser: 10001\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Update strategy\n  strategy:\n    type: RollingUpdate\n  # -- Liveness probe\n  livenessProbe: {}\n  # -- Readiness probe\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  # -- Startup probe\n  startupProbe: {}\n  # -- Request and limit Kubernetes resources\n  # -- Values are defined in small.yaml and large.yaml\n  resources: {}\n  # -- Configure optional environment variables\n  env: []\n  # -- Configure optional initContainers\n  initContainers: []\n  # -- Configure optional extraContainers\n  extraContainers: []\n  # -- Additional volumes for Pods\n  extraVolumes: []\n  # -- Additional volume mounts for Pods\n  extraVolumeMounts: []\n  # -- Affinity for admin-api Pods\n  # The value will be passed through tpl.\n  affinity: {}\n  # -- Node selector for admin-api Pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for admin-api pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for admin-api Pods\n  tolerations: []\n  # -- Grace period to allow the admin-api to shutdown before it is killed\n  terminationGracePeriodSeconds: 60\n  # -- Use the host's user namespace in admin-api pods\n  hostUsers: nil\n######################################################################################################################\n#\n# Gateway and Ingress\n#\n# By default this chart will deploy a Nginx container to act as a gateway which handles routing of traffic\n# and can also do auth.\n#\n# If you would prefer you can optionally disable this and enable using k8s ingress to do the incoming routing.\n#\n######################################################################################################################\n\n# Configuration for the gateway\ngateway:\n  # -- Specifies whether the gateway should be enabled\n  enabled: true\n  # -- Number of replicas for the gateway\n  replicas: 1\n  # -- Default container port\n  containerPort: 8080\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  autoscaling:\n    # -- Enable autoscaling for the gateway\n    enabled: false\n    # -- Minimum autoscaling replicas for the gateway\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the gateway\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the gateway\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the gateway\n    targetMemoryUtilizationPercentage:\n    # -- See `kubectl explain deployment.spec.strategy` for more\n    # -- ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\n    # -- Behavior policies while scaling.\n    behavior: {}\n    #    scaleUp:\n    #     stabilizationWindowSeconds: 300\n    #     policies:\n    #     - type: Pods\n    #       value: 1\n    #       periodSeconds: 60\n    #    scaleDown:\n    #     stabilizationWindowSeconds: 300\n    #     policies:\n    #     - type: Pods\n    #       value: 1\n    #       periodSeconds: 180\n  deploymentStrategy:\n    type: RollingUpdate\n  image:\n    # -- The Docker registry for the gateway image\n    registry: docker.io\n    # -- The gateway image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The gateway image tag\n    tag: 1.29-alpine\n    # -- Overrides the gateway image tag with an image digest\n    digest: null\n    # -- The gateway image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for gateway pods\n  priorityClassName: null\n  # -- Annotations for gateway deployment\n  annotations: {}\n  # -- Annotations for gateway pods\n  podAnnotations: {}\n  # -- Additional labels for gateway pods\n  podLabels: {}\n  # -- Additional CLI args for the gateway\n  extraArgs: []\n  # -- Environment variables to add to the gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the gateway pods\n  extraEnvFrom: []\n  # -- Lifecycle for the gateway container\n  lifecycle: {}\n  # -- Volumes to add to the gateway pods\n  extraVolumes: []\n  # -- Volume mounts to add to the gateway pods\n  extraVolumeMounts: []\n  # -- The SecurityContext for gateway containers\n  podSecurityContext:\n    fsGroup: 101\n    runAsGroup: 101\n    runAsNonRoot: true\n    runAsUser: 101\n  # -- The SecurityContext for gateway containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Use the host's user namespace in the gateway\n  hostUsers: nil\n  # -- Resource requests and limits for the gateway\n  resources: {}\n  # -- Containers to add to the gateway pods\n  extraContainers: []\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for gateway pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: gateway\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- DNS config for gateway pods\n  dnsConfig: {}\n  # -- Node selector for gateway pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for gateway pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for gateway pods\n  tolerations: []\n  # Gateway service configuration\n  service:\n    # -- Port of the gateway service\n    port: 80\n    # -- Type of the gateway service\n    type: ClusterIP\n    # -- ClusterIP of the gateway service\n    clusterIP: null\n    # -- (int) Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IPO address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the gateway service\n    annotations: {}\n    # -- Labels for gateway service\n    labels: {}\n  # Gateway ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the gateway should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    ingressClassName: \"\"\n    # -- Annotations for the gateway ingress\n    annotations: {}\n    # -- Labels for the gateway ingress\n    labels: {}\n    # -- Hosts configuration for the gateway ingress, passed through the `tpl` function to allow templating\n    hosts:\n      - host: gateway.loki.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the gateway ingress. Hosts passed through the `tpl` function to allow templating\n    tls:\n      - secretName: loki-gateway-tls\n        hosts:\n          - gateway.loki.example.com\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for the gateway\n    enabled: false\n    # -- The basic auth username for the gateway\n    username: null\n    # -- The basic auth password for the gateway\n    password: null\n    # -- Uses the specified users from the `loki.tenants` list to create the htpasswd file.\n    # if `loki.tenants` is not set, the `gateway.basicAuth.username` and `gateway.basicAuth.password` are used.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    # @default -- Either `loki.tenants` or `gateway.basicAuth.username` and `gateway.basicAuth.password`.\n    htpasswd: |\n      {{- with $tenants := .Values.loki.tenants }}\n        {{- range $t := $tenants }}\n          {{- $username := required \"All tenants must have a 'name' set\" $t.name }}\n          {{- if $passwordHash := $t.passwordHash }}\n            {{- printf \"%s:%s\\n\" $username $passwordHash }}\n          {{- else if $password := $t.password }}\n            {{- printf \"%s\\n\" (htpasswd $username $password) }}\n          {{- else }}\n            {{- fail \"All tenants must have a 'password' or 'passwordHash' set\" }}\n          {{- end }}\n        {{- end }}\n      {{- else }}\n        {{- printf \"%s\\n\" (htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password)) }}\n      {{- end }}\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # -- liveness probe for the nginx container in the gateway pods.\n  livenessProbe: {}\n  # Configures the readiness probe for the gateway\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http-metrics\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n  # -- startup probe for the nginx container in the gateway pods.\n  startupProbe: {}\n  nginxConfig:\n    # -- Which schema to be used when building URLs. Can be 'http' or 'https'.\n    schema: http\n    # -- Enable listener for IPv6, disable on IPv4-only systems\n    enableIPv6: true\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block, passed through the `tpl` function to allow templating\n    httpSnippet: \"\"\n    # -- Allows appending custom configuration inside every location block, useful for authentication or setting headers that are not inherited from the server block, passed through the `tpl` function to allow templating.\n    locationSnippet: \u003e-\n      {{ if .Values.loki.tenants }}proxy_set_header X-Scope-OrgID $remote_user;{{ end }}\n    # -- Allows customizing the `client_max_body_size` directive\n    clientMaxBodySize: 4M\n    # -- Whether ssl should be appended to the listen directive of the server block or not.\n    ssl: false\n    # -- Override Read URL\n    customReadUrl: null\n    # -- Override Write URL\n    customWriteUrl: null\n    # -- Override Backend URL\n    customBackendUrl: null\n    # -- Allows overriding the DNS resolver address nginx will use.\n    resolver: \"\"\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      {{- include \"loki.nginxFile\" . -}}\n# -- If running enterprise and using the default enterprise gateway, configs go here.\nenterpriseGateway:\n  # -- Define the amount of instances\n  replicas: 1\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the `gateway` pod\n  hostUsers: nil\n  # -- Additional CLI arguments for the `gateway` target\n  extraArgs: {}\n  # -- Environment variables from secrets or configmaps to add to the enterprise gateway pods\n  extraEnvFrom: []\n  # -- Additional labels for the `gateway` Pod\n  labels: {}\n  # -- Additional annotations for the `gateway` Pod\n  annotations: {}\n  # -- Additional labels and annotations for the `gateway` Service\n  # -- Service overriding service type\n  service:\n    type: ClusterIP\n    labels: {}\n    annotations: {}\n  # -- Run container as user `enterprise-logs(uid=10001)`\n  podSecurityContext:\n    runAsNonRoot: true\n    runAsGroup: 10001\n    runAsUser: 10001\n    fsGroup: 10001\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- If you want to use your own proxy URLs, set this to false.\n  useDefaultProxyURLs: true\n  # -- update strategy\n  strategy:\n    type: RollingUpdate\n  # -- Readiness probe\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  # -- Request and limit Kubernetes resources\n  # -- Values are defined in small.yaml and large.yaml\n  resources: {}\n  # -- Configure optional environment variables\n  env: []\n  # -- Configure optional initContainers\n  initContainers: []\n  # -- Conifgure optional extraContainers\n  extraContainers: []\n  # -- Additional volumes for Pods\n  extraVolumes: []\n  # -- Additional volume mounts for Pods\n  extraVolumeMounts: []\n  # -- Affinity for gateway Pods\n  # The value will be passed through tpl.\n  affinity: {}\n  # -- Node selector for gateway Pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for enterprise-gateway pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for gateway Pods\n  tolerations: []\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 60\n# -- Ingress configuration Use either this ingress or the gateway, but not both at once.\n# If you enable this, make sure to disable the gateway.\n# You'll need to supply authn configuration for your ingress controller.\ningress:\n  enabled: false\n  ingressClassName: \"\"\n  annotations: {}\n  #    nginx.ingress.kubernetes.io/auth-type: basic\n  #    nginx.ingress.kubernetes.io/auth-secret: loki-distributed-basic-auth\n  #    nginx.ingress.kubernetes.io/auth-secret-type: auth-map\n  #    nginx.ingress.kubernetes.io/configuration-snippet: |\n  #      proxy_set_header X-Scope-OrgID $remote_user;\n  labels: {}\n  #    blackbox.monitoring.exclude: \"true\"\n  paths:\n    # -- Paths that are exposed by Loki Distributor.\n    # If deployment mode is Distributed, the requests are forwarded to the service: `{{\"loki.distributorFullname\"}}`.\n    # If deployment mode is SimpleScalable, the requests are forwarded to write k8s service: `{{\"loki.writeFullname\"}}`.\n    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{\"loki.singleBinaryFullname\"}}`\n    distributor:\n      - /api/prom/push\n      - /loki/api/v1/push\n      - /otlp/v1/logs\n      - /ui\n    # -- Paths that are exposed by Loki Query Frontend.\n    # If deployment mode is Distributed, the requests are forwarded to the service: `{{\"loki.queryFrontendFullname\"}}`.\n    # If deployment mode is SimpleScalable, the requests are forwarded to write k8s service: `{{\"loki.readFullname\"}}`.\n    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{\"loki.singleBinaryFullname\"}}`\n    queryFrontend:\n      - /api/prom/query\n      # this path covers labels and labelValues endpoints\n      - /api/prom/label\n      - /api/prom/series\n      - /api/prom/tail\n      - /loki/api/v1/query\n      - /loki/api/v1/query_range\n      - /loki/api/v1/tail\n      # this path covers labels and labelValues endpoints\n      - /loki/api/v1/label\n      - /loki/api/v1/labels\n      - /loki/api/v1/series\n      - /loki/api/v1/index/stats\n      - /loki/api/v1/index/volume\n      - /loki/api/v1/index/volume_range\n      - /loki/api/v1/format_query\n      - /loki/api/v1/detected_field\n      - /loki/api/v1/detected_fields\n      - /loki/api/v1/detected_labels\n      - /loki/api/v1/patterns\n    # -- Paths that are exposed by Loki Ruler.\n    # If deployment mode is Distributed, the requests are forwarded to the service: `{{\"loki.rulerFullname\"}}`.\n    # If deployment mode is SimpleScalable, the requests are forwarded to k8s service: `{{\"loki.backendFullname\"}}`.\n    # If deployment mode is SimpleScalable but `read.legacyReadTarget` is `true`, the requests are forwarded to k8s service: `{{\"loki.readFullname\"}}`.\n    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{\"loki.singleBinaryFullname\"}}`\n    ruler:\n      - /api/prom/rules\n      - /api/prom/api/v1/rules\n      - /api/prom/api/v1/alerts\n      - /loki/api/v1/rules\n      - /prometheus/api/v1/rules\n      - /prometheus/api/v1/alerts\n    # -- Paths that are exposed by Loki Compactor.\n    # If deployment mode is Distributed, the requests are forwarded to the service: `{{\"loki.compactorFullname\"}}`.\n    # If deployment mode is SimpleScalable, the requests are forwarded to k8s service: `{{\"loki.backendFullname\"}}`.\n    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{\"loki.singleBinaryFullname\"}}`\n    compactor:\n      - /loki/api/v1/delete\n  # -- Hosts configuration for the ingress, passed through the `tpl` function to allow templating\n  hosts:\n    - loki.example.com\n  # -- TLS configuration for the ingress. Hosts passed through the `tpl` function to allow templating\n  tls: []\n#    - hosts:\n#       - loki.example.com\n#      secretName: loki-distributed-tls\n\n######################################################################################################################\n#\n# Migration\n#\n######################################################################################################################\n\n# -- Options that may be necessary when performing a migration from another helm chart\nmigrate:\n  # -- When migrating from a distributed chart like loki-distributed or enterprise-logs\n  fromDistributed:\n    # -- Set to true if migrating from a distributed helm chart\n    enabled: false\n    # -- If migrating from a distributed service, provide the distributed deployment's\n    # memberlist service DNS so the new deployment can join its ring.\n    memberlistService: \"\"\n######################################################################################################################\n#\n# Single Binary Deployment\n#\n# For small Loki installations up to a few 10's of GB per day, or for testing and development.\n#\n######################################################################################################################\n\n# Configuration for the single binary node(s)\nsingleBinary:\n  # -- Number of replicas for the single binary\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling\n    enabled: false\n    # -- Minimum autoscaling replicas for the single binary\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the single binary\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the single binary\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the single binary\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the single binary image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the single binary image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the single binary image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for single binary pods\n  priorityClassName: null\n  # -- Annotations for single binary StatefulSet\n  annotations: {}\n  # -- Annotations for single binary pods\n  podAnnotations: {}\n  # -- Additional labels for each `single binary` pod\n  podLabels: {}\n  # -- Additional selector labels for each `single binary` pod\n  selectorLabels: {}\n  service:\n    # -- Annotations for single binary Service\n    annotations: {}\n    # -- Additional labels for single binary Service\n    labels: {}\n    # -- Service Type for single binary Service\n    type: \"ClusterIP\"\n  # -- Comma-separated list of Loki modules to load for the single binary\n  targetModule: \"all\"\n  # -- Labels for single binary service\n  extraArgs: []\n  # -- Environment variables to add to the single binary pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the single binary pods\n  extraEnvFrom: []\n  # -- Extra containers to add to the single binary loki pod\n  extraContainers: []\n  # -- Init containers to add to the single binary pods\n  initContainers: []\n  # -- Volume mounts to add to the single binary pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the single binary pods\n  extraVolumes: [\n    {\n      name: \"loki-data\",\n      emptyDir: {}\n    }\n  ]\n  # -- Resource requests and limits for the single binary\n  resources: {}\n  # -- Grace period to allow the single binary to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Use the host's user namespace in the single binary pods\n  hostUsers: nil\n  # -- Affinity for single binary pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: single-binary\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- DNS config for single binary pods\n  dnsConfig: {}\n  # -- Node selector for single binary pods\n  nodeSelector: {}\n  # -- Tolerations for single binary pods\n  tolerations: []\n  persistence:\n    # -- What to do with the volume when the StatefulSet is scaled down.\n    whenScaled: Delete\n    # -- What to do with the volumes when the StatefulSet is deleted.\n    whenDeleted: Delete\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: true\n    # -- Enable persistent disk\n    enabled: true\n    # -- Set access modes on the PersistentVolumeClaim\n    accessModes:\n      - ReadWriteOnce\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Selector for persistent disk\n    selector: null\n    # -- Annotations for volume claim\n    annotations: {}\n    # -- Labels for volume claim\n    labels: {}\n######################################################################################################################\n#\n# Simple Scalable Deployment (SSD) Mode\n#\n# For small to medium size Loki deployments up to around 1 TB/day, this is the default mode for this helm chart\n#\n######################################################################################################################\n\n# Configuration for the write pod(s)\nwrite:\n  # -- Number of replicas for the write\n  replicas: 0\n  autoscaling:\n    # -- Enable autoscaling for the write.\n    enabled: false\n    # -- Minimum autoscaling replicas for the write.\n    minReplicas: 2\n    # -- Maximum autoscaling replicas for the write.\n    maxReplicas: 6\n    # -- Target CPU utilisation percentage for the write.\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilization percentage for the write.\n    targetMemoryUtilizationPercentage:\n    # -- Behavior policies while scaling.\n    behavior:\n      # -- see https://github.com/grafana/loki/blob/main/docs/sources/operations/storage/wal.md#how-to-scale-updown for scaledown details\n      scaleUp:\n        policies:\n          - type: Pods\n            value: 1\n            periodSeconds: 900\n      scaleDown:\n        policies:\n          - type: Pods\n            value: 1\n            periodSeconds: 1800\n        stabilizationWindowSeconds: 3600\n  image:\n    # -- The Docker registry for the write image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the write image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the write image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for write pods\n  priorityClassName: null\n  # -- Annotations for write StatefulSet\n  annotations: {}\n  # -- Annotations for write pods\n  podAnnotations: {}\n  # -- Additional labels for each `write` pod\n  podLabels: {}\n  # -- Additional selector labels for each `write` pod\n  selectorLabels: {}\n  service:\n    # -- Annotations for write Service\n    annotations: {}\n    # -- Additional labels for write Service\n    labels: {}\n    # -- Service Type for write Service\n    type: \"ClusterIP\"\n  # -- Comma-separated list of Loki modules to load for the write\n  targetModule: \"write\"\n  # -- Additional CLI args for the write\n  extraArgs: []\n  # -- Environment variables to add to the write pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the write pods\n  extraEnvFrom: []\n  # -- Lifecycle for the write container\n  lifecycle: {}\n  # -- The default /flush_shutdown preStop hook is recommended as part of the ingester\n  # scaledown process so it's added to the template by default when autoscaling is enabled,\n  # but it's disabled to optimize rolling restarts in instances that will never be scaled\n  # down or when using chunks storage with WAL disabled.\n  # https://github.com/grafana/loki/blob/main/docs/sources/operations/storage/wal.md#how-to-scale-updown\n  # -- Init containers to add to the write pods\n  initContainers: []\n  # -- Containers to add to the write pods\n  extraContainers: []\n  # -- Volume mounts to add to the write pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the write pods\n  extraVolumes: []\n  # -- volumeClaimTemplates to add to StatefulSet\n  extraVolumeClaimTemplates: []\n  # -- Resource requests and limits for the write\n  resources: {}\n  # -- Grace period to allow the write to shutdown before it is killed. Especially for the ingester,\n  # this must be increased. It must be long enough so writes can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Use the host's user namespace in the write pods.\n  hostUsers: nil\n  # -- Affinity for write pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: write\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- DNS config for write pods\n  dnsConfig: {}\n  # -- Node selector for write pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for write pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for write pods\n  tolerations: []\n  # -- The default is to deploy all pods in parallel.\n  podManagementPolicy: \"Parallel\"\n  persistence:\n    # -- Enable volume claims in pod spec\n    volumeClaimsEnabled: true\n    # -- Set access modes on the PersistentVolumeClaim\n    accessModes:\n      - ReadWriteOnce\n    # -- Parameters used for the `data` volume when volumeClaimEnabled if false\n    dataVolumeParameters:\n      emptyDir: {}\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: false\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Selector for persistent disk\n    selector: null\n    # -- Annotations for volume claim\n    annotations: {}\n    # -- Labels for volume claim\n    labels: {}\n# --  Configuration for the read pod(s)\nread:\n  # -- Number of replicas for the read\n  replicas: 0\n  autoscaling:\n    # -- Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`\n    enabled: false\n    # -- Minimum autoscaling replicas for the read\n    minReplicas: 2\n    # -- Maximum autoscaling replicas for the read\n    maxReplicas: 6\n    # -- Target CPU utilisation percentage for the read\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the read\n    targetMemoryUtilizationPercentage:\n    # -- Behavior policies while scaling.\n    behavior: {}\n    #  scaleUp:\n    #   stabilizationWindowSeconds: 300\n    #   policies:\n    #   - type: Pods\n    #     value: 1\n    #     periodSeconds: 60\n    #  scaleDown:\n    #   stabilizationWindowSeconds: 300\n    #   policies:\n    #   - type: Pods\n    #     value: 1\n    #     periodSeconds: 180\n  image:\n    # -- The Docker registry for the read image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the read image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the read image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for read pods\n  priorityClassName: null\n  # -- Annotations for read deployment\n  annotations: {}\n  # -- Annotations for read pods\n  podAnnotations: {}\n  # -- Additional labels for each `read` pod\n  podLabels: {}\n  # -- Additional selector labels for each `read` pod\n  selectorLabels: {}\n  service:\n    # -- Annotations for read Service\n    annotations: {}\n    # -- Additional labels for read Service\n    labels: {}\n    # -- Service Type for read Service\n    type: ClusterIP\n  # -- Comma-separated list of Loki modules to load for the read\n  targetModule: \"read\"\n  # -- Whether or not to use the 2 target type simple scalable mode (read, write) or the\n  # 3 target type (read, write, backend). Legacy refers to the 2 target type, so true will\n  # run two targets, false will run 3 targets.\n  legacyReadTarget: false\n  # -- Additional CLI args for the read\n  extraArgs: []\n  # -- init containers to add to the read pods\n  initContainers: []\n  # -- Containers to add to the read pods\n  extraContainers: []\n  # -- Environment variables to add to the read pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the read pods\n  extraEnvFrom: []\n  # -- Lifecycle for the read container\n  lifecycle: {}\n  # -- Volume mounts to add to the read pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the read pods\n  extraVolumes: []\n  # -- Resource requests and limits for the read\n  resources: {}\n  # -- liveness probe settings for read pods. If empty, applies no livenessProbe\n  livenessProbe: {}\n  # -- Grace period to allow the read to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Use the host's user namespace in the read pods.\n  hostUsers: nil\n  # -- Affinity for read pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: read\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- DNS config for read pods\n  dnsConfig: {}\n  # -- Node selector for read pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for read pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for read pods\n  tolerations: []\n  # -- The default is to deploy all pods in parallel.\n  podManagementPolicy: \"Parallel\"\n  # -- read.persistence is used only if legacyReadTarget is set to true\n  persistence:\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: true\n    # -- Set access modes on the PersistentVolumeClaim\n    accessModes:\n      - ReadWriteOnce\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Selector for persistent disk\n    selector: null\n    # -- Annotations for volume claim\n    annotations: {}\n    # -- Labels for volume claim\n    labels: {}\n# --  Configuration for the backend pod(s)\nbackend:\n  # -- Number of replicas for the backend\n  replicas: 0\n  autoscaling:\n    # -- Enable autoscaling for the backend.\n    enabled: false\n    # -- Minimum autoscaling replicas for the backend.\n    minReplicas: 0\n    # -- Maximum autoscaling replicas for the backend.\n    maxReplicas: 0\n    # -- Target CPU utilization percentage for the backend.\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilization percentage for the backend.\n    targetMemoryUtilizationPercentage:\n    # -- Behavior policies while scaling.\n    behavior: {}\n    #    scaleUp:\n    #     stabilizationWindowSeconds: 300\n    #     policies:\n    #     - type: Pods\n    #       value: 1\n    #       periodSeconds: 60\n    #    scaleDown:\n    #     stabilizationWindowSeconds: 300\n    #     policies:\n    #     - type: Pods\n    #       value: 1\n    #       periodSeconds: 180\n  image:\n    # -- The Docker registry for the backend image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the backend image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the backend image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for backend pods\n  priorityClassName: null\n  # -- Annotations for backend StatefulSet\n  annotations: {}\n  # -- Annotations for backend pods\n  podAnnotations: {}\n  # -- Additional labels for each `backend` pod\n  podLabels: {}\n  # -- Additional selector labels for each `backend` pod\n  selectorLabels: {}\n  service:\n    # -- Annotations for backend Service\n    annotations: {}\n    # -- Additional labels for backend Service\n    labels: {}\n    # -- Service type for backend Service\n    type: ClusterIP\n  # -- Comma-separated list of Loki modules to load for the backend\n  targetModule: \"backend\"\n  # -- Additional CLI args for the backend\n  extraArgs: []\n  # -- Environment variables to add to the backend pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the backend pods\n  extraEnvFrom: []\n  # -- Init containers to add to the backend pods\n  initContainers: []\n  # -- Containers to add to the backend pods\n  extraContainers: []\n  # -- Volume mounts to add to the backend pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the backend pods\n  extraVolumes: []\n  # -- Resource requests and limits for the backend\n  resources: {}\n  # -- Grace period to allow the backend to shutdown before it is killed. Especially for the ingester,\n  # this must be increased. It must be long enough so backends can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Use the host's user namespace in the backend pods.\n  hostUsers: nil\n  # -- Affinity for backend pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: backend\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- DNS config for backend pods\n  dnsConfig: {}\n  # -- Node selector for backend pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for backend pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for backend pods\n  tolerations: []\n  # -- The default is to deploy all pods in parallel.\n  podManagementPolicy: \"Parallel\"\n  persistence:\n    # -- Enable volume claims in pod spec\n    volumeClaimsEnabled: true\n    # -- Set access modes on the PersistentVolumeClaim\n    accessModes:\n      - ReadWriteOnce\n    # -- Parameters used for the `data` volume when volumeClaimEnabled if false\n    dataVolumeParameters:\n      emptyDir: {}\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: true\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Selector for persistent disk\n    selector: null\n    # -- Annotations for volume claim\n    annotations: {}\n    # -- Labels for volume claim\n    labels: {}\n######################################################################################################################\n#\n# Microservices Mode\n#\n# For large Loki deployments ingesting more than 1 TB/day\n#\n######################################################################################################################\n\n# -- Configuration for the ingester\ningester:\n  # -- Number of replicas for the ingester, when zoneAwareReplication.enabled is true, the total\n  # number of replicas will match this value with each zone having 1/3rd of the total replicas.\n  replicas: 0\n  # -- DNSConfig for ingester pods\n  dnsConfig: {}\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the ingester\n  hostUsers: nil\n  autoscaling:\n    # -- Enable autoscaling for the ingester\n    enabled: false\n    # -- Minimum autoscaling replicas for the ingester\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the ingester\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the ingester\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the ingester\n    targetMemoryUtilizationPercentage: null\n    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)\n    customMetrics: []\n    # - type: Pods\n    #   pods:\n    #     metric:\n    #       name: loki_lines_total\n    #     target:\n    #       type: AverageValue\n    #       averageValue: 10k\n    behavior:\n      # -- Enable autoscaling behaviours\n      enabled: false\n      # -- define scale down policies, must conform to HPAScalingRules\n      scaleDown: {}\n      # -- define scale up policies, must conform to HPAScalingRules\n      scaleUp: {}\n  image:\n    # -- The Docker registry for the ingester image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the ingester image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the ingester image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  labels: {}\n  priorityClassName: null\n  # -- Labels for ingester pods\n  podLabels: {}\n  # -- Annotations for ingester pods\n  podAnnotations: {}\n  # -- The name of the PriorityClass for ingester pods\n  # -- Labels for ingestor service\n  serviceLabels: {}\n  # -- Annotations for ingestor service\n  serviceAnnotations: {}\n  # -- Service type for ingestor service\n  serviceType: \"ClusterIP\"\n  # -- Additional CLI args for the ingester\n  extraArgs: []\n  # -- Environment variables to add to the ingester pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the ingester pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the ingester pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the ingester pods\n  extraVolumes: []\n  # -- Resource requests and limits for the ingester\n  resources: {}\n  # -- Containers to add to the ingester pods\n  extraContainers: []\n  # -- Init containers to add to the ingester pods\n  initContainers: []\n  # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Lifecycle for the ingester container\n  lifecycle: {}\n  # -- topologySpread for ingester pods.\n  # @default -- Defaults to allow skew no more than 1 node\n  # The value will be passed through tpl.\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: kubernetes.io/hostname\n      whenUnsatisfiable: ScheduleAnyway\n      labelSelector:\n        matchLabels:\n          app.kubernetes.io/component: ingester\n          app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n          app.kubernetes.io/instance: '{{ .Release.Name }}'\n  # -- Affinity for ingester pods. Ignored if zoneAwareReplication is enabled.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: ingester\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: 1\n  # -- Node selector for ingester pods\n  nodeSelector: {}\n  # -- Tolerations for ingester pods\n  tolerations: []\n  # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`\n  readinessProbe: {}\n  # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`\n  livenessProbe: {}\n  # -- UpdateStrategy for the ingester StatefulSets.\n  updateStrategy:\n    # -- One of  'OnDelete' or 'RollingUpdate'\n    type: RollingUpdate\n    # -- Optional for updateStrategy.type=RollingUpdate. See [Partitioned rolling updates](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions) in the StatefulSet docs for details.\n    # rollingUpdate:\n    #   partition: 0\n  persistence:\n    # -- Enable creating PVCs which is required when using boltdb-shipper\n    enabled: false\n    # -- Use emptyDir with ramdisk for storage. **Please note that all data in ingester will be lost on pod restart**\n    inMemory: false\n    # -- List of the ingester PVCs\n    # @notationType -- list\n    claims:\n      - name: data\n        # -- Set access modes on the PersistentVolumeClaim\n        accessModes:\n          - ReadWriteOnce\n        size: 10Gi\n        #   -- Storage class to be used.\n        #   If defined, storageClassName: \u003cstorageClass\u003e.\n        #   If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n        #   If empty or set to null, no storageClassName spec is\n        #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n        storageClass: null\n        # - name: wal\n        #   size: 150Gi\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: false\n    whenDeleted: Retain\n    whenScaled: Retain\n  # -- Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.\n  appProtocol:\n    # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n    grpc: \"\"\n  # -- Enabling zone awareness on ingesters will create 3 statefulests where all writes will send a replica to each zone.\n  # This is primarily intended to accelerate rollout operations by allowing for multiple ingesters within a single\n  # zone to be shutdown and restart simultaneously (the remaining 2 zones will be guaranteed to have at least one copy\n  # of the data).\n  # Note: This can be used to run Loki over multiple cloud provider availability zones however this is not currently\n  # recommended as Loki is not optimized for this and cross zone network traffic costs can become extremely high\n  # extremely quickly. Even with zone awareness enabled, it is recommended to run Loki in a single availability zone.\n  zoneAwareReplication:\n    # -- Enable zone awareness.\n    enabled: true\n    # -- The percent of replicas in each zone that will be restarted at once. In a value of 0-100\n    maxUnavailablePct: 33\n    # -- zoneA configuration\n    zoneA:\n      # -- optionally define a node selector for this zone\n      nodeSelector: null\n      # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host\n      # The value will be passed through tpl.\n      extraAffinity: {}\n      # -- Specific annotations to add to zone A statefulset\n      annotations: {}\n      # -- Specific annotations to add to zone A pods\n      podAnnotations: {}\n    zoneB:\n      # -- optionally define a node selector for this zone\n      nodeSelector: null\n      # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host\n      # The value will be passed through tpl.\n      extraAffinity: {}\n      # -- Specific annotations to add to zone B statefulset\n      annotations: {}\n      # -- Specific annotations to add to zone B pods\n      podAnnotations: {}\n    zoneC:\n      # -- optionally define a node selector for this zone\n      nodeSelector: null\n      # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host\n      # The value will be passed through tpl.\n      extraAffinity: {}\n      # -- Specific annotations to add to zone C statefulset\n      annotations: {}\n      # -- Specific annotations to add to zone C pods\n      podAnnotations: {}\n    # -- The migration block allows migrating non zone aware ingesters to zone aware ingesters.\n    migration:\n      enabled: false\n      excludeDefaultZone: false\n      readPath: false\n      writePath: false\n\n  # optionally allow adding arbitrary prefix to the ingester rollout-group label\n  rolloutGroupPrefix: null\n  # optionally allow adding 'loki-' prefix to ingester name label\n  addIngesterNamePrefix: false\n\n# --  Configuration for the distributor\ndistributor:\n  # -- Number of replicas for the distributor\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the distributor\n  hostUsers: nil\n  # -- DNSConfig for distributor pods\n  dnsConfig: {}\n  autoscaling:\n    # -- Enable autoscaling for the distributor\n    enabled: false\n    # -- Minimum autoscaling replicas for the distributor\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the distributor\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the distributor\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the distributor\n    targetMemoryUtilizationPercentage: null\n    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)\n    customMetrics: []\n    # - type: Pods\n    #   pods:\n    #     metric:\n    #       name: loki_lines_total\n    #     target:\n    #       type: AverageValue\n    #       averageValue: 10k\n    behavior:\n      # -- Enable autoscaling behaviours\n      enabled: false\n      # -- define scale down policies, must conform to HPAScalingRules\n      scaleDown: {}\n      # -- define scale up policies, must conform to HPAScalingRules\n      scaleUp: {}\n  image:\n    # -- The Docker registry for the distributor image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the distributor image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the distributor image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for distributor pods\n  priorityClassName: null\n  # -- Labels for distributor pods\n  podLabels: {}\n  # -- Annotations for distributor pods\n  podAnnotations: {}\n  # -- Labels for distributor service\n  serviceLabels: {}\n  # -- Annotations for distributor service\n  serviceAnnotations: {}\n  # -- Service type for distributor service\n  serviceType: ClusterIP\n  # -- Additional CLI args for the distributor\n  extraArgs: []\n  # -- Environment variables to add to the distributor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the distributor pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the distributor pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the distributor pods\n  extraVolumes: []\n  # -- Resource requests and limits for the distributor\n  resources: {}\n  # -- Init containers to add to the distributor pods\n  initContainers: []\n  # -- Containers to add to the distributor pods\n  extraContainers: []\n  # -- Grace period to allow the distributor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for distributor pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: distributor\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Max Surge for distributor pods\n  maxSurge: 0\n  # -- Node selector for distributor pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for distributor pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for distributor pods\n  tolerations: []\n  # -- Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.\n  appProtocol:\n    # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n    grpc: \"\"\n  # -- trafficDistribution for distributor service\n  trafficDistribution: \"\"\n# --  Configuration for the querier\nquerier:\n  # -- Number of replicas for the querier\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the querier\n  hostUsers: nil\n  autoscaling:\n    # -- Enable autoscaling for the querier, this is only used if `indexGateway.enabled: true`\n    enabled: false\n    # -- Minimum autoscaling replicas for the querier\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the querier\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the querier\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the querier\n    targetMemoryUtilizationPercentage: null\n    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)\n    customMetrics: []\n    # - type: External\n    #   external:\n    #     metric:\n    #       name: loki_inflight_queries\n    #     target:\n    #       type: AverageValue\n    #       averageValue: 12\n    behavior:\n      # -- Enable autoscaling behaviours\n      enabled: false\n      # -- define scale down policies, must conform to HPAScalingRules\n      scaleDown: {}\n      # -- define scale up policies, must conform to HPAScalingRules\n      scaleUp: {}\n  image:\n    # -- The Docker registry for the querier image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the querier image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the querier image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for querier pods\n  priorityClassName: null\n  # -- Labels for querier pods\n  podLabels: {}\n  # -- Annotations for querier pods\n  podAnnotations: {}\n  # -- Labels for querier service\n  serviceLabels: {}\n  # -- Annotations for querier service\n  serviceAnnotations: {}\n  # -- Service Type for querier service\n  serviceType: \"ClusterIP\"\n  # -- Additional CLI args for the querier\n  extraArgs: []\n  # -- Environment variables to add to the querier pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the querier pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the querier pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the querier pods\n  extraVolumes: []\n  # -- Resource requests and limits for the querier\n  resources: {}\n  # -- Containers to add to the querier pods\n  extraContainers: []\n  # -- Init containers to add to the querier pods\n  initContainers: []\n  # -- Grace period to allow the querier to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- topologySpread for querier pods.\n  # @default -- Defaults to allow skew no more then 1 node\n  # The value will be passed through tpl.\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: kubernetes.io/hostname\n      whenUnsatisfiable: ScheduleAnyway\n      labelSelector:\n        matchLabels:\n          app.kubernetes.io/component: querier\n          app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n          app.kubernetes.io/instance: '{{ .Release.Name }}'\n  # -- Affinity for querier pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: querier\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Max Surge for querier pods\n  maxSurge: 0\n  # -- Node selector for querier pods\n  nodeSelector: {}\n  # -- Tolerations for querier pods\n  tolerations: []\n  # -- DNSConfig for querier pods\n  dnsConfig: {}\n  # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.\n  appProtocol:\n    # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n    grpc: \"\"\n# -- Configuration for the query-frontend\nqueryFrontend:\n  # -- Number of replicas for the query-frontend\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the query-frontend\n  hostUsers: nil\n  autoscaling:\n    # -- Enable autoscaling for the query-frontend\n    enabled: false\n    # -- Minimum autoscaling replicas for the query-frontend\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the query-frontend\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the query-frontend\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the query-frontend\n    targetMemoryUtilizationPercentage: null\n    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)\n    customMetrics: []\n    # - type: Pods\n    #   pods:\n    #     metric:\n    #       name: loki_query_rate\n    #     target:\n    #       type: AverageValue\n    #       averageValue: 100\n    behavior:\n      # -- Enable autoscaling behaviours\n      enabled: false\n      # -- define scale down policies, must conform to HPAScalingRules\n      scaleDown: {}\n      # -- define scale up policies, must conform to HPAScalingRules\n      scaleUp: {}\n  image:\n    # -- The Docker registry for the query-frontend image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the query-frontend image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the query-frontend image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for query-frontend pods\n  priorityClassName: null\n  # -- Labels for query-frontend pods\n  podLabels: {}\n  # -- Annotations for query-frontend pods\n  podAnnotations: {}\n  # -- Labels for query-frontend service\n  serviceLabels: {}\n  # -- Annotations for query-frontend service\n  serviceAnnotations: {}\n  # -- Service Type for query-frontend service\n  serviceType: ClusterIP\n  # -- Additional CLI args for the query-frontend\n  extraArgs: []\n  # -- Environment variables to add to the query-frontend pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the query-frontend pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the query-frontend pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the query-frontend pods\n  extraVolumes: []\n  # -- Resource requests and limits for the query-frontend\n  resources: {}\n  # -- init containers to add to the query-frontend pods\n  initContainers: []\n  # -- Containers to add to the query-frontend pods\n  extraContainers: []\n  # -- Grace period to allow the query-frontend to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for query-frontend pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: query-frontend\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Node selector for query-frontend pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for query-frontend pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for query-frontend pods\n  tolerations: []\n  # -- Adds the appProtocol field to the queryFrontend service. This allows queryFrontend to work with istio protocol selection.\n  appProtocol:\n    # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n    grpc: \"\"\n# -- Configuration for the query-scheduler\nqueryScheduler:\n  # -- Number of replicas for the query-scheduler.\n  # It should be lower than `-querier.max-concurrent` to avoid generating back-pressure in queriers;\n  # it's also recommended that this value evenly divides the latter\n  replicas: 0\n  # -- DNSConfig for query-scheduler\n  dnsConfig: {}\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the query-scheduler\n  hostUsers: nil\n  image:\n    # -- The Docker registry for the query-scheduler image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the query-scheduler image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the query-scheduler image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for query-scheduler pods\n  priorityClassName: null\n  # -- Labels for query-scheduler pods\n  podLabels: {}\n  # -- Annotations for query-scheduler pods\n  podAnnotations: {}\n  # -- Labels for query-scheduler service\n  serviceLabels: {}\n  # -- Annotations for query-scheduler service\n  serviceAnnotations: {}\n  # -- Additional CLI args for the query-scheduler\n  extraArgs: []\n  # -- Environment variables to add to the query-scheduler pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the query-scheduler pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the query-scheduler pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the query-scheduler pods\n  extraVolumes: []\n  # -- Resource requests and limits for the query-scheduler\n  resources: {}\n  # -- init containers to add to the query-scheduler pods\n  initContainers: []\n  # -- Containers to add to the query-scheduler pods\n  extraContainers: []\n  # -- Grace period to allow the query-scheduler to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for query-scheduler pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: query-scheduler\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: 1\n  # -- Node selector for query-scheduler pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for query-scheduler pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for query-scheduler pods\n  tolerations: []\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n# -- Configuration for the index-gateway\nindexGateway:\n  # -- Number of replicas for the index-gateway\n  replicas: 0\n  # -- Whether the index gateway should join the memberlist hashring\n  joinMemberlist: true\n  # -- DNSConfig for index-gateway pods\n  dnsConfig: {}\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the index-gateway\n  hostUsers: nil\n  image:\n    # -- The Docker registry for the index-gateway image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the index-gateway image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the index-gateway image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for index-gateway pods\n  priorityClassName: null\n  # -- Labels for index-gateway pods\n  podLabels: {}\n  # -- Annotations for index-gateway pods\n  podAnnotations: {}\n  # -- Labels for index-gateway service\n  serviceLabels: {}\n  # -- Annotations for index-gateway service\n  serviceAnnotations: {}\n  # -- Service type for index-gateway service\n  serviceType: \"ClusterIP\"\n  # -- Additional CLI args for the index-gateway\n  extraArgs: []\n  # -- Environment variables to add to the index-gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the index-gateway pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the index-gateway pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the index-gateway pods\n  extraVolumes: []\n  # -- Resource requests and limits for the index-gateway\n  resources: {}\n  # -- Containers to add to the index-gateway pods\n  extraContainers: []\n  # -- Init containers to add to the index-gateway pods\n  initContainers: []\n  # -- Grace period to allow the index-gateway to shutdown before it is killed.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for index-gateway pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: index-gateway\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Node selector for index-gateway pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for index-gateway pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for index-gateway pods\n  tolerations: []\n  persistence:\n    # -- Enable creating PVCs which is required when using boltdb-shipper\n    enabled: false\n    # -- Set access modes on the PersistentVolumeClaim\n    accessModes:\n      - ReadWriteOnce\n    # -- Use emptyDir with ramdisk for storage. **Please note that all data in indexGateway will be lost on pod restart**\n    inMemory: false\n    # -- Size of persistent or memory disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Annotations for index gateway PVCs\n    annotations: {}\n    # -- Labels for index gateway PVCs\n    labels: {}\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: false\n    whenDeleted: Retain\n    whenScaled: Retain\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n  # -- UpdateStrategy for the indexGateway StatefulSet.\n  updateStrategy:\n    # -- One of  'OnDelete' or 'RollingUpdate'\n    type: RollingUpdate\n    # -- Optional for updateStrategy.type=RollingUpdate. See [Partitioned rolling updates](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions) in the StatefulSet docs for details.\n    # rollingUpdate:\n    #   partition: 0\n# -- Configuration for the compactor\ncompactor:\n  # -- Number of replicas for the compactor\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the compactor\n  hostUsers: nil\n  # -- DNSConfig for compactor pods\n  dnsConfig: {}\n  image:\n    # -- The Docker registry for the compactor image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the compactor image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the compactor image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for compactor pods\n  priorityClassName: null\n  # -- Labels for compactor pods\n  podLabels: {}\n  # -- Annotations for compactor pods\n  podAnnotations: {}\n  # -- Affinity for compactor pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: compactor\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Labels for compactor service\n  serviceLabels: {}\n  # -- Annotations for compactor service\n  serviceAnnotations: {}\n  # -- Service type for compactor service\n  serviceType: \"ClusterIP\"\n  # -- Additional CLI args for the compactor\n  extraArgs: []\n  # -- Environment variables to add to the compactor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the compactor pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the compactor pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the compactor pods\n  extraVolumes: []\n  # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`\n  readinessProbe: {}\n  # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`\n  livenessProbe: {}\n  # -- Resource requests and limits for the compactor\n  resources: {}\n  # -- Containers to add to the compactor pods\n  extraContainers: []\n  # -- Init containers to add to the compactor pods\n  initContainers: []\n  # -- Grace period to allow the compactor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Node selector for compactor pods\n  nodeSelector: {}\n  # -- Tolerations for compactor pods\n  tolerations: []\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n  persistence:\n    # -- Enable creating PVCs for the compactor\n    enabled: false\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- List of the compactor PVCs\n    # @notationType -- list\n    claims:\n      - name: data\n        # -- Set access modes on the PersistentVolumeClaim\n        accessModes:\n          - ReadWriteOnce\n        size: 10Gi\n        #   -- Storage class to be used.\n        #   If defined, storageClassName: \u003cstorageClass\u003e.\n        #   If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n        #   If empty or set to null, no storageClassName spec is\n        #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n        storageClass: null\n        # -- Annotations for compactor PVCs\n        annotations: {}\n        # -- Labels for compactor PVCs\n        labels: {}\n        # - name: wal\n        #   size: 150Gi\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: false\n    whenDeleted: Retain\n    whenScaled: Retain\n  serviceAccount:\n    create: false\n    # -- The name of the ServiceAccount to use for the compactor.\n    # If not set and create is true, a name is generated by appending\n    # \"-compactor\" to the common ServiceAccount.\n    name: null\n    # -- Image pull secrets for the compactor service account\n    imagePullSecrets: []\n    # -- Annotations for the compactor service account\n    annotations: {}\n    # -- Set this toggle to false to opt out of automounting API credentials for the service account\n    automountServiceAccountToken: true\n# -- Configuration for the bloom-gateway\nbloomGateway:\n  # -- Number of replicas for the bloom-gateway\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the bloom-gateway\n  hostUsers: nil\n  # -- DNSConfig for bloom-gateway pods\n  dnsConfig: {}\n  image:\n    # -- The Docker registry for the bloom-gateway image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the bloom-gateway image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the bloom-gateway image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for bloom-gateway pods\n  priorityClassName: null\n  # -- Labels for bloom-gateway pods\n  podLabels: {}\n  # -- Annotations for bloom-gateway pods\n  podAnnotations: {}\n  # -- Affinity for bloom-gateway pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: bloom-gateway\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Labels for bloom-gateway service\n  serviceLabels: {}\n  # -- Annotations for bloom-gateway service\n  serviceAnnotations: {}\n  # -- Additional CLI args for the bloom-gateway\n  extraArgs: []\n  # -- Environment variables to add to the bloom-gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the bloom-gateway pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the bloom-gateway pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the bloom-gateway pods\n  extraVolumes: []\n  # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`\n  readinessProbe: {}\n  # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`\n  livenessProbe: {}\n  # -- startup probe settings for ingester pods. If empty, use `loki.startupProbe`\n  startupProbe: {}\n  # -- Resource requests and limits for the bloom-gateway\n  resources: {}\n  # -- Containers to add to the bloom-gateway pods\n  extraContainers: []\n  # -- Init containers to add to the bloom-gateway pods\n  initContainers: []\n  # -- Grace period to allow the bloom-gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Node selector for bloom-gateway pods\n  nodeSelector: {}\n  # -- Tolerations for bloom-gateway pods\n  tolerations: []\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n  persistence:\n    # -- Enable creating PVCs for the bloom-gateway\n    enabled: false\n    # -- Annotations for bloom-gateway PVCs\n    annotations: {}\n    # -- Labels for bloom gateway PVCs\n    labels: {}\n    # -- List of the bloom-gateway PVCs\n    # @notationType -- list\n    claims:\n      - name: data\n        # -- Set access modes on the PersistentVolumeClaim\n        accessModes:\n          - ReadWriteOnce\n        # -- Size of persistent disk\n        size: 10Gi\n        #   -- Storage class to be used.\n        #   If defined, storageClassName: \u003cstorageClass\u003e.\n        #   If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n        #   If empty or set to null, no storageClassName spec is\n        #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n        storageClass: null\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: false\n    whenDeleted: Retain\n    whenScaled: Retain\n  serviceAccount:\n    create: false\n    # -- The name of the ServiceAccount to use for the bloom-gateway.\n    # If not set and create is true, a name is generated by appending\n    # \"-bloom-gateway\" to the common ServiceAccount.\n    name: null\n    # -- Image pull secrets for the bloom-gateway service account\n    imagePullSecrets: []\n    # -- Annotations for the bloom-gateway service account\n    annotations: {}\n    # -- Set this toggle to false to opt out of automounting API credentials for the service account\n    automountServiceAccountToken: true\n\n# -- Configuration for the bloom-planner\nbloomPlanner:\n  # -- Number of replicas for the bloom-planner\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the bloom-planner\n  hostUsers: nil\n  # -- DNSConfig for bloom-planner pods\n  dnsConfig: {}\n  image:\n    # -- The Docker registry for the bloom-planner image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the bloom-planner image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the bloom-planner image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for bloom-planner pods\n  priorityClassName: null\n  # -- Labels for bloom-planner pods\n  podLabels: {}\n  # -- Annotations for bloom-planner pods\n  podAnnotations: {}\n  # -- Affinity for bloom-planner pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: bloom-planner\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Labels for bloom-planner service\n  serviceLabels: {}\n  # -- Annotations for bloom-planner service\n  serviceAnnotations: {}\n  # -- Additional CLI args for the bloom-planner\n  extraArgs: []\n  # -- Environment variables to add to the bloom-planner pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the bloom-planner pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the bloom-planner pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the bloom-planner pods\n  extraVolumes: []\n  # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`\n  readinessProbe: {}\n  # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`\n  livenessProbe: {}\n  # -- startup probe settings for ingester pods. If empty use `loki.startupProbe`\n  startupProbe: {}\n  # -- Resource requests and limits for the bloom-planner\n  resources: {}\n  # -- Containers to add to the bloom-planner pods\n  extraContainers: []\n  # -- Init containers to add to the bloom-planner pods\n  initContainers: []\n  # -- Grace period to allow the bloom-planner to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Node selector for bloom-planner pods\n  nodeSelector: {}\n  # -- Tolerations for bloom-planner pods\n  tolerations: []\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n  persistence:\n    # -- Enable creating PVCs for the bloom-planner\n    enabled: false\n    # -- List of the bloom-planner PVCs\n    # @notationType -- list\n    claims:\n      - name: data\n        # -- Set access modes on the PersistentVolumeClaim\n        accessModes:\n          - ReadWriteOnce\n        # -- Size of persistent disk\n        size: 10Gi\n        #   -- Storage class to be used.\n        #   If defined, storageClassName: \u003cstorageClass\u003e.\n        #   If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n        #   If empty or set to null, no storageClassName spec is\n        #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n        storageClass: null\n        # -- Annotations for bloom-planner PVCs\n        annotations: {}\n        # -- Labels for bloom planner PVCs\n        labels: {}\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: false\n    whenDeleted: Retain\n    whenScaled: Retain\n  serviceAccount:\n    create: false\n    # -- The name of the ServiceAccount to use for the bloom-planner.\n    # If not set and create is true, a name is generated by appending\n    # \"-bloom-planner\" to the common ServiceAccount.\n    name: null\n    # -- Image pull secrets for the bloom-planner service account\n    imagePullSecrets: []\n    # -- Annotations for the bloom-planner service account\n    annotations: {}\n    # -- Set this toggle to false to opt out of automounting API credentials for the service account\n    automountServiceAccountToken: true\n\n# -- Configuration for the bloom-builder\nbloomBuilder:\n  # -- Number of replicas for the bloom-builder\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the boom-builder\n  hostUsers: nil\n  # -- DNSConfig for bloom-builder pods\n  dnsConfig: {}\n  autoscaling:\n    # -- Enable autoscaling for the bloom-builder\n    enabled: false\n    # -- Minimum autoscaling replicas for the bloom-builder\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the bloom-builder\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the bloom-builder\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the bloom-builder\n    targetMemoryUtilizationPercentage: null\n    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)\n    customMetrics: []\n    # - type: Pods\n    #   pods:\n    #     metric:\n    #       name: loki_query_rate\n    #     target:\n    #       type: AverageValue\n    #       averageValue: 100\n    behavior:\n      # -- Enable autoscaling behaviours\n      enabled: false\n      # -- define scale down policies, must conform to HPAScalingRules\n      scaleDown: {}\n      # -- define scale up policies, must conform to HPAScalingRules\n      scaleUp: {}\n  image:\n    # -- The Docker registry for the bloom-builder image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the bloom-builder image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the bloom-builder image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for bloom-builder pods\n  priorityClassName: null\n  # -- Labels for bloom-builder pods\n  podLabels: {}\n  # -- Annotations for bloom-builder pods\n  podAnnotations: {}\n  # -- Labels for bloom-builder service\n  serviceLabels: {}\n  # -- Annotations for bloom-builder service\n  serviceAnnotations: {}\n  # -- Additional CLI args for the bloom-builder\n  extraArgs: []\n  # -- Environment variables to add to the bloom-builder pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the bloom-builder pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the bloom-builder pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the bloom-builder pods\n  extraVolumes: []\n  # -- Resource requests and limits for the bloom-builder\n  resources: {}\n  # -- Init containers to add to the bloom-builder pods\n  initContainers: []\n  # -- Containers to add to the bloom-builder pods\n  extraContainers: []\n  # -- Grace period to allow the bloom-builder to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for bloom-builder pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: bloom-builder\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Node selector for bloom-builder pods\n  nodeSelector: {}\n  # -- Tolerations for bloom-builder pods\n  tolerations: []\n  # -- Adds the appProtocol field to the queryFrontend service. This allows bloomBuilder to work with istio protocol selection.\n  appProtocol:\n    # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n    grpc: \"\"\n\n# -- Configuration for the pattern ingester\npatternIngester:\n  # -- Number of replicas for the pattern ingester\n  replicas: 0\n  # -- DNSConfig for pattern ingester pods\n  dnsConfig: {}\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the pattern ingester\n  hostUsers: nil\n  image:\n    # -- The Docker registry for the pattern ingester image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the pattern ingester image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the pattern ingester image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for pattern ingester pods\n  priorityClassName: null\n  # -- Labels for pattern ingester pods\n  podLabels: {}\n  # -- Annotations for pattern ingester pods\n  podAnnotations: {}\n  # -- Affinity for pattern ingester pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: pattern-ingester\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Labels for pattern ingester service\n  serviceLabels: {}\n  # -- Annotations for pattern ingester service\n  serviceAnnotations: {}\n  # -- Additional CLI args for the pattern ingester\n  extraArgs: []\n  # -- Environment variables to add to the pattern ingester pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the pattern ingester pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the pattern ingester pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the pattern ingester pods\n  extraVolumes: []\n  # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`\n  readinessProbe: {}\n  # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`\n  livenessProbe: {}\n  # -- Resource requests and limits for the pattern ingester\n  resources: {}\n  # -- Containers to add to the pattern ingester pods\n  extraContainers: []\n  # -- Init containers to add to the pattern ingester pods\n  initContainers: []\n  # -- Grace period to allow the pattern ingester to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Node selector for pattern ingester pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for pattern ingester pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for pattern ingester pods\n  tolerations: []\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n  persistence:\n    # -- Enable creating PVCs for the pattern ingester\n    enabled: false\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- List of the pattern ingester PVCs\n    # @notationType -- list\n    claims:\n      - name: data\n        # -- Set access modes on the PersistentVolumeClaim\n        accessModes:\n          - ReadWriteOnce\n        size: 10Gi\n        #   -- Storage class to be used.\n        #   If defined, storageClassName: \u003cstorageClass\u003e.\n        #   If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n        #   If empty or set to null, no storageClassName spec is\n        #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n        storageClass: null\n        # -- Annotations for pattern ingester PVCs\n        annotations: {}\n        # -- Labels for pattern ingester PVCs\n        labels: {}\n        # - name: wal\n        #   size: 150Gi\n    # -- Enable StatefulSetAutoDeletePVC feature\n    enableStatefulSetAutoDeletePVC: false\n    whenDeleted: Retain\n    whenScaled: Retain\n  serviceAccount:\n    create: false\n    # -- The name of the ServiceAccount to use for the pattern ingester.\n    # If not set and create is true, a name is generated by appending\n    # \"-pattern-ingester\" to the common ServiceAccount.\n    name: null\n    # -- Image pull secrets for the pattern ingester service account\n    imagePullSecrets: []\n    # -- Annotations for the pattern ingester service account\n    annotations: {}\n    # -- Set this toggle to false to opt out of automounting API credentials for the service account\n    automountServiceAccountToken: true\n# -- Configuration for the ruler\nruler:\n  # -- The ruler component is optional and can be disabled if desired.\n  enabled: true\n  # -- Whether to enable the rules sidecar\n  sidecar: false\n  # -- Number of replicas for the ruler\n  replicas: 0\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the ruler\n  hostUsers: nil\n  image:\n    # -- The Docker registry for the ruler image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the ruler image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the ruler image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for ruler pods\n  priorityClassName: null\n  # -- Labels for compactor pods\n  podLabels: {}\n  # -- Annotations for ruler pods\n  podAnnotations: {}\n  # -- Labels for ruler service\n  serviceLabels: {}\n  # -- Annotations for ruler service\n  serviceAnnotations: {}\n  # -- Additional CLI args for the ruler\n  extraArgs: []\n  # -- Environment variables to add to the ruler pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the ruler pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the ruler pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the ruler pods\n  extraVolumes: []\n  # -- Resource requests and limits for the ruler\n  resources: {}\n  # -- Containers to add to the ruler pods\n  extraContainers: []\n  # -- Init containers to add to the ruler pods\n  initContainers: []\n  # -- Grace period to allow the ruler to shutdown before it is killed\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for ruler pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: ruler\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Node selector for ruler pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for ruler pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for ruler pods\n  tolerations: []\n  # -- DNSConfig for ruler pods\n  dnsConfig: {}\n  persistence:\n    # -- Enable creating PVCs which is required when using recording rules\n    enabled: false\n    # -- Set access modes on the PersistentVolumeClaim\n    accessModes:\n      - ReadWriteOnce\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Annotations for ruler PVCs\n    annotations: {}\n    # -- Labels for ruler PVCs\n    labels: {}\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n  # -- Directories containing rules files. If used, you must also configure `loki.rulerConfig.storage` to use local storage.\n  directories: {}\n  # tenant_foo:\n  #   rules1.txt: |\n  #     groups:\n  #       - name: should_fire\n  #         rules:\n  #           - alert: HighPercentageError\n  #             expr: |\n  #               sum(rate({app=\"foo\", env=\"production\"} |= \"error\" [5m])) by (job)\n  #                 /\n  #               sum(rate({app=\"foo\", env=\"production\"}[5m])) by (job)\n  #                 \u003e 0.05\n  #             for: 10m\n  #             labels:\n  #               severity: warning\n  #             annotations:\n  #               summary: High error rate\n  #       - name: credentials_leak\n  #         rules:\n  #           - alert: http-credentials-leaked\n  #             annotations:\n  #               message: \"{{ $labels.job }} is leaking http basic auth credentials.\"\n  #             expr: 'sum by (cluster, job, pod) (count_over_time({namespace=\"prod\"} |~ \"http(s?)://(\\\\w+):(\\\\w+)@\" [5m]) \u003e 0)'\n  #             for: 10m\n  #             labels:\n  #               severity: critical\n  #   rules2.txt: |\n  #     groups:\n  #       - name: example\n  #         rules:\n  #         - alert: HighThroughputLogStreams\n  #           expr: sum by(container) (rate({job=~\"loki-dev/.*\"}[1m])) \u003e 1000\n  #           for: 2m\n  # tenant_bar:\n  #   rules1.txt: |\n  #     groups:\n  #       - name: should_fire\n  #         rules:\n  #           - alert: HighPercentageError\n  #             expr: |\n  #               sum(rate({app=\"foo\", env=\"production\"} |= \"error\" [5m])) by (job)\n  #                 /\n  #               sum(rate({app=\"foo\", env=\"production\"}[5m])) by (job)\n  #                 \u003e 0.05\n  #             for: 10m\n  #             labels:\n  #               severity: warning\n  #             annotations:\n  #               summary: High error rate\n  #       - name: credentials_leak\n  #         rules:\n  #           - alert: http-credentials-leaked\n  #             annotations:\n  #               message: \"{{ $labels.job }} is leaking http basic auth credentials.\"\n  #             expr: 'sum by (cluster, job, pod) (count_over_time({namespace=\"prod\"} |~ \"http(s?)://(\\\\w+):(\\\\w+)@\" [5m]) \u003e 0)'\n  #             for: 10m\n  #             labels:\n  #               severity: critical\n  #   rules2.txt: |\n  #     groups:\n  #       - name: example\n  #         rules:\n  #         - alert: HighThroughputLogStreams\n  #           expr: sum by(container) (rate({job=~\"loki-dev/.*\"}[1m])) \u003e 1000\n  #           for: 2m\n\n# -- Configuration for the overrides-exporter\noverridesExporter:\n  # -- The overrides-exporter component is optional and can be disabled if desired.\n  enabled: false\n  # -- Number of replicas for the overrides-exporter\n  replicas: 0\n  # -- DNSConfig for overrides-exporter\n  dnsConfig: {}\n  # -- hostAliases to add\n  hostAliases: []\n  #  - ip: 1.2.3.4\n  #    hostnames:\n  #      - domain.tld\n  # -- Use the host's user namespace in the overrides-exporter\n  hostUsers: nil\n  image:\n    # -- The Docker registry for the overrides-exporter image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the overrides-exporter image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the overrides-exporter image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for overrides-exporter pods\n  priorityClassName: null\n  # -- Labels for overrides-exporter pods\n  podLabels: {}\n  # -- Annotations for overrides-exporter pods\n  podAnnotations: {}\n  # -- Labels for overrides-exporter service\n  serviceLabels: {}\n  # -- Annotations for overrides-exporter service\n  serviceAnnotations: {}\n  # -- Additional CLI args for the overrides-exporter\n  extraArgs: []\n  # -- Environment variables to add to the overrides-exporter pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the overrides-exporter pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the overrides-exporter pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the overrides-exporter pods\n  extraVolumes: []\n  # -- Resource requests and limits for the overrides-exporter\n  resources: {}\n  # -- Containers to add to the overrides-exporter pods\n  extraContainers: []\n  # -- Init containers to add to the overrides-exporter pods\n  initContainers: []\n  # -- Grace period to allow the overrides-exporter to shutdown before it is killed\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for overrides-exporter pods.\n  # @default -- Hard node anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: overrides-exporter\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: null\n  # -- Node selector for overrides-exporter pods\n  nodeSelector: {}\n  # -- Topology Spread Constraints for overrides-exporter pods\n  # The value will be passed through tpl.\n  topologySpreadConstraints: []\n  # -- Tolerations for overrides-exporter pods\n  tolerations: []\n  # -- Set the optional grpc service protocol. Ex: \"grpc\", \"http2\" or \"https\"\n  appProtocol:\n    grpc: \"\"\n\n# You can use a self hosted memcached by setting enabled to false and providing addresses.\nmemcached:\n  # -- Enable the built in memcached server provided by the chart\n  enabled: true\n  image:\n    # -- Memcached Docker image repository\n    repository: memcached\n    # -- Memcached Docker image tag\n    tag: 1.6.39-alpine\n    # -- Memcached Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- The SecurityContext override for memcached pods\n  podSecurityContext:\n    runAsNonRoot: true\n    runAsUser: 11211\n    runAsGroup: 11211\n    fsGroup: 11211\n  # -- The name of the PriorityClass for memcached pods\n  priorityClassName: null\n  # -- The SecurityContext for memcached containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n    allowPrivilegeEscalation: false\n  # -- Readiness probe for memcached pods (probe port defaults to container port)\n  readinessProbe:\n    tcpSocket:\n      port: client\n    initialDelaySeconds: 5\n    periodSeconds: 5\n    timeoutSeconds: 3\n    failureThreshold: 6\n  # -- Liveness probe for memcached pods\n  livenessProbe:\n    tcpSocket:\n      port: client\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 3\n  # -- Startup probe for memcached pods\n  startupProbe: {}\n\nmemcachedExporter:\n  # -- Whether memcached metrics should be exported\n  enabled: true\n  image:\n    repository: prom/memcached-exporter\n    tag: v0.15.3\n    pullPolicy: IfNotPresent\n  resources:\n    requests: {}\n    limits: {}\n  # -- The SecurityContext for memcached exporter containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n    allowPrivilegeEscalation: false\n  # -- Extra args to add to the exporter container.\n  # Example:\n  # extraArgs:\n  #   memcached.tls.enable: true\n  #   memcached.tls.cert-file: /certs/cert.crt\n  #   memcached.tls.key-file: /certs/cert.key\n  #   memcached.tls.ca-file: /certs/ca.crt\n  #   memcached.tls.insecure-skip-verify: false\n  #   memcached.tls.server-name: memcached\n  extraArgs: {}\n  # -- Liveness probe for memcached exporter\n  livenessProbe:\n    httpGet:\n      path: /metrics\n      port: http-metrics\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 3\n  # -- Readiness probe for memcached exporter\n  readinessProbe:\n    httpGet:\n      path: /metrics\n      port: http-metrics\n    initialDelaySeconds: 5\n    periodSeconds: 5\n    timeoutSeconds: 3\n    failureThreshold: 3\n  # -- Startup probe for memcached exporter\n  startupProbe: {}\n\nresultsCache:\n  # -- Specifies whether memcached based results-cache should be enabled\n  enabled: false\n  # -- Comma separated addresses list in DNS Service Discovery format\n  addresses: dnssrvnoa+_memcached-client._tcp.{{ include \"loki.resourceName\" (dict \"ctx\" $ \"component\" \"results-cache\") }}.{{ include \"loki.namespace\" $ }}.svc\n  # -- Specify how long cached results should be stored in the results-cache before being expired\n  defaultValidity: 12h\n  # -- Memcached operation timeout\n  timeout: 500ms\n  # -- Total number of results-cache replicas\n  replicas: 1\n  # -- Port of the results-cache service\n  port: 11211\n  # -- Amount of memory allocated to results-cache for object storage (in MB).\n  allocatedMemory: 1024\n  # -- Maximum item results-cache for memcached (in MB).\n  maxItemMemory: 5\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n  # -- Max memory to use for cache write back\n  writebackSizeLimit: 500MB\n  # -- Max number of objects to use for cache write back\n  writebackBuffer: 500000\n  # -- Number of parallel threads for cache write back\n  writebackParallelism: 1\n  # -- Extra init containers for results-cache pods\n  initContainers: []\n  # -- Annotations for the results-cache pods\n  annotations: {}\n  # -- Node selector for results-cache pods\n  nodeSelector: {}\n  # -- Affinity for results-cache pods\n  affinity: {}\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: []\n  #  maxSkew: 1\n  #  topologyKey: kubernetes.io/hostname\n  #  whenUnsatisfiable: ScheduleAnyway\n  # -- Tolerations for results-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: 1\n  # -- DNSConfig for results-cache\n  dnsConfig: {}\n  # -- The name of the PriorityClass for results-cache pods\n  priorityClassName: null\n  # -- Use the host's user namespace in results-cache pods\n  hostUsers: nil\n  # -- Labels for results-cache pods\n  podLabels: {}\n  # -- Annotations for results-cache pods\n  podAnnotations: {}\n  # -- Management policy for results-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the results-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 60\n  # -- Stateful results-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n  # -- Add extended options for results-cache memcached container. The format is the same as for the memcached -o/--extend flag.\n  # Example:\n  # extraExtendedOptions: 'tls,modern,track_sizes'\n  extraExtendedOptions: \"\"\n  # -- Additional CLI args for results-cache\n  extraArgs: {}\n  # -- Additional containers to be added to the results-cache pod.\n  extraContainers: []\n  # -- Additional volumes to be added to the results-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumes:\n  # - name: extra-volume\n  #   secret:\n  #    secretName: extra-volume-secret\n  extraVolumes: []\n  # -- Additional volume mounts to be added to the results-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumeMounts:\n  # - name: extra-volume\n  #   mountPath: /etc/extra-volume\n  #   readOnly: true\n  extraVolumeMounts: []\n  # -- Resource requests and limits for the results-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n  # -- Persistence settings for the results-cache\n  persistence:\n    # -- Enable creating PVCs for the results-cache\n    enabled: false\n    # -- Size of persistent disk, must be in G or Gi\n    storageSize: 10G\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Volume mount path\n    mountPath: /data\n    # -- PVC additional labels\n    labels: {}\nchunksCache:\n  # -- Append to the name of the resources to make names different for l1 and l2\n  suffix: \"\"\n  # -- Specifies whether memcached based chunks-cache should be enabled\n  enabled: false\n  # -- Comma separated addresses list in DNS Service Discovery format\n  addresses: dnssrvnoa+_memcached-client._tcp.{{ include \"loki.resourceName\" (dict \"ctx\" $ \"component\" \"chunks-cache\" \"suffix\" $.Values.chunksCache.suffix ) }}.{{ include \"loki.namespace\" $ }}.svc\n  # -- Batchsize for sending and receiving chunks from chunks cache\n  batchSize: 4\n  # -- Parallel threads for sending and receiving chunks from chunks cache\n  parallelism: 5\n  # -- Memcached operation timeout\n  timeout: 2000ms\n  # -- Specify how long cached chunks should be stored in the chunks-cache before being expired\n  defaultValidity: 0s\n  # -- Specify how long cached chunks should be stored in the chunks-cache before being expired\n  replicas: 1\n  # -- Port of the chunks-cache service\n  port: 11211\n  # -- Amount of memory allocated to chunks-cache for object storage (in MB).\n  allocatedMemory: 8192\n  # -- Maximum item memory for chunks-cache (in MB).\n  maxItemMemory: 5\n  # -- Maximum number of connections allowed\n  connectionLimit: 16384\n  # -- Max memory to use for cache write back\n  writebackSizeLimit: 500MB\n  # -- Max number of objects to use for cache write back\n  writebackBuffer: 500000\n  # -- Number of parallel threads for cache write back\n  writebackParallelism: 1\n  # -- Extra init containers for chunks-cache pods\n  initContainers: []\n  # -- Annotations for the chunks-cache pods\n  annotations: {}\n  # -- Node selector for chunks-cache pods\n  nodeSelector: {}\n  # -- Affinity for chunks-cache pods\n  affinity: {}\n  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n  topologySpreadConstraints: []\n  #  maxSkew: 1\n  #  topologyKey: kubernetes.io/hostname\n  #  whenUnsatisfiable: ScheduleAnyway\n  # -- Tolerations for chunks-cache pods\n  tolerations: []\n  # -- Pod Disruption Budget maxUnavailable\n  maxUnavailable: 1\n  # -- DNSConfig for chunks-cache\n  dnsConfig: {}\n  # -- The name of the PriorityClass for chunks-cache pods\n  priorityClassName: null\n  # -- Use the host's user namespace in chunks-cache pods\n  hostUsers: nil\n  # -- Labels for chunks-cache pods\n  podLabels: {}\n  # -- Annotations for chunks-cache pods\n  podAnnotations: {}\n  # -- Management policy for chunks-cache pods\n  podManagementPolicy: Parallel\n  # -- Grace period to allow the chunks-cache to shutdown before it is killed\n  terminationGracePeriodSeconds: 60\n  # -- Stateful chunks-cache strategy\n  statefulStrategy:\n    type: RollingUpdate\n  # -- Add extended options for chunks-cache memcached container. The format is the same as for the memcached -o/--extend flag.\n  # Example:\n  # extraExtendedOptions: 'tls,no_hashexpand'\n  extraExtendedOptions: \"\"\n  # -- Additional CLI args for chunks-cache\n  extraArgs: {}\n  # -- Additional containers to be added to the chunks-cache pod.\n  extraContainers: []\n  # -- Additional volumes to be added to the chunks-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumes:\n  # - name: extra-volume\n  #   secret:\n  #    secretName: extra-volume-secret\n  extraVolumes: []\n  # -- Additional volume mounts to be added to the chunks-cache pod (applies to both memcached and exporter containers).\n  # Example:\n  # extraVolumeMounts:\n  # - name: extra-volume\n  #   mountPath: /etc/extra-volume\n  #   readOnly: true\n  extraVolumeMounts: []\n  # -- Resource requests and limits for the chunks-cache\n  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n  resources: null\n  # -- Service annotations and labels\n  service:\n    annotations: {}\n    labels: {}\n  # -- Persistence settings for the chunks-cache\n  persistence:\n    # -- Enable creating PVCs for the chunks-cache\n    enabled: false\n    # -- Size of persistent disk, must be in G or Gi\n    storageSize: 10G\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n    # -- Volume mount path\n    mountPath: /data\n    labels: {}\n  # -- l2 memcache configuration\n  l2:\n    # -- Append to the name of the resources to make names different for l1 and l2\n    suffix: \"l2\"\n    # -- The age of chunks should be transfered from l1 cache to l2\n    # 4 days\n    l2ChunkCacheHandoff: 345600s\n    # -- Specifies whether memcached based chunks-cache-l2 should be enabled\n    enabled: false\n    # -- Comma separated addresses list in DNS Service Discovery format\n    addresses: 'dnssrvnoa+_memcached-client._tcp.{{ include \"loki.resourceName\" (dict \"ctx\" $ \"component\" \"chunks-cache\" \"suffix\" $.Values.chunksCache.l2.suffix ) }}.{{ include \"loki.namespace\" $ }}.svc'\n    # -- Batchsize for sending and receiving chunks from chunks cache\n    batchSize: 4\n    # -- Parallel threads for sending and receiving chunks from chunks cache\n    parallelism: 5\n    # -- Memcached operation timeout\n    timeout: 2000ms\n    # -- Specify how long cached chunks should be stored in the chunks-cache-l2 before being expired\n    defaultValidity: 0s\n    # -- Specify how long cached chunks should be stored in the chunks-cache-l2 before being expired\n    replicas: 1\n    # -- Port of the chunks-cache-l2 service\n    port: 11211\n    # -- Amount of memory allocated to chunks-cache-l2 for object storage (in MB).\n    allocatedMemory: 8192\n    # -- Maximum item memory for chunks-cache-l2 (in MB).\n    maxItemMemory: 5\n    # -- Maximum number of connections allowed\n    connectionLimit: 16384\n    # -- Max memory to use for cache write back\n    writebackSizeLimit: 500MB\n    # -- Max number of objects to use for cache write back\n    writebackBuffer: 500000\n    # -- Number of parallel threads for cache write back\n    writebackParallelism: 1\n    # -- Extra init containers for chunks-cache-l2 pods\n    initContainers: []\n    # -- Annotations for the chunks-cache-l2 pods\n    annotations: {}\n    # -- Node selector for chunks-cach-l2 pods\n    nodeSelector: {}\n    # -- Affinity for chunks-cache-l2 pods\n    affinity: {}\n    # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.\n    # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.\n    topologySpreadConstraints: []\n    #  maxSkew: 1\n    #  topologyKey: kubernetes.io/hostname\n    #  whenUnsatisfiable: ScheduleAnyway\n    # -- Tolerations for chunks-cache-l2 pods\n    tolerations: []\n    # -- Pod Disruption Budget maxUnavailable\n    maxUnavailable: 1\n    # -- DNSConfig for chunks-cache-l2\n    dnsConfig: {}\n    # -- The name of the PriorityClass for chunks-cache-l2 pods\n    priorityClassName: null\n    # -- Use the host's user namespace in chunks-cache-l2 pods\n    hostUsers: nil\n    # -- Labels for chunks-cache-l2 pods\n    podLabels: {}\n    # -- Annotations for chunks-cache-l2 pods\n    podAnnotations: {}\n    # -- Management policy for chunks-cache-l2 pods\n    podManagementPolicy: Parallel\n    # -- Grace period to allow the chunks-cache-l2 to shutdown before it is killed\n    terminationGracePeriodSeconds: 60\n    # -- Stateful chunks-cache strategy\n    statefulStrategy:\n      type: RollingUpdate\n    # -- Add extended options for chunks-cache-l2 memcached container. The format is the same as for the memcached -o/--extend flag.\n    # Example:\n    # extraExtendedOptions: 'tls,no_hashexpand'\n    extraExtendedOptions: \"\"\n    # -- Additional CLI args for chunks-cache-l2\n    extraArgs: {}\n    # -- Additional containers to be added to the chunks-cache-l2 pod.\n    extraContainers: []\n    # -- Additional volumes to be added to the chunks-cache-l2 pod (applies to both memcached and exporter containers).\n    # Example:\n    # extraVolumes:\n    # - name: extra-volume\n    #   secret:\n    #    secretName: extra-volume-secret\n    extraVolumes: []\n    # -- Additional volume mounts to be added to the chunks-cache-l2 pod (applies to both memcached and exporter containers).\n    # Example:\n    # extraVolumeMounts:\n    # - name: extra-volume\n    #   mountPath: /etc/extra-volume\n    #   readOnly: true\n    extraVolumeMounts: []\n    # -- Resource requests and limits for the chunks-cache-l2\n    # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).\n    resources: null\n    # -- Service annotations and labels\n    service:\n      annotations: {}\n      labels: {}\n    # -- Persistence settings for the chunks-cache-l2\n    persistence:\n      # -- Enable creating PVCs for the chunks-cache-l2\n      enabled: false\n      # -- Size of persistent disk, must be in G or Gi\n      storageSize: 10G\n      # -- Storage class to be used.\n      # If defined, storageClassName: \u003cstorageClass\u003e.\n      # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n      # If empty or set to null, no storageClassName spec is\n      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n      storageClass: null\n      # -- Volume mount path\n      mountPath: /data\n      labels: {}\n######################################################################################################################\n#\n# Subchart configurations\n#\n######################################################################################################################\n# -- Setting for the Grafana Rollout Operator https://github.com/grafana/helm-charts/tree/main/charts/rollout-operator\nrollout_operator:\n  enabled: false\n  # -- podSecurityContext is the pod security context for the rollout operator.\n  # When installing on OpenShift, override podSecurityContext settings with\n  #\n  # rollout_operator:\n  #   podSecurityContext:\n  #     fsGroup: null\n  #     runAsGroup: null\n  #     runAsUser: null\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n    seccompProfile:\n      type: RuntimeDefault\n  # Set the container security context\n  securityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop: [ALL]\n    allowPrivilegeEscalation: false\n# -- Configuration for the minio subchart\nminio:\n  enabled: false\n  replicas: 1\n  # Minio requires 2 to 16 drives for erasure code (drivesPerNode * replicas)\n  # https://docs.min.io/docs/minio-erasure-code-quickstart-guide\n  # Since we only have 1 replica, that means 2 drives must be used.\n  drivesPerNode: 2\n  # root user; not used for GEL authentication\n  rootUser: root-user\n  rootPassword: supersecretpassword\n  # The first user in the list below is used for Loki/GEL authentication.\n  # You can add additional users if desired; they will not impact Loki/GEL.\n  # `accessKey` = username, `secretKey` = password\n  users:\n    - accessKey: logs-user\n      secretKey: supersecretpassword\n      policy: readwrite\n  buckets:\n    - name: chunks\n      policy: none\n      purge: false\n    - name: ruler\n      policy: none\n      purge: false\n    - name: admin\n      policy: none\n      purge: false\n  persistence:\n    size: 5Gi\n    annotations: {}\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n  # Allow the address used by Loki to refer to Minio to be overridden\n  address: null\n\n# Create extra manifests via values\n# Can be a list or dictionary, both are passed through `tpl`.  If dict, keys are ignored and only values are used.\n# Objects can also be defined as multiline strings, useful for templating field names\nextraObjects: null\n# - apiVersion: v1\n#   kind: ConfigMap\n#   metadata:\n#     name: loki-alerting-rules\n#   data:\n#     loki-alerting-rules.yaml: |-\n#       groups:\n#         - name: example\n#           rules:\n#           - alert: example\n#             expr: |\n#               sum(count_over_time({app=\"loki\"} |~ \"error\")) \u003e 0\n#             for: 3m\n#             labels:\n#               severity: warning\n#               category: logs\n#             annotations:\n#               message: \"loki has encountered errors\"\n# - |\n#     apiVersion: v1\n#     kind: Secret\n#     type: Opaque\n#     metadata:\n#       name: loki-distributed-basic-auth\n#     data:\n#       {{- range .Values.loki.tenants }}\n#       {{ .name }}: {{ b64enc .password | quote }}\n#       {{- end }}\n\nsidecar:\n  image:\n    # -- The Docker registry and image for the k8s sidecar\n    repository: docker.io/kiwigrid/k8s-sidecar\n    # -- Docker image tag\n    tag: 1.30.10\n    # -- Docker image sha. If empty, no sha will be used\n    sha: \"\"\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- Resource requests and limits for the sidecar\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 100Mi\n  #   requests:\n  #     cpu: 50m\n  #     memory: 50Mi\n  # -- The SecurityContext for the sidecar.\n  securityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Set to true to skip tls verification for kube api calls.\n  skipTlsVerify: false\n  # -- Ensure that rule files aren't conflicting and being overwritten by prefixing their name with the namespace they are defined in.\n  enableUniqueFilenames: false\n  # -- Readiness probe definition. Probe is disabled on the sidecar by default.\n  readinessProbe: {}\n  # -- Liveness probe definition. Probe is disabled on the sidecar by default.\n  livenessProbe: {}\n  # -- Startup probe definition. Probe is disabled on the sidecar by default.\n  startupProbe: {}\n  rules:\n    # -- Whether or not to create a sidecar to ingest rule from specific ConfigMaps and/or Secrets.\n    enabled: true\n    # -- Label that the configmaps/secrets with rules will be marked with.\n    label: loki_rule\n    # -- Label value that the configmaps/secrets with rules will be set to.\n    labelValue: \"\"\n    # -- Folder into which the rules will be placed.\n    folder: /rules\n    # -- The annotation overwriting the folder value.\n    # The annotation value can be either an absolute or a relative path. Relative paths will be relative to FOLDER.\n    # Useful for multi-tenancy setups.\n    folderAnnotation: null\n    # -- Comma separated list of namespaces. If specified, the sidecar will search for config-maps/secrets inside these namespaces.\n    # Otherwise the namespace in which the sidecar is running will be used.\n    # It's also possible to specify 'ALL' to search in all namespaces.\n    searchNamespace: null\n    # -- Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH request, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.\n    watchMethod: WATCH\n    # -- Search in configmap, secret, or both.\n    resource: both\n    # -- Absolute path to the shell script to execute after a configmap or secret has been reloaded.\n    script: null\n    # -- WatchServerTimeout: request to the server, asking it to cleanly close the connection after that.\n    # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S.\n    watchServerTimeout: 60\n    #\n    # -- WatchClientTimeout: is a client-side timeout, configuring your local socket.\n    # If you have a network outage dropping all packets with no RST/FIN,\n    # this is how long your client waits before realizing \u0026 dropping the connection.\n    # Defaults to 66sec.\n    watchClientTimeout: 60\n    # -- Log level of the sidecar container.\n    logLevel: INFO\n\n# -- Monitoring section determines which monitoring features to enable\nmonitoring:\n  # Dashboards for monitoring Loki\n  dashboards:\n    # -- If enabled, create configmap with dashboards for monitoring Loki\n    enabled: false\n    # -- Alternative namespace to create dashboards ConfigMap in\n    namespace: null\n    # -- Additional annotations for the dashboards ConfigMap\n    annotations: {}\n    # -- Labels for the dashboards ConfigMap\n    labels:\n      grafana_dashboard: \"1\"\n  # -- Recording rules for monitoring Loki, required for some dashboards\n  rules:\n    # -- If enabled, create PrometheusRule resource with Loki recording rules\n    enabled: false\n    # -- Include alerting rules\n    alerting: true\n    # -- Specify which individual alerts should be disabled\n    # -- Instead of turning off each alert one by one, set the .monitoring.rules.alerting value to false instead.\n    # -- If you disable all the alerts and keep .monitoring.rules.alerting set to true, the chart will fail to render.\n    disabled: {}\n    #  LokiRequestErrors: true\n    #  LokiRequestPanics: true\n    # -- Alternative namespace to create PrometheusRule resources in\n    namespace: null\n    # -- Additional annotations for the rules PrometheusRule resource\n    annotations: {}\n    # -- Additional labels for the rules PrometheusRule resource\n    labels: {}\n    # -- Additional annotations for PrometheusRule alerts\n    additionalRuleAnnotations: {}\n    # e.g.:\n    # additionalRuleAnnotations:\n    #   runbook_url: \"https://runbooks.example.com/oncall/loki\"\n    #   summary: \"What this alert means and how to respond\"\n    # -- Additional labels for PrometheusRule alerts\n    additionalRuleLabels: {}\n    # -- Additional groups to add to the rules file\n    additionalGroups: []\n    # - name: additional-loki-rules\n    #   rules:\n    #     - record: job:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)\n    #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)\n    #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n    #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)\n  # -- ServiceMonitor configuration\n  serviceMonitor:\n    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n    enabled: false\n    # -- Namespace selector for ServiceMonitor resources\n    namespaceSelector: {}\n    # -- ServiceMonitor annotations\n    annotations: {}\n    # -- Additional ServiceMonitor labels\n    labels: {}\n    # -- ServiceMonitor scrape interval\n    # Default is 15s because included recording rules use a 1m rate, and scrape interval needs to be at\n    # least 1/4 rate interval.\n    interval: 15s\n    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n    scrapeTimeout: null\n    # -- ServiceMonitor relabel configs to apply to samples before scraping\n    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n    relabelings: []\n    # -- ServiceMonitor metric relabel configs to apply to samples before ingestion\n    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n    metricRelabelings: []\n    # -- ServiceMonitor will use http by default, but you can pick https as well\n    scheme: http\n    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n    tlsConfig: null\n    # -- DEPRECATED If defined, will create a MetricsInstance for the Grafana Agent Operator.\n    metricsInstance:\n      # -- If enabled, MetricsInstance resources for Grafana Agent Operator are created\n      enabled: true\n      # -- MetricsInstance annotations\n      annotations: {}\n      # -- Additional MetricsInstance labels\n      labels: {}\n      # -- If defined a MetricsInstance will be created to remote write metrics.\n      remoteWrite: null\n  # -- DEPRECATED Self monitoring determines whether Loki should scrape its own logs.\n  # This feature relies on Grafana Agent Operator, which is deprecated.\n  # It will create custom resources for GrafanaAgent, LogsInstance, and PodLogs to configure\n  # scrape configs to scrape its own logs with the labels expected by the included dashboards.\n  selfMonitoring:\n    enabled: false\n    # -- Tenant to use for self monitoring\n    tenant:\n      # -- Name of the tenant\n      name: \"self-monitoring\"\n      # -- Password of the gateway for Basic auth\n      password: null\n      # -- Namespace to create additional tenant token secret in. Useful if your Grafana instance\n      # is in a separate namespace. Token will still be created in the canary namespace.\n      # @default -- The same namespace as the loki chart is installed in.\n      secretNamespace: '{{ include \"loki.namespace\" . }}'\n    # -- DEPRECATED Grafana Agent configuration\n    grafanaAgent:\n      # -- DEPRECATED Controls whether to install the Grafana Agent Operator and its CRDs.\n      # Note that helm will not install CRDs if this flag is enabled during an upgrade.\n      # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds\n      installOperator: false\n      # -- Grafana Agent annotations\n      annotations: {}\n      # -- Additional Grafana Agent labels\n      labels: {}\n      # -- Enable the config read api on port 8080 of the agent\n      enableConfigReadAPI: false\n      # -- The name of the PriorityClass for GrafanaAgent pods\n      priorityClassName: null\n      # -- Resource requests and limits for the grafanaAgent pods\n      resources: {}\n      #   limits:\n      #     memory: 200Mi\n      #   requests:\n      #     cpu: 50m\n      #     memory: 100Mi\n      # -- Tolerations for GrafanaAgent pods\n      tolerations: []\n    # PodLogs configuration\n    podLogs:\n      # -- PodLogs version\n      apiVersion: monitoring.grafana.com/v1alpha1\n      # -- PodLogs annotations\n      annotations: {}\n      # -- Additional PodLogs labels\n      labels: {}\n      # -- PodLogs relabel configs to apply to samples before scraping\n      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n      relabelings: []\n      # -- Additional pipeline stages to process logs after scraping\n      # https://grafana.com/docs/agent/latest/operator/api/#pipelinestagespec-a-namemonitoringgrafanacomv1alpha1pipelinestagespeca\n      additionalPipelineStages: []\n    # LogsInstance configuration\n    logsInstance:\n      # -- LogsInstance annotations\n      annotations: {}\n      # -- Additional LogsInstance labels\n      labels: {}\n      # -- Additional clients for remote write\n      clients: null\n\n# -- DEPRECATED Configuration for the table-manager. The table-manager is only necessary when using a deprecated\n# index type such as Cassandra, Bigtable, or DynamoDB, it has not been necessary since loki introduced self-\n# contained index types like 'boltdb-shipper' and 'tsdb'. This will be removed in a future helm chart.\ntableManager:\n  # -- Specifies whether the table-manager should be enabled\n  enabled: false\n  image:\n    # -- The Docker registry for the table-manager image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the table-manager image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the table-manager image. Overrides `loki.image.tag`\n    tag: null\n  # -- Command to execute instead of defined in Docker image\n  command: null\n  # -- The name of the PriorityClass for table-manager pods\n  priorityClassName: null\n  # -- Labels for table-manager pods\n  podLabels: {}\n  # -- Annotations for table-manager deployment\n  annotations: {}\n  # -- Annotations for table-manager pods\n  podAnnotations: {}\n  service:\n    # -- Annotations for table-manager Service\n    annotations: {}\n    # -- Additional labels for table-manager Service\n    labels: {}\n  # -- Additional CLI args for the table-manager\n  extraArgs: []\n  # -- Environment variables to add to the table-manager pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the table-manager pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the table-manager pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the table-manager pods\n  extraVolumes: []\n  # -- Resource requests and limits for the table-manager\n  resources: {}\n  # -- Containers to add to the table-manager pods\n  extraContainers: []\n  # -- Grace period to allow the table-manager to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Use the host's user namespace in table-manager pods\n  hostUsers: nil\n  # -- Affinity for table-manager pods.\n  # @default -- Hard node and anti-affinity\n  # The value will be passed through tpl.\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: table-manager\n              app.kubernetes.io/name: '{{ include \"loki.name\" . }}'\n              app.kubernetes.io/instance: '{{ .Release.Name }}'\n          topologyKey: kubernetes.io/hostname\n  # -- DNS config table-manager pods\n  dnsConfig: {}\n  # -- Node selector for table-manager pods\n  nodeSelector: {}\n  # -- Tolerations for table-manager pods\n  tolerations: []\n  # -- Enable deletes by retention\n  retention_deletes_enabled: false\n  # -- Set retention period\n  retention_period: 0\n"
            ],
            "verify": false,
            "version": "6.43.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "mimir",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "mimir-distributed",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "mimir",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.10.0",
                "chart": "mimir-distributed",
                "first_deployed": 1761174893,
                "last_deployed": 1761177135,
                "name": "mimir",
                "namespace": "monitoring",
                "notes": "\nWelcome to Grafana Mimir!\nRemote write endpoints for Prometheus or Grafana Agent:\nIngress is not enabled, see the nginx.ingress values.\nFrom inside the cluster:\n  http://mimir-nginx.monitoring.svc:80/api/v1/push\n\nRead address, Grafana data source (Prometheus) URL:\nIngress is not enabled, see the nginx.ingress values.\nFrom inside the cluster:\n  http://mimir-nginx.monitoring.svc:80/prometheus\n\n**IMPORTANT**: Always consult CHANGELOG.md file at https://github.com/grafana/mimir/blob/main/operations/helm/charts/mimir-distributed/CHANGELOG.md and the deprecation list there to learn about breaking changes that require action during upgrade.\n",
                "revision": 2,
                "values": "{\"alertmanager\":{\"persistentVolume\":{\"enabled\":false},\"replicas\":1,\"zoneAwareReplication\":{\"enabled\":false}},\"chunks-cache\":{\"enabled\":false},\"compactor\":{\"persistentVolume\":{\"enabled\":false},\"replicas\":1},\"distributor\":{\"replicas\":1},\"index-cache\":{\"enabled\":false},\"ingester\":{\"persistentVolume\":{\"enabled\":false},\"replicas\":1,\"zoneAwareReplication\":{\"enabled\":false}},\"metadata-cache\":{\"enabled\":false},\"mimir\":{\"structuredConfig\":{\"alertmanager\":{\"data_dir\":\"/data/alertmanager-data\",\"sharding_ring\":{\"heartbeat_timeout\":\"10m\",\"kvstore\":{\"store\":\"memberlist\"},\"replication_factor\":1}},\"alertmanager_storage\":{\"backend\":\"filesystem\",\"filesystem\":{\"dir\":\"/data/alertmanager-storage\"}},\"blocks_storage\":{\"backend\":\"filesystem\",\"filesystem\":{\"dir\":\"/data/blocks\"},\"tsdb\":{\"dir\":\"/data/ingester\"}},\"compactor\":{\"data_dir\":\"/data/compactor\"},\"distributor\":{\"ring\":{\"kvstore\":{\"store\":\"memberlist\"}}},\"ingester\":{\"ring\":{\"heartbeat_timeout\":\"10m\",\"kvstore\":{\"store\":\"memberlist\"},\"replication_factor\":1}},\"limits\":{\"ingestion_burst_size\":1000000,\"ingestion_rate\":500000,\"max_global_series_per_user\":0},\"multitenancy_enabled\":false,\"ruler\":{\"ring\":{\"heartbeat_timeout\":\"10m\",\"kvstore\":{\"store\":\"memberlist\"}},\"rule_path\":\"/data/ruler-data\"},\"ruler_storage\":{\"backend\":\"filesystem\",\"filesystem\":{\"dir\":\"/data/ruler-storage\"}},\"store_gateway\":{\"sharding_ring\":{\"heartbeat_timeout\":\"10m\",\"kvstore\":{\"store\":\"memberlist\"},\"replication_factor\":1}}}},\"minio\":{\"enabled\":false},\"nginx\":{\"replicas\":1},\"overrides_exporter\":{\"replicas\":1},\"querier\":{\"replicas\":1},\"query_frontend\":{\"replicas\":1},\"results-cache\":{\"enabled\":false},\"rollout_operator\":{\"enabled\":false},\"ruler\":{\"replicas\":1},\"store_gateway\":{\"persistentVolume\":{\"enabled\":false},\"replicas\":1,\"zoneAwareReplication\":{\"enabled\":false}}}",
                "version": "5.1.0"
              }
            ],
            "name": "mimir",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 900,
            "upgrade_install": null,
            "values": [
              "\"alertmanager\":\n  \"persistentVolume\":\n    \"enabled\": false\n  \"replicas\": 1\n  \"zoneAwareReplication\":\n    \"enabled\": false\n\"chunks-cache\":\n  \"enabled\": false\n\"compactor\":\n  \"persistentVolume\":\n    \"enabled\": false\n  \"replicas\": 1\n\"distributor\":\n  \"replicas\": 1\n\"index-cache\":\n  \"enabled\": false\n\"ingester\":\n  \"persistentVolume\":\n    \"enabled\": false\n  \"replicas\": 1\n  \"zoneAwareReplication\":\n    \"enabled\": false\n\"metadata-cache\":\n  \"enabled\": false\n\"mimir\":\n  \"structuredConfig\":\n    \"alertmanager\":\n      \"data_dir\": \"/data/alertmanager-data\"\n      \"sharding_ring\":\n        \"heartbeat_timeout\": \"10m\"\n        \"kvstore\":\n          \"store\": \"memberlist\"\n        \"replication_factor\": 1\n    \"alertmanager_storage\":\n      \"backend\": \"filesystem\"\n      \"filesystem\":\n        \"dir\": \"/data/alertmanager-storage\"\n    \"blocks_storage\":\n      \"backend\": \"filesystem\"\n      \"filesystem\":\n        \"dir\": \"/data/blocks\"\n      \"tsdb\":\n        \"dir\": \"/data/ingester\"\n    \"compactor\":\n      \"data_dir\": \"/data/compactor\"\n    \"distributor\":\n      \"ring\":\n        \"kvstore\":\n          \"store\": \"memberlist\"\n    \"ingester\":\n      \"ring\":\n        \"heartbeat_timeout\": \"10m\"\n        \"kvstore\":\n          \"store\": \"memberlist\"\n        \"replication_factor\": 1\n    \"limits\":\n      \"ingestion_burst_size\": 1000000\n      \"ingestion_rate\": 500000\n      \"max_global_series_per_user\": 0\n    \"multitenancy_enabled\": false\n    \"ruler\":\n      \"ring\":\n        \"heartbeat_timeout\": \"10m\"\n        \"kvstore\":\n          \"store\": \"memberlist\"\n      \"rule_path\": \"/data/ruler-data\"\n    \"ruler_storage\":\n      \"backend\": \"filesystem\"\n      \"filesystem\":\n        \"dir\": \"/data/ruler-storage\"\n    \"store_gateway\":\n      \"sharding_ring\":\n        \"heartbeat_timeout\": \"10m\"\n        \"kvstore\":\n          \"store\": \"memberlist\"\n        \"replication_factor\": 1\n\"minio\":\n  \"enabled\": false\n\"nginx\":\n  \"replicas\": 1\n\"overrides_exporter\":\n  \"replicas\": 1\n\"querier\":\n  \"replicas\": 1\n\"query_frontend\":\n  \"replicas\": 1\n\"results-cache\":\n  \"enabled\": false\n\"rollout_operator\":\n  \"enabled\": false\n\"ruler\":\n  \"replicas\": 1\n\"store_gateway\":\n  \"persistentVolume\":\n    \"enabled\": false\n  \"replicas\": 1\n  \"zoneAwareReplication\":\n    \"enabled\": false\n"
            ],
            "verify": false,
            "version": "5.1.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "helm_release.loki"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "otel-collector",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "otel-collector",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "otel-collector",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.46.0",
                "chart": "otel-collector",
                "first_deployed": 1761173287,
                "last_deployed": 1761176415,
                "name": "otel-collector",
                "namespace": "monitoring",
                "notes": "Send telemetry to this pod via the clusterIP:\n\n1. http://otel-collector.monitoring:4317\n2. GRPC to otel-collector.monitoring:4318\n\nIt will then be processed and sent to exporters based on the configFile or configFileSecret.\n",
                "revision": 2,
                "values": "{\"configFile\":\"receivers:\\n  otlp:\\n    protocols:\\n      grpc:\\n        endpoint: \\\"0.0.0.0:4317\\\"\\n      http:\\n        endpoint: \\\"0.0.0.0:4318\\\"\\n\\nprocessors:\\n  batch:\\n  memory_limiter:\\n    limit_mib: 1500\\n    spike_limit_mib: 512\\n    check_interval: 5s\\n\\nextensions:\\n  health_check:\\n    endpoint: 0.0.0.0:13133\\n  zpages: {}\\n  memory_ballast:\\n    size_mib: 683\\n\\nexporters:\\n  logging:\\n    logLevel: debug\\n  \\n  otlphttp/loki:\\n    endpoint: http://loki-gateway.monitoring.svc.cluster.local:80/otlp\\n    tls:\\n      insecure: true\\n  \\n  otlp/tempo:\\n    endpoint: tempo.monitoring.svc.cluster.local:4317\\n    tls:\\n      insecure: true\\n  \\n  prometheusremotewrite/mimir:\\n    endpoint: http://mimir-nginx.monitoring.svc.cluster.local/api/v1/push\\n    tls:\\n      insecure: true\\n\\nservice:\\n  extensions: [zpages, memory_ballast, health_check]\\n  pipelines:\\n    traces:\\n      receivers: [otlp]\\n      processors: [memory_limiter, batch]\\n      exporters: [otlp/tempo, logging]\\n    \\n    logs:\\n      receivers: [otlp]\\n      processors: [memory_limiter, batch]\\n      exporters: [otlphttp/loki, logging]\\n    \\n    metrics:\\n      receivers: [otlp]\\n      processors: [memory_limiter, batch]\\n      exporters: [prometheusremotewrite/mimir, logging]\",\"configFileSecret\":null,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"otel/opentelemetry-collector-contrib\",\"tag\":null},\"ingress\":{\"main\":{\"enabled\":false}},\"metrics\":{\"enabled\":false,\"prometheusRule\":{\"enabled\":false,\"labels\":{},\"rules\":[]},\"serviceMonitor\":{\"interval\":\"3m\",\"labels\":{},\"scrapeTimeout\":\"1m\"}},\"probes\":{\"liveness\":{\"custom\":true,\"enabled\":true,\"spec\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/\",\"port\":13133},\"initialDelaySeconds\":30,\"periodSeconds\":10,\"timeoutSeconds\":1}},\"readiness\":{\"enabled\":false},\"startup\":{\"enabled\":false}},\"service\":{\"main\":{\"enabled\":false},\"otlpports\":{\"enabled\":true,\"ports\":{\"metrics\":{\"enabled\":true,\"port\":8888,\"protocol\":\"TCP\",\"targetPort\":8888},\"otlpgrpc\":{\"enabled\":true,\"port\":4317,\"protocol\":\"TCP\",\"targetPort\":4317},\"otlphttp\":{\"enabled\":true,\"port\":4318,\"protocol\":\"TCP\",\"targetPort\":4318}},\"type\":\"ClusterIP\"}},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"}}",
                "version": "1.2.2"
              }
            ],
            "name": "otel-collector",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "#\n# IMPORTANT NOTE\n#\n# This chart inherits from our common library chart. You can check the default values/options here:\n# https://github.com/k8s-at-home/library-charts/tree/main/charts/stable/common/values.yaml\n#\n\nimage:\n  # -- image repository\n  repository: otel/opentelemetry-collector-contrib\n  # -- image tag\n  tag:\n  # -- image pull policy\n  pullPolicy: IfNotPresent\n\n# -- Configures service settings for the chart.\n# @default -- The defaults expose the services needed to receive http and otlp traces\nservice:\n  main:\n    enabled: false\n  otlpports:\n    enabled: true\n    type: ClusterIP\n    ports:\n      # Default endpoint for OpenTelemetry gRPC receiver.\n      otlpgrpc:\n        enabled: true\n        protocol: TCP\n        port: 4317\n        targetPort: 4317\n      # Default endpoint for OpenTelemetry HTTP receiver.\n      otlphttp:\n        enabled: true\n        protocol: TCP\n        port: 4318\n        targetPort: 4318\n      # Default endpoint for querying metrics.\n      metrics:\n        enabled: true\n        protocol: TCP\n        port: 8888\n        targetPort: 8888\n\n# -- probes is configured to use an otel extension to get health information from the pod\n# @default -- expects config to include `extensions:health_check:endpoint: 0.0.0.0:13133`\nprobes:\n  liveness:\n    enabled: true\n    custom: true\n    spec:\n      initialDelaySeconds: 30\n      periodSeconds: 10\n      timeoutSeconds: 1\n      failureThreshold: 3\n      httpGet:\n        path: /\n        port: 13133\n  readiness:\n    enabled: false\n  startup:\n    enabled: false\n\ningress:\n  # -- Enable and configure ingress settings for the chart under this key.\n  # This OTEL Collector is built to trust items within the same cluster so\n  # exposing externally will allow unauthenticated traces to be processed.\n  # @default -- disabled\n  main:\n    enabled: false\n\n# -- Configure the open telemetry secret using an existing secret or create\n# a configuration file using the `configFile` below\n# The secret needs a single key inside it called `otelConfigFile`\nconfigFileSecret:\n\n# -- Create a new secret with the following multi-line spec which gets mounted\n# to /conf/otel-collector-config.yaml. For more information, see the\n# [otel docs](https://opentelemetry.io/docs/collector/configuration/)\nconfigFile: |-\n  receivers:\n    otlp:\n      protocols:\n        grpc:\n          endpoint: \"0.0.0.0:4317\"\n        http:\n          endpoint: \"0.0.0.0:4318\"\n\n  processors:\n    batch:\n    memory_limiter:\n      limit_mib: 1500\n      spike_limit_mib: 512\n      check_interval: 5s\n\n  extensions:\n    health_check:\n      endpoint: 0.0.0.0:13133\n    zpages: {}\n    memory_ballast:\n      size_mib: 683\n\n  exporters:\n    logging:\n      logLevel: debug\n    \n    otlphttp/loki:\n      endpoint: http://loki-gateway.monitoring.svc.cluster.local:80/otlp\n      tls:\n        insecure: true\n    \n    otlp/tempo:\n      endpoint: tempo.monitoring.svc.cluster.local:4317\n      tls:\n        insecure: true\n    \n    prometheusremotewrite/mimir:\n      endpoint: http://mimir-nginx.monitoring.svc.cluster.local/api/v1/push\n      tls:\n        insecure: true\n\n  service:\n    extensions: [zpages, memory_ballast, health_check]\n    pipelines:\n      traces:\n        receivers: [otlp]\n        processors: [memory_limiter, batch]\n        exporters: [otlp/tempo, logging]\n      \n      logs:\n        receivers: [otlp]\n        processors: [memory_limiter, batch]\n        exporters: [otlphttp/loki, logging]\n      \n      metrics:\n        receivers: [otlp]\n        processors: [memory_limiter, batch]\n        exporters: [prometheusremotewrite/mimir, logging]\n\n\nmetrics:\n  # -- Configure Prometheus serviceMonitor for the built-in exporter.\n  # @default -- enabled: false is set so it can scrape itself but\n  # circular dependencies are never good enable this for a secondary scraper\n  enabled: false\n  serviceMonitor:\n    interval: 3m\n    scrapeTimeout: 1m\n    labels: {}\n  # -- Enable and configure Prometheus Rules for the chart under this key.\n  # @default -- See values.yaml\n  prometheusRule:\n    enabled: false\n    labels: {}\n    # -- Configure additionial rules for the chart under this key.\n    # @default -- See prometheusrules.yaml\n    rules: []\n      # - alert: OtelCollectorDown\n      #   annotations:\n      #     description: Otel Collector service is down.\n      #     summary: Otel Collector is down.\n      #   expr: |\n      #     up == 0\n      #   for: 5m\n      #   labels:\n      #     severity: critical\n\nserviceAccount:\n  # -- Specifies whether a service account should be created\n  create: true\n\n  # -- Annotations to add to the service account\n  annotations: {}\n\n  # -- The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n"
            ],
            "verify": false,
            "version": "1.2.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "prometheus",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v3.7.1",
                "chart": "prometheus",
                "first_deployed": 1761175125,
                "last_deployed": 1761175455,
                "name": "prometheus",
                "namespace": "monitoring",
                "notes": "1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9100 to use your application\"\n  kubectl port-forward --namespace monitoring $POD_NAME 9100\nkube-state-metrics is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.\nThe exposed metrics can be found here:\nhttps://github.com/kubernetes/kube-state-metrics/blob/master/docs/README.md#exposed-metrics\n\nThe metrics are exported on the HTTP endpoint /metrics on the listening port.\nIn your case, prometheus-kube-state-metrics.monitoring.svc.cluster.local:8080/metrics\n\nThey are served either as plaintext or protobuf depending on the Accept header.\nThey are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint.\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9093 to use your application\"\n  kubectl --namespace monitoring port-forward $POD_NAME 9093:80\n\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:\nprometheus-server.monitoring.svc.cluster.local\n\n\nGet the Prometheus server URL by running these commands in the same shell:\n  export NODE_PORT=$(kubectl get --namespace monitoring -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server)\n  export NODE_IP=$(kubectl get nodes --namespace monitoring -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n  echo http://$NODE_IP:$NODE_PORT\n\n\nThe Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:\nprometheus-alertmanager.monitoring.svc.cluster.local\n\n\nGet the Alertmanager URL by running these commands in the same shell:\n  export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl --namespace monitoring port-forward $POD_NAME 9093\n#################################################################################\n######   WARNING: Pod Security Policy has been disabled by default since    #####\n######            it deprecated after k8s 1.25+. use                        #####\n######            (index .Values \"prometheus-node-exporter\" \"rbac\"          #####\n###### .          \"pspEnabled\") with (index .Values                         #####\n######            \"prometheus-node-exporter\" \"rbac\" \"pspAnnotations\")       #####\n######            in case you still need it.                                #####\n#################################################################################\n\n\nThe Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:\nprometheus-prometheus-pushgateway.monitoring.svc.cluster.local\n\n\nGet the PushGateway URL by running these commands in the same shell:\n  export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app=prometheus-pushgateway,component=pushgateway\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl --namespace monitoring port-forward $POD_NAME 9091\n\nFor more information on running Prometheus, visit:\nhttps://prometheus.io/\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=prometheus-pushgateway,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl port-forward $POD_NAME 9091\n  echo \"Visit http://127.0.0.1:9091 to use your application\"\n",
                "revision": 2,
                "values": "{\"alertRelabelConfigs\":{},\"alertmanager\":{\"enabled\":true,\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"emptyDir\":{},\"enabled\":true,\"labels\":{},\"size\":\"2Gi\"},\"podSecurityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534}},\"commonMetaLabels\":{},\"configmapReload\":{\"env\":[],\"prometheus\":{\"containerPort\":8080,\"containerPortName\":\"metrics\",\"containerSecurityContext\":{},\"enabled\":true,\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraVolumeDirs\":[],\"extraVolumeMounts\":[],\"image\":{\"digest\":\"\",\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus-operator/prometheus-config-reloader\",\"tag\":\"v0.86.1\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/healthz\",\"port\":\"metrics\",\"scheme\":\"HTTP\"},\"initialDelaySeconds\":2,\"periodSeconds\":10},\"name\":\"configmap-reload\",\"readinessProbe\":{\"httpGet\":{\"path\":\"/healthz\",\"port\":\"metrics\",\"scheme\":\"HTTP\"},\"periodSeconds\":10},\"resources\":{},\"startupProbe\":{\"enabled\":false,\"httpGet\":{\"path\":\"/healthz\",\"port\":\"metrics\",\"scheme\":\"HTTP\"},\"periodSeconds\":10}},\"reloadUrl\":\"\"},\"extraManifests\":[],\"extraScrapeConfigs\":\"\",\"forceNamespace\":\"\",\"imagePullSecrets\":[],\"kube-state-metrics\":{\"enabled\":true},\"networkPolicy\":{\"enabled\":false},\"prometheus-node-exporter\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false},\"enabled\":true,\"rbac\":{\"pspEnabled\":false}},\"prometheus-pushgateway\":{\"enabled\":true,\"serviceAnnotations\":{\"prometheus.io/probe\":\"pushgateway\"}},\"rbac\":{\"create\":true},\"ruleFiles\":{},\"scrapeConfigFiles\":[],\"server\":{\"affinity\":{},\"alertmanagers\":[],\"baseURL\":\"\",\"clusterRoleNameOverride\":\"\",\"command\":[],\"configFromSecret\":\"\",\"configMapAnnotations\":{},\"configMapOverrideName\":\"\",\"configPath\":\"/etc/config/prometheus.yml\",\"containerSecurityContext\":{},\"daemonSet\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"defaultFlagsOverride\":[],\"deploymentAnnotations\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"medium\":\"\",\"sizeLimit\":\"\"},\"enableServiceLinks\":true,\"env\":[],\"exemplars\":{},\"extraArgs\":{},\"extraConfigmapLabels\":{},\"extraConfigmapMounts\":[],\"extraFlags\":[\"web.enable-lifecycle\"],\"extraHostPathMounts\":[],\"extraInitContainers\":[],\"extraSecretMounts\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"global\":{\"evaluation_interval\":\"1m\",\"scrape_interval\":\"1m\",\"scrape_timeout\":\"10s\"},\"hostAliases\":[],\"hostNetwork\":false,\"image\":{\"digest\":\"\",\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus/prometheus\",\"tag\":\"\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraLabels\":{},\"extraPaths\":[],\"hosts\":[],\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"livenessProbeFailureThreshold\":3,\"livenessProbeInitialDelay\":30,\"livenessProbePeriodSeconds\":15,\"livenessProbeSuccessThreshold\":1,\"livenessProbeTimeout\":10,\"name\":\"server\",\"nodeSelector\":{},\"otlp\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"labels\":{},\"mountPath\":\"/data\",\"size\":\"8Gi\",\"statefulSetNameOverride\":\"\",\"subPath\":\"\"},\"podAnnotations\":{},\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podDisruptionBudget\":{\"enabled\":false},\"podLabels\":{},\"portName\":\"\",\"prefixURL\":\"\",\"priorityClassName\":\"\",\"probeHeaders\":[],\"probeScheme\":\"HTTP\",\"readinessProbeFailureThreshold\":3,\"readinessProbeInitialDelay\":30,\"readinessProbePeriodSeconds\":5,\"readinessProbeSuccessThreshold\":1,\"readinessProbeTimeout\":4,\"releaseNamespace\":false,\"remoteRead\":[],\"remoteWrite\":[{\"url\":\"http://mimir-nginx.monitoring.svc.cluster.local/api/v1/push\"}],\"replicaCount\":1,\"resources\":{},\"retention\":\"15d\",\"retentionSize\":\"\",\"revisionHistoryLimit\":10,\"route\":{\"main\":{\"additionalRules\":[],\"annotations\":{},\"apiVersion\":\"\",\"enabled\":false,\"filters\":[],\"hostnames\":[],\"httpsRedirect\":false,\"kind\":\"\",\"labels\":{},\"matches\":[{\"path\":{\"type\":\"PathPrefix\",\"value\":\"/\"}}],\"parentRefs\":[]}},\"runtimeClassName\":\"\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"enabled\":true,\"externalIPs\":[],\"externalTrafficPolicy\":\"\",\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"servicePort\":80,\"sessionAffinity\":\"None\",\"statefulsetReplica\":{\"enabled\":false,\"replica\":0},\"type\":\"NodePort\"},\"sidecarContainers\":{},\"sidecarTemplateValues\":{},\"startupProbe\":{\"enabled\":false,\"failureThreshold\":30,\"periodSeconds\":5,\"timeoutSeconds\":10},\"statefulSet\":{\"annotations\":{},\"enabled\":false,\"headless\":{\"annotations\":{},\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"servicePort\":80},\"labels\":{},\"podManagementPolicy\":\"OrderedReady\",\"pvcDeleteOnStsDelete\":false,\"pvcDeleteOnStsScale\":false},\"storagePath\":\"\",\"strategy\":{\"type\":\"Recreate\"},\"tcpSocketProbeEnabled\":false,\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[],\"tsdb\":{},\"verticalAutoscaler\":{\"enabled\":false}},\"serverFiles\":{\"alerting_rules.yml\":{},\"alerts\":{},\"prometheus.yml\":{\"rule_files\":[\"/etc/config/recording_rules.yml\",\"/etc/config/alerting_rules.yml\",\"/etc/config/rules\",\"/etc/config/alerts\"],\"scrape_configs\":[{\"job_name\":\"prometheus\",\"static_configs\":[{\"targets\":[\"localhost:9090\"]}]},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-apiservers\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"default;kubernetes;https\",\"source_labels\":[\"__meta_kubernetes_namespace\",\"__meta_kubernetes_service_name\",\"__meta_kubernetes_endpoint_port_name\"]}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes-cadvisor\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics/cadvisor\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"}},{\"honor_labels\":true,\"job_name\":\"kubernetes-service-endpoints\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(.+?)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-service-endpoints-slow\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(.+?)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"},{\"honor_labels\":true,\"job_name\":\"prometheus-pushgateway\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"pushgateway\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-services\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"metrics_path\":\"/probe\",\"params\":{\"module\":[\"http_2xx\"]},\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]},{\"source_labels\":[\"__address__\"],\"target_label\":\"__param_target\"},{\"replacement\":\"blackbox\",\"target_label\":\"__address__\"},{\"source_labels\":[\"__param_target\"],\"target_label\":\"instance\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-pods\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\",\"replacement\":\"[$2]:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);((([0-9]+?)(\\\\.|$)){4})\",\"replacement\":\"$2:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-pods-slow\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\",\"replacement\":\"[$2]:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);((([0-9]+?)(\\\\.|$)){4})\",\"replacement\":\"$2:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"}]},\"recording_rules.yml\":{},\"rules\":{}},\"serviceAccounts\":{\"server\":{\"annotations\":{},\"create\":true,\"name\":\"\"}}}",
                "version": "27.41.1"
              }
            ],
            "name": "prometheus",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# yaml-language-server: $schema=values.schema.json\n# Default values for prometheus.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nrbac:\n  create: true\n\nimagePullSecrets: []\n# - name: \"image-pull-secret\"\n\n## Define serviceAccount names for components. Defaults to component's fully qualified name.\n##\nserviceAccounts:\n  server:\n    create: true\n    name: \"\"\n    annotations: {}\n\n    ## Opt out of automounting Kubernetes API credentials.\n    ## It will be overriden by server.automountServiceAccountToken value, if set.\n    # automountServiceAccountToken: false\n\n## Additional labels to attach to all resources\ncommonMetaLabels: {}\n\n## Monitors ConfigMap changes and POSTs to a URL\n## Ref: https://github.com/prometheus-operator/prometheus-operator/tree/main/cmd/prometheus-config-reloader\n##\nconfigmapReload:\n  ## URL for configmap-reload to use for reloads\n  ##\n  reloadUrl: \"\"\n\n  ## env sets environment variables to pass to the container. Can be set as name/value pairs,\n  ## read from secrets or configmaps.\n  env: []\n    # - name: SOMEVAR\n    #   value: somevalue\n    # - name: PASSWORD\n    #   valueFrom:\n    #     secretKeyRef:\n    #       name: mysecret\n    #       key: password\n    #       optional: false\n\n  prometheus:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: quay.io/prometheus-operator/prometheus-config-reloader\n      tag: v0.86.1\n      # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).\n      digest: \"\"\n      pullPolicy: IfNotPresent\n\n    ## config-reloader's container port and port name for probes and metrics\n    containerPort: 8080\n    containerPortName: metrics\n\n    ## Additional configmap-reload container arguments\n    ## Set to null for argumentless flags\n    ##\n    extraArgs: {}\n\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n    ## Additional configmap-reload volume mounts\n    ##\n    extraVolumeMounts: []\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n    ## Security context to be added to configmap-reload container\n    containerSecurityContext: {}\n\n    ## Settings for Prometheus reloader's readiness, liveness and startup probes\n    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n    ##\n\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: metrics\n        scheme: HTTP\n      periodSeconds: 10\n      initialDelaySeconds: 2\n\n    readinessProbe:\n      httpGet:\n        path: /healthz\n        port: metrics\n        scheme: HTTP\n      periodSeconds: 10\n\n    startupProbe:\n      enabled: false\n      httpGet:\n        path: /healthz\n        port: metrics\n        scheme: HTTP\n      periodSeconds: 10\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n\nserver:\n  ## Prometheus server container name\n  ##\n  name: server\n\n  ## Opt out of automounting Kubernetes API credentials.\n  ## If set it will override serviceAccounts.server.automountServiceAccountToken value for ServiceAccount.\n  # automountServiceAccountToken: false\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY\n  ##\n  ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.\n  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.\n  ##\n  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.\n  ##\n  # useExistingClusterRoleName: nameofclusterrole\n\n  ## If set it will override prometheus.server.fullname value for ClusterRole and ClusterRoleBinding\n  ##\n  clusterRoleNameOverride: \"\"\n\n  # Enable only the release namespace for monitoring. By default all namespaces are monitored.\n  # If releaseNamespace and namespaces are both set a merged list will be monitored.\n  releaseNamespace: false\n\n  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.\n  # namespaces:\n  #   - yournamespace\n\n  # sidecarContainers - add more containers to prometheus server\n  # Key/Value where Key is the sidecar `- name: \u003cKey\u003e`\n  # Example:\n  #   sidecarContainers:\n  #      webserver:\n  #        image: nginx\n  # OR for adding OAuth authentication to Prometheus\n  #   sidecarContainers:\n  #     oauth-proxy:\n  #       image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n  #       args:\n  #       - --upstream=http://127.0.0.1:9090\n  #       - --http-address=0.0.0.0:8081\n  #       - ...\n  #       ports:\n  #       - containerPort: 8081\n  #         name: oauth-proxy\n  #         protocol: TCP\n  #       resources: {}\n  sidecarContainers: {}\n\n  # sidecarTemplateValues - context to be used in template for sidecarContainers\n  # Example:\n  #   sidecarTemplateValues: *your-custom-globals\n  #   sidecarContainers:\n  #     webserver: |-\n  #       {{ include \"webserver-container-template\" . }}\n  # Template for `webserver-container-template` might looks like this:\n  #   image: \"{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}\"\n  #   ...\n  #\n  sidecarTemplateValues: {}\n\n  ## Prometheus server container image\n  ##\n  image:\n    repository: quay.io/prometheus/prometheus\n    # if not set appVersion field from Chart.yaml is used\n    tag: \"\"\n    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).\n    digest: \"\"\n    pullPolicy: IfNotPresent\n\n  ## Prometheus server command\n  ##\n  command: []\n\n  ## prometheus server priorityClassName\n  ##\n  priorityClassName: \"\"\n  ## prometheus server runtimeClassName\n  ##\n  runtimeClassName: \"\"\n\n  ## EnableServiceLinks indicates whether information about services should be injected\n  ## into pod's environment variables, matching the syntax of Docker links.\n  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.\n  ##\n  enableServiceLinks: true\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access prometheus\n  ## Maybe same with Ingress host name\n  baseURL: \"\"\n\n  ## Additional server container environment variables\n  ##\n  ## You specify this manually like you would a raw deployment manifest.\n  ## This means you can bind in environment variables from secrets.\n  ##\n  ## e.g. static environment variable:\n  ##  - name: DEMO_GREETING\n  ##    value: \"Hello from the environment\"\n  ##\n  ## e.g. secret environment variable:\n  ## - name: USERNAME\n  ##   valueFrom:\n  ##     secretKeyRef:\n  ##       name: mysecret\n  ##       key: username\n  env: []\n\n  # List of flags to override default parameters, e.g:\n  # - --enable-feature=agent\n  # - --storage.agent.retention.max-time=30m\n  # - --config.file=/etc/config/prometheus.yml\n  defaultFlagsOverride: []\n\n  extraFlags:\n    - web.enable-lifecycle\n    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as\n    ## deleting time series. This is disabled by default.\n    # - web.enable-admin-api\n    ##\n    ## storage.tsdb.no-lockfile flag controls BD locking\n    # - storage.tsdb.no-lockfile\n    ##\n    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)\n    # - storage.tsdb.wal-compression\n\n  ## Path to a configuration file on prometheus server container FS\n  configPath: /etc/config/prometheus.yml\n\n  ### The data directory used by prometheus to set --storage.tsdb.path\n  ### When empty server.persistentVolume.mountPath is used instead\n  storagePath: \"\"\n\n  global:\n    ## How frequently to scrape targets by default\n    ##\n    scrape_interval: 1m\n    ## How long until a scrape request times out\n    ##\n    scrape_timeout: 10s\n    ## How frequently to evaluate rules\n    ##\n    evaluation_interval: 1m\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\n  ##\n  remoteWrite: [{\n    url: \"http://mimir-nginx.monitoring.svc.cluster.local/api/v1/push\"\n    }]\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read\n  ##\n  remoteRead: []\n\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb\n  ##\n  tsdb: {}\n    # out_of_order_time_window: 0s\n\n  ## https://prometheus.io/docs/guides/opentelemetry\n  ##\n  otlp: {}\n    # promote_resource_attributes: []\n    # keep_identifying_resource_attributes: false\n    # translation_strategy: NoUTF8EscapingWithSuffixes\n\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#exemplars\n  ## Must be enabled via --enable-feature=exemplar-storage\n  ##\n  exemplars: {}\n    # max_exemplars: 100000\n\n  ## Custom HTTP headers for Liveness/Readiness/Startup Probe\n  ##\n  ## Useful for providing HTTP Basic Auth to healthchecks\n  probeHeaders: []\n    # - name: \"Authorization\"\n    #   value: \"Bearer ABCDEabcde12345\"\n\n  ## Additional Prometheus server container arguments\n  ## Set to null for argumentless flags\n  ##\n  extraArgs: {}\n    # web.enable-remote-write-receiver: null\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional Prometheus server Volume mounts\n  ##\n  extraVolumeMounts: []\n\n  ## Additional Prometheus server Volumes\n  ##\n  extraVolumes: []\n\n  ## Additional Prometheus server hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: certs-dir\n    #   mountPath: /etc/kubernetes/certs\n    #   subPath: \"\"\n    #   hostPath: /etc/kubernetes/certs\n    #   readOnly: true\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   subPath: \"\"\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Additional Prometheus server Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: prom-secret-files\n    #   readOnly: true\n\n  ## Prometheus server configuration from a secret\n  ## Do not set both `configMapOverrideName` and `configFromSecret` simultaneously.\n  ## Use either `configMapOverrideName` or `configFromSecret`.\n  ## If `configFromSecret` is defined, a ConfigMap resource will NOT be generated.\n  configFromSecret: \"\"\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/server-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ## Extra labels for Prometheus server ConfigMap (ConfigMap that holds serverFiles)\n  extraConfigmapLabels: {}\n\n  ## Override the prometheus.server.fullname for all objects related to the Prometheus server\n  fullnameOverride: \"\"\n\n  ingress:\n    ## If true, Prometheus server Ingress will be created\n    ##\n    enabled: false\n\n    ingressClassName: \"\"\n\n    ## Prometheus server Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## Prometheus server Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## Redirect ingress to an additional defined port on the service\n    # servicePort: 8081\n\n    ## Prometheus server Ingress hostnames with optional path (passed through tpl)\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - prometheus.domain.com\n    #   - domain.com/prometheus\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services. (passed through tpl)\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## Prometheus server Ingress TLS configuration (hosts passed through tpl)\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-server-tls\n    #     hosts:\n    #       - prometheus.domain.com\n\n  ## route (map) allows configuration of HTTPRoute resources\n  ## Requires Gateway API resources and suitable controller installed within the cluster\n  ## Ref. https://gateway-api.sigs.k8s.io/guides/http-routing/\n  route:\n    main:\n      ## Enable this route\n      enabled: false\n\n      ## ApiVersion set by default to \"gateway.networking.k8s.io/v1\"\n      apiVersion: \"\"\n      ## kind set by default to HTTPRoute\n      kind: \"\"\n\n      ## Annotations to attach to the HTTPRoute resource\n      annotations: {}\n      ## Labels to attach to the HTTPRoute resource\n      labels: {}\n\n      ## ParentRefs refers to resources this HTTPRoute is to be attached to (Gateways)\n      parentRefs: []\n        # - name: contour\n        #   sectionName: http\n\n      ## Hostnames (templated) defines a set of hostnames that should match against the HTTP Host\n      ## header to select a HTTPRoute used to process the request\n      hostnames: []\n        # - my.example.com\n\n      ## additionalRules (templated) allows adding custom rules to the route\n      additionalRules: []\n\n      ## Filters define the filters that are applied to requests that match\n      ## this rule\n      filters: []\n\n      ## Matches define conditions used for matching the rule against incoming\n      ## HTTP requests\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n\n      ## httpsRedirect adds a filter for redirecting to https (HTTP 301 Moved Permanently).\n      ## To redirect HTTP traffic to HTTPS, you need to have a Gateway with both HTTP and HTTPS listeners.\n      ## Matches and filters do not take effect if enabled.\n      ## Ref. https://gateway-api.sigs.k8s.io/guides/http-redirect-rewrite/\n      httpsRedirect: false\n\n  ## Server Deployment Strategy type\n  strategy:\n    type: Recreate\n\n  ## hostAliases allows adding entries to /etc/hosts inside the containers\n  hostAliases: []\n  #   - ip: \"127.0.0.1\"\n  #     hostnames:\n  #       - \"example.com\"\n\n  ## Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for Prometheus server pod assignment\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## Pod anti-affinity can prevent the scheduler from placing Prometheus server replicas on the same node.\n  ## The value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n  ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n  ## The default value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured (unless set in `server.affinity`).\n  ##\n  podAntiAffinity: \"\"\n\n  ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n  ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n  ##\n  podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n  ## Pod topology spread constraints\n  ## ref. https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  topologySpreadConstraints: []\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    # maxUnavailable: 1\n    # minAvailable: 1\n    ## unhealthyPodEvictionPolicy is available since 1.27.0 (beta)\n    ## https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy\n    # unhealthyPodEvictionPolicy: IfHealthyBudget\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, Prometheus server will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## If set it will override the name of the created persistent volume claim\n    ## generated by the stateful set.\n    ##\n    statefulSetNameOverride: \"\"\n\n    ## Prometheus server data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## Prometheus server data Persistent Volume labels\n    ##\n    labels: {}\n\n    ## Prometheus server data Persistent Volume annotations\n    ##\n    annotations: {}\n\n    ## Prometheus server data Persistent Volume existing claim name\n    ## Requires server.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## Prometheus server data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## Prometheus server data Persistent Volume size\n    ##\n    size: 8Gi\n\n    ## Prometheus server data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## Subdirectory of Prometheus server data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n    ## Persistent Volume Name\n    ## Useful if Persistent Volumes have been provisioned in advance and you want to use a specific one\n    ##\n    # volumeName: \"\"\n\n  emptyDir:\n    ## Prometheus server emptyDir volume\n    ## Configure size limit and medium\n    ##\n    medium: \"\"\n    sizeLimit: \"\"\n\n  ## Annotations to be added to Prometheus server pods\n  ##\n  podAnnotations: {}\n    # iam.amazonaws.com/role: prometheus\n\n  ## Labels to be added to Prometheus server pods\n  ##\n  podLabels: {}\n\n  ## Prometheus AlertManager configuration\n  ##\n  alertmanagers: []\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Number of old history to retain to allow rollback\n  ## Default Kubernetes value is set to 10\n  ##\n  revisionHistoryLimit: 10\n\n  ## Annotations to be added to ConfigMap\n  ##\n  configMapAnnotations: {}\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n      servicePort: 80\n      ## Enable gRPC port on service to allow auto discovery with thanos-querier\n      gRPC:\n        enabled: false\n        servicePort: 10901\n        # nodePort: 10901\n\n    ## Statefulset's persistent volume claim retention policy\n    ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether\n    ## statefulset's PVCs are deleted (true) or retained (false) on scaling down\n    ## and deleting statefulset, respectively. Requires 1.27.0+.\n    ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    ##\n    pvcDeleteOnStsDelete: false\n    pvcDeleteOnStsScale: false\n\n  daemonSet:\n    ## If true, use a daemonset instead of a deployment for pod management.\n    ## This allows to run prometheus agent on every node in the cluster.\n    ##\n    enabled: false\n    annotations: {}\n    labels: {}\n\n  ## Prometheus server readiness and liveness probe initial delay and timeout\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  ##\n  tcpSocketProbeEnabled: false\n  probeScheme: HTTP\n  readinessProbeInitialDelay: 30\n  readinessProbePeriodSeconds: 5\n  readinessProbeTimeout: 4\n  readinessProbeFailureThreshold: 3\n  readinessProbeSuccessThreshold: 1\n  livenessProbeInitialDelay: 30\n  livenessProbePeriodSeconds: 15\n  livenessProbeTimeout: 10\n  livenessProbeFailureThreshold: 3\n  livenessProbeSuccessThreshold: 1\n  startupProbe:\n    enabled: false\n    periodSeconds: 5\n    failureThreshold: 30\n    timeoutSeconds: 10\n\n  ## Prometheus server resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 500m\n    #   memory: 512Mi\n    # requests:\n    #   cpu: 500m\n    #   memory: 512Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  # When hostNetwork is enabled, this will set to ClusterFirstWithHostNet automatically\n  dnsPolicy: ClusterFirst\n\n  # Use hostPort\n  # hostPort: 9090\n\n  # Use portName\n  portName: \"\"\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-server'\n\n  # Custom DNS configuration to be added to prometheus server pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to server pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  ## Security context to be added to server container\n  ##\n  containerSecurityContext: {}\n\n  service:\n    ## If false, no Service will be created for the Prometheus server\n    ##\n    enabled: true\n\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    sessionAffinity: None\n    type: NodePort\n    ## externalTrafficPolicy is applicable to service with externally-facing addresses (NodePorts, ExternalIPs, and LoadBalancer IPs)\n    externalTrafficPolicy: \"\"\n\n    ## Enable gRPC port on service to allow auto discovery with thanos-querier\n    gRPC:\n      enabled: false\n      servicePort: 10901\n      # nodePort: 10901\n\n    ## If using a statefulSet (statefulSet.enabled=true), configure the\n    ## service to connect to a specific replica to have a consistent view\n    ## of the data.\n    statefulsetReplica:\n      enabled: false\n      replica: 0\n\n    ## Additional port to define in the Service\n    additionalPorts: []\n    # additionalPorts:\n    # - name: authenticated\n    #   port: 8081\n    #   targetPort: 8081\n\n  ## Prometheus server pod termination grace period\n  ##\n  terminationGracePeriodSeconds: 300\n\n  ## Prometheus data retention period (default if not specified is 15 days)\n  ##\n  retention: \"15d\"\n\n  ## Prometheus' data retention size. Supported units: B, KB, MB, GB, TB, PB, EB.\n  ##\n  retentionSize: \"\"\n\n## Prometheus server ConfigMap entries for rule files (allow prometheus labels interpolation)\nruleFiles: {}\n\n## Prometheus server ConfigMap entries for scrape_config_files\n## (allows scrape configs defined in additional files)\n##\nscrapeConfigFiles: []\n\n## Prometheus server ConfigMap entries\n##\nserverFiles:\n  ## Alerts configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n  alerting_rules.yml: {}\n  # groups:\n  #   - name: Instances\n  #     rules:\n  #       - alert: InstanceDown\n  #         expr: up == 0\n  #         for: 5m\n  #         labels:\n  #           severity: page\n  #         annotations:\n  #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'\n  #           summary: 'Instance {{ $labels.instance }} down'\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml\n  alerts: {}\n\n  ## Records configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/\n  recording_rules.yml: {}\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml\n  rules: {}\n\n  prometheus.yml:\n    rule_files:\n      - /etc/config/recording_rules.yml\n      - /etc/config/alerting_rules.yml\n    ## Below two files are DEPRECATED will be removed from this default values file\n      - /etc/config/rules\n      - /etc/config/alerts\n\n    scrape_configs:\n      - job_name: prometheus\n        static_configs:\n          - targets:\n            - localhost:9090\n\n      # A scrape configuration for running Prometheus on a Kubernetes cluster.\n      # This uses separate scrape configs for cluster components (i.e. API server, node)\n      # and services to allow each to use different authentication configs.\n      #\n      # Kubernetes labels will be added as Prometheus labels on metrics via the\n      # `labelmap` relabeling action.\n\n      # Scrape config for API servers.\n      #\n      # Kubernetes exposes API servers as endpoints to the default/kubernetes\n      # service so this uses `endpoints` role and uses relabelling to only keep\n      # the endpoints associated with the default/kubernetes service using the\n      # default named port `https`. This works for single API server deployments as\n      # well as HA API server deployments.\n      - job_name: 'kubernetes-apiservers'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          # insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        # Keep only the default/kubernetes service endpoints for the https port. This\n        # will add targets for each API server which Kubernetes adds an endpoint to\n        # the default/kubernetes service.\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n            action: keep\n            regex: default;kubernetes;https\n\n      - job_name: 'kubernetes-nodes'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          # insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics\n\n\n      - job_name: 'kubernetes-nodes-cadvisor'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          # insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        # This configuration will work only on kubelet 1.7.3+\n        # As the scrape endpoints for cAdvisor have changed\n        # if you are using older version you need to change the replacement to\n        # replacement: /api/v1/nodes/$1:4194/proxy/metrics\n        # more info here https://github.com/coreos/prometheus-operator/issues/633\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n\n        # Metric relabel configs to apply to samples before ingestion.\n        # [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)\n        # metric_relabel_configs:\n        # - action: labeldrop\n        #   regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n\n      # Scrape config for service endpoints.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape services that have a value of\n      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: (.+?)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Scrape config for slow service endpoints; same as above, but with a larger\n      # timeout and a larger interval\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints-slow'\n        honor_labels: true\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: (.+?)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      - job_name: 'prometheus-pushgateway'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: pushgateway\n\n      # Example scrape config for probing services via the Blackbox Exporter.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/probe`: Only probe services that have a value of `true`\n      - job_name: 'kubernetes-services'\n        honor_labels: true\n\n        metrics_path: /probe\n        params:\n          module: [http_2xx]\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: true\n          - source_labels: [__address__]\n            target_label: __param_target\n          - target_label: __address__\n            replacement: blackbox\n          - source_labels: [__param_target]\n            target_label: instance\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            target_label: service\n\n      # Example scrape config for pods\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,\n      # except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\n            replacement: '[$2]:$1'\n            target_label: __address__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);((([0-9]+?)(\\.|$)){4})\n            replacement: $2:$1\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Example Scrape config for pods which should be scraped slower. An useful example\n      # would be stackriver-exporter which queries an API on every scrape of the pod\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods-slow'\n        honor_labels: true\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\n            replacement: '[$2]:$1'\n            target_label: __address__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);((([0-9]+?)(\\.|$)){4})\n            replacement: $2:$1\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n# adds additional scrape configs to prometheus.yml\n# must be a string so you have to add a | after extraScrapeConfigs:\n# example adds prometheus-blackbox-exporter scrape config\nextraScrapeConfigs: \"\"\n  # - job_name: 'prometheus-blackbox-exporter'\n  #   metrics_path: /probe\n  #   params:\n  #     module: [http_2xx]\n  #   static_configs:\n  #     - targets:\n  #       - https://example.com\n  #   relabel_configs:\n  #     - source_labels: [__address__]\n  #       target_label: __param_target\n  #     - source_labels: [__param_target]\n  #       target_label: instance\n  #     - target_label: __address__\n  #       replacement: prometheus-blackbox-exporter:9115\n\n# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager\n# useful in H/A prometheus with different external labels but the same alerts\nalertRelabelConfigs: {}\n  # alert_relabel_configs:\n  # - source_labels: [dc]\n  #   regex: (.+)\\d+\n  #   target_label: dc\n\nnetworkPolicy:\n  ## Enable creation of NetworkPolicy resources.\n  ##\n  enabled: false\n\n# Force namespace of namespaced resources\nforceNamespace: \"\"\n\n# Extra manifests to deploy as an array\nextraManifests: []\n  # - |\n  #   apiVersion: v1\n  #   kind: ConfigMap\n  #   metadata:\n  #   labels:\n  #     name: prometheus-extra\n  #   data:\n  #     extra-data: \"value\"\n\n# Configuration of subcharts defined in Chart.yaml\n\n## alertmanager sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/alertmanager\n##\nalertmanager:\n  ## If false, alertmanager will not be installed\n  ##\n  enabled: true\n\n  persistence:\n    ## If true, storage will create or use Persistence Volume\n    ## If false, storage will use emptyDir\n    ##\n    enabled: true\n\n    ## Custom annotations for the PVC created by the alertmanager StatefulSet.\n    ## Useful for configuring storage provider options such as disk type, KMS encryption keys, or custom volume name prefixes.\n    annotations: {}\n\n    ## Custom labels for the PVC created by the alertmanager StatefulSet.\n    ## Useful for selecting, grouping, and organizing so that they can be queried or targeted in deployments, policies, etc.\n    labels: {}\n\n    ## Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ## set, choosing the default provisioner.\n    ##\n    # storageClass: \"-\"\n    accessModes:\n      - ReadWriteOnce\n    size: 2Gi\n\n    ## Configure emptyDir volume\n    ##\n    emptyDir: {}\n\n  podSecurityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n## kube-state-metrics sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics\n##\nkube-state-metrics:\n  ## If false, kube-state-metrics sub-chart will not be installed\n  ##\n  enabled: true\n\n## prometheus-node-exporter sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter\n##\nprometheus-node-exporter:\n  ## If false, node-exporter will not be installed\n  ##\n  enabled: true\n\n  rbac:\n    pspEnabled: false\n\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n\n## prometheus-pushgateway sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-pushgateway\n##\nprometheus-pushgateway:\n  ## If false, pushgateway will not be installed\n  ##\n  enabled: true\n\n  # Optional service annotations\n  serviceAnnotations:\n    prometheus.io/probe: pushgateway\n"
            ],
            "verify": false,
            "version": "27.41.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "promtail",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "promtail",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "promtail",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "3.5.1",
                "chart": "promtail",
                "first_deployed": 1761159724,
                "last_deployed": 1761173287,
                "name": "promtail",
                "namespace": "monitoring",
                "notes": "***********************************************************************\n Welcome to Grafana Promtail\n Chart version: 6.17.0\n Promtail version: 3.5.1\n***********************************************************************\n\nVerify the application is working by running these commands:\n* kubectl --namespace monitoring port-forward daemonset/promtail 3101\n* curl http://127.0.0.1:3101/metrics\n",
                "revision": 3,
                "values": "{\"affinity\":{},\"annotations\":{},\"automountServiceAccountToken\":true,\"config\":{\"clients\":[{\"url\":\"http://loki-gateway:80/loki/api/v1/push\"}],\"enableTracing\":false,\"enabled\":true,\"file\":\"server:\\n  log_level: {{ .Values.config.logLevel }}\\n  log_format: {{ .Values.config.logFormat }}\\n  http_listen_port: {{ .Values.config.serverPort }}\\n  {{- with .Values.httpPathPrefix }}\\n  http_path_prefix: {{ . }}\\n  {{- end }}\\n  {{- tpl .Values.config.snippets.extraServerConfigs . | nindent 2 }}\\n\\nclients:\\n  {{- tpl (toYaml .Values.config.clients) . | nindent 2 }}\\n\\npositions:\\n  {{- tpl (toYaml .Values.config.positions) . | nindent 2 }}\\n\\nscrape_configs:\\n  {{- tpl .Values.config.snippets.scrapeConfigs . | nindent 2 }}\\n  {{- tpl .Values.config.snippets.extraScrapeConfigs . | nindent 2 }}\\n\\nlimits_config:\\n  {{- tpl .Values.config.snippets.extraLimitsConfig . | nindent 2 }}\\n\\ntracing:\\n  enabled: {{ .Values.config.enableTracing }}\\n\",\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"positions\":{\"filename\":\"/run/promtail/positions.yaml\"},\"serverPort\":3101,\"snippets\":{\"addScrapeJobLabel\":false,\"common\":[{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node_name\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"replacement\":\"$1\",\"separator\":\"/\",\"source_labels\":[\"namespace\",\"app\"],\"target_label\":\"job\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_container_name\"],\"target_label\":\"container\"},{\"action\":\"replace\",\"replacement\":\"/var/log/pods/*$1/*.log\",\"separator\":\"/\",\"source_labels\":[\"__meta_kubernetes_pod_uid\",\"__meta_kubernetes_pod_container_name\"],\"target_label\":\"__path__\"},{\"action\":\"replace\",\"regex\":\"true/(.*)\",\"replacement\":\"/var/log/pods/*$1/*.log\",\"separator\":\"/\",\"source_labels\":[\"__meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\",\"__meta_kubernetes_pod_annotation_kubernetes_io_config_hash\",\"__meta_kubernetes_pod_container_name\"],\"target_label\":\"__path__\"}],\"extraLimitsConfig\":\"\",\"extraRelabelConfigs\":[],\"extraScrapeConfigs\":\"\",\"extraServerConfigs\":\"\",\"pipelineStages\":[{\"cri\":{}}],\"scrapeConfigs\":\"# See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\\n- job_name: kubernetes-pods\\n  pipeline_stages:\\n    {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}\\n  kubernetes_sd_configs:\\n    - role: pod\\n  relabel_configs:\\n    - source_labels:\\n        - __meta_kubernetes_pod_controller_name\\n      regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\\n      action: replace\\n      target_label: __tmp_controller_name\\n    - source_labels:\\n        - __meta_kubernetes_pod_label_app_kubernetes_io_name\\n        - __meta_kubernetes_pod_label_app\\n        - __tmp_controller_name\\n        - __meta_kubernetes_pod_name\\n      regex: ^;*([^;]+)(;.*)?$\\n      action: replace\\n      target_label: app\\n    - source_labels:\\n        - __meta_kubernetes_pod_label_app_kubernetes_io_instance\\n        - __meta_kubernetes_pod_label_instance\\n      regex: ^;*([^;]+)(;.*)?$\\n      action: replace\\n      target_label: instance\\n    - source_labels:\\n        - __meta_kubernetes_pod_label_app_kubernetes_io_component\\n        - __meta_kubernetes_pod_label_component\\n      regex: ^;*([^;]+)(;.*)?$\\n      action: replace\\n      target_label: component\\n    {{- if .Values.config.snippets.addScrapeJobLabel }}\\n    - replacement: kubernetes-pods\\n      target_label: scrape_job\\n    {{- end }}\\n    {{- toYaml .Values.config.snippets.common | nindent 4 }}\\n    {{- with .Values.config.snippets.extraRelabelConfigs }}\\n    {{- toYaml . | nindent 4 }}\\n    {{- end }}\\n\"}},\"configmap\":{\"enabled\":false},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"daemonset\":{\"autoscaling\":{\"controlledResources\":[],\"enabled\":false,\"maxAllowed\":{},\"minAllowed\":{}},\"enabled\":true},\"defaultVolumeMounts\":[{\"mountPath\":\"/run/promtail\",\"name\":\"run\"},{\"mountPath\":\"/var/lib/docker/containers\",\"name\":\"containers\",\"readOnly\":true},{\"mountPath\":\"/var/log/pods\",\"name\":\"pods\",\"readOnly\":true}],\"defaultVolumes\":[{\"hostPath\":{\"path\":\"/run/promtail\"},\"name\":\"run\"},{\"hostPath\":{\"path\":\"/var/lib/docker/containers\"},\"name\":\"containers\"},{\"hostPath\":{\"path\":\"/var/log/pods\"},\"name\":\"pods\"}],\"deployment\":{\"autoscaling\":{\"enabled\":false,\"maxReplicas\":10,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":80,\"targetMemoryUtilizationPercentage\":null},\"enabled\":false,\"replicaCount\":1,\"strategy\":{\"type\":\"RollingUpdate\"}},\"enableServiceLinks\":true,\"extraArgs\":[],\"extraContainers\":{},\"extraEnv\":[],\"extraEnvFrom\":[],\"extraObjects\":[],\"extraPorts\":{},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":null,\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\"},\"hostAliases\":[],\"hostNetwork\":null,\"httpPathPrefix\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/promtail\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainer\":[],\"livenessProbe\":{},\"nameOverride\":null,\"namespace\":null,\"networkPolicy\":{\"enabled\":false,\"k8sApi\":{\"cidrs\":[],\"port\":8443},\"metrics\":{\"cidrs\":[],\"namespaceSelector\":{},\"podSelector\":{}}},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"runAsGroup\":0,\"runAsUser\":0},\"podSecurityPolicy\":{\"allowPrivilegeEscalation\":true,\"fsGroup\":{\"rule\":\"RunAsAny\"},\"hostIPC\":false,\"hostNetwork\":false,\"hostPID\":false,\"privileged\":true,\"readOnlyRootFilesystem\":true,\"requiredDropCapabilities\":[\"ALL\"],\"runAsUser\":{\"rule\":\"RunAsAny\"},\"seLinux\":{\"rule\":\"RunAsAny\"},\"supplementalGroups\":{\"rule\":\"RunAsAny\"},\"volumes\":[\"secret\",\"hostPath\",\"downwardAPI\"]},\"priorityClassName\":null,\"rbac\":{\"create\":true,\"pspEnabled\":false},\"readinessProbe\":{\"failureThreshold\":5,\"httpGet\":{\"path\":\"{{ printf `%s/ready` .Values.httpPathPrefix }}\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"secret\":{\"annotations\":{},\"labels\":{}},\"service\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"imagePullSecrets\":[],\"name\":null},\"serviceMonitor\":{\"annotations\":{},\"enabled\":false,\"interval\":null,\"labels\":{},\"metricRelabelings\":[],\"namespace\":null,\"namespaceSelector\":{},\"prometheusRule\":{\"additionalLabels\":{},\"enabled\":false,\"rules\":[]},\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":null,\"targetLabels\":[],\"tlsConfig\":null},\"sidecar\":{\"configReloader\":{\"config\":{\"serverPort\":9533},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"enabled\":false,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"ghcr.io\",\"repository\":\"jimmidyson/configmap-reload\",\"tag\":\"v0.12.0\"},\"livenessProbe\":{},\"readinessProbe\":{},\"resources\":{},\"serviceMonitor\":{\"enabled\":true}}},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Exists\"},{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/control-plane\",\"operator\":\"Exists\"}],\"updateStrategy\":{}}",
                "version": "6.17.0"
              }
            ],
            "name": "promtail",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# -- Overrides the chart's name\nnameOverride: null\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n\nglobal:\n  # -- Allow parent charts to override registry hostname\n  imageRegistry: \"\"\n  # -- Allow parent charts to override registry credentials\n  imagePullSecrets: []\n\ndaemonset:\n  # -- Deploys Promtail as a DaemonSet\n  enabled: true\n  autoscaling:\n    # -- Creates a VerticalPodAutoscaler for the daemonset\n    enabled: false\n\n    # Recommender responsible for generating recommendation for the object.\n    # List should be empty (then the default recommender will generate the recommendation)\n    # or contain exactly one recommender.\n    # recommenders:\n    # - name: custom-recommender-performance\n\n    # -- List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory\n    controlledResources: []\n\n    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.\n    # controlledValues: RequestsAndLimits\n\n    # -- Defines the max allowed resources for the pod\n    maxAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n    # -- Defines the min allowed resources for the pod\n    minAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n\n    # updatePolicy:\n      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction\n      # minReplicas: 1\n      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates\n      # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\".\n      # updateMode: Auto\n\ndeployment:\n  # -- Deploys Promtail as a Deployment\n  enabled: false\n  replicaCount: 1\n  autoscaling:\n    # -- Creates a HorizontalPodAutoscaler for the deployment\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 10\n    targetCPUUtilizationPercentage: 80\n    targetMemoryUtilizationPercentage:\n    # behavior: {}\n\n  # -- Set deployment object update strategy\n  strategy:\n    type: RollingUpdate\n\nservice:\n  enabled: false\n  # -- Labels for the service\n  labels: {}\n  # -- Annotations for the service\n  annotations: {}\n\nsecret:\n  # -- Labels for the Secret\n  labels: {}\n  # -- Annotations for the Secret\n  annotations: {}\n\nconfigmap:\n  # -- If enabled, promtail config will be created as a ConfigMap instead of a secret\n  enabled: false\n\ninitContainer: []\n  # # -- Specifies whether the init container for setting inotify max user instances is to be enabled\n  # - name: init\n  #   # -- Docker registry, image and tag for the init container image\n  #   image: docker.io/busybox:1.33\n  #   # -- Docker image pull policy for the init container image\n  #   imagePullPolicy: IfNotPresent\n  #   # -- The inotify max user instances to configure\n  #   command:\n  #     - sh\n  #     - -c\n  #     - sysctl -w fs.inotify.max_user_instances=128\n  #   securityContext:\n  #     privileged: true\n\nimage:\n  # -- The Docker registry\n  registry: docker.io\n  # -- Docker image repository\n  repository: grafana/promtail\n  # -- Overrides the image tag whose default is the chart's appVersion\n  tag: \"\"\n  # -- Docker image pull policy\n  pullPolicy: IfNotPresent\n\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n\n# -- hostAliases to add\nhostAliases: []\n#  - ip: 1.2.3.4\n#    hostnames:\n#      - domain.tld\n\n# -- Controls whether the pod has the `hostNetwork` flag set.\nhostNetwork: null\n\n# -- Annotations for the DaemonSet\nannotations: {}\n\n# -- Number of old history to retain to allow rollback (If not set, default Kubernetes value is set to 10)\n# revisionHistoryLimit: 1\n\n# -- The update strategy for the DaemonSet\nupdateStrategy: {}\n\n# -- Pod labels\npodLabels: {}\n\n# -- Pod annotations\npodAnnotations: {}\n#  prometheus.io/scrape: \"true\"\n#  prometheus.io/port: \"http-metrics\"\n\n# -- The name of the PriorityClass\npriorityClassName: null\n\n# -- Liveness probe\nlivenessProbe: {}\n\n# -- Readiness probe\n# @default -- See `values.yaml`\nreadinessProbe:\n  failureThreshold: 5\n  httpGet:\n    path: \"{{ printf `%s/ready` .Values.httpPathPrefix }}\"\n    port: http-metrics\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 1\n\n# -- Resource requests and limits\nresources: {}\n#  limits:\n#    cpu: 200m\n#    memory: 128Mi\n#  requests:\n#    cpu: 100m\n#    memory: 128Mi\n\n# -- The security context for pods\npodSecurityContext:\n  runAsUser: 0\n  runAsGroup: 0\n\n# -- The security context for containers\ncontainerSecurityContext:\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n      - ALL\n  allowPrivilegeEscalation: false\n\nrbac:\n  # -- Specifies whether RBAC resources are to be created\n  create: true\n  # -- Specifies whether a PodSecurityPolicy is to be created\n  pspEnabled: false\n\n# -- The name of the Namespace to deploy\n# If not set, `.Release.Namespace` is used\nnamespace: null\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and `create` is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n  # -- Automatically mount a ServiceAccount's API credentials\n  automountServiceAccountToken: true\n\n# -- Automatically mount API credentials for a particular Pod\nautomountServiceAccountToken: true\n\n# -- Node selector for pods\nnodeSelector: {}\n\n# -- Affinity configuration for pods\naffinity: {}\n\n# -- Tolerations for pods. By default, pods will be scheduled on master/control-plane nodes.\ntolerations:\n  - key: node-role.kubernetes.io/master\n    operator: Exists\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/control-plane\n    operator: Exists\n    effect: NoSchedule\n\n# -- Default volumes that are mounted into pods. In most cases, these should not be changed.\n# Use `extraVolumes`/`extraVolumeMounts` for additional custom volumes.\n# @default -- See `values.yaml`\ndefaultVolumes:\n  - name: run\n    hostPath:\n      path: /run/promtail\n  - name: containers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: pods\n    hostPath:\n      path: /var/log/pods\n\n# -- Default volume mounts. Corresponds to `volumes`.\n# @default -- See `values.yaml`\ndefaultVolumeMounts:\n  - name: run\n    mountPath: /run/promtail\n  - name: containers\n    mountPath: /var/lib/docker/containers\n    readOnly: true\n  - name: pods\n    mountPath: /var/log/pods\n    readOnly: true\n\n# Extra volumes to be added in addition to those specified under `defaultVolumes`.\nextraVolumes: []\n\n# Extra volume mounts together. Corresponds to `extraVolumes`.\nextraVolumeMounts: []\n\n# Extra args for the Promtail container.\nextraArgs: []\n# -- Example:\n# -- extraArgs:\n# --   - -client.external-labels=hostname=$(HOSTNAME)\n\n# -- Extra environment variables. Set up tracing enviroment variables here if .Values.config.enableTracing is true.\n# Tracing currently only support configure via environment variables. See:\n# https://grafana.com/docs/loki/latest/clients/promtail/configuration/#tracing_config\n# https://www.jaegertracing.io/docs/1.16/client-features/\nextraEnv: []\n\n# -- Extra environment variables from secrets or configmaps\nextraEnvFrom: []\n\n# -- Configure enableServiceLinks in pod\nenableServiceLinks: true\n\n# ServiceMonitor configuration\nserviceMonitor:\n  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n  enabled: false\n  # -- Alternative namespace for ServiceMonitor resources\n  namespace: null\n  # -- Namespace selector for ServiceMonitor resources\n  namespaceSelector: {}\n  # -- ServiceMonitor annotations\n  annotations: {}\n  # -- Additional ServiceMonitor labels\n  labels: {}\n  # -- ServiceMonitor scrape interval\n  interval: null\n  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n  scrapeTimeout: null\n  # -- ServiceMonitor relabel configs to apply to samples before scraping\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `relabel_configs`)\n  relabelings: []\n  # -- ServiceMonitor relabel configs to apply to samples as the last\n  # step before ingestion\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `metric_relabel_configs`)\n  metricRelabelings: []\n  # -- ServiceMonitor will add labels from the service to the Prometheus metric\n  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec\n  targetLabels: []\n  # -- ServiceMonitor will use http by default, but you can pick https as well\n  scheme: http\n  # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n  tlsConfig: null\n  # -- Prometheus rules will be deployed for alerting purposes\n  prometheusRule:\n    enabled: false\n    additionalLabels: {}\n    # namespace:\n    rules: []\n    #  - alert: PromtailRequestErrors\n    #    expr: 100 * sum(rate(promtail_request_duration_seconds_count{status_code=~\"5..|failed\"}[1m])) by (namespace, job, route, instance) / sum(rate(promtail_request_duration_seconds_count[1m])) by (namespace, job, route, instance) \u003e 10\n    #    for: 5m\n    #    labels:\n    #      severity: critical\n    #    annotations:\n    #      description: |\n    #        The {{ $labels.job }} {{ $labels.route }} is experiencing\n    #        {{ printf \\\"%.2f\\\" $value }} errors.\n    #        VALUE = {{ $value }}\n    #        LABELS = {{ $labels }}\n    #      summary: Promtail request errors (instance {{ $labels.instance }})\n    #  - alert: PromtailRequestLatency\n    #    expr: histogram_quantile(0.99, sum(rate(promtail_request_duration_seconds_bucket[5m])) by (le)) \u003e 1\n    #    for: 5m\n    #    labels:\n    #      severity: critical\n    #    annotations:\n    #      summary: Promtail request latency (instance {{ $labels.instance }})\n    #      description: |\n    #        The {{ $labels.job }} {{ $labels.route }} is experiencing\n    #        {{ printf \\\"%.2f\\\" $value }}s 99th percentile latency.\n    #        VALUE = {{ $value }}\n    #        LABELS = {{ $labels }}\n\n# Extra containers created as part of a Promtail Deployment resource\n# - spec for Container:\n#   https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core\n#\n# Note that the key is used as the `name` field, i.e. below will create a\n# container named `promtail-proxy`.\nextraContainers: {}\n  # promtail-proxy:\n  #   image: nginx\n  #   ...\n\n# -- Configure additional ports and services. For each configured port, a corresponding service is created.\n# See values.yaml for details\nextraPorts: {}\n#  syslog:\n#    name: tcp-syslog\n#    annotations: {}\n#    labels: {}\n#    containerPort: 1514\n#    protocol: TCP\n#    service:\n#      type: ClusterIP\n#      clusterIP: null\n#      port: 1514\n#      externalIPs: []\n#      nodePort: null\n#      loadBalancerIP: null\n#      loadBalancerSourceRanges: []\n#      externalTrafficPolicy: null\n#    ingress:\n#      # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n#      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n#      # ingressClassName: nginx\n#      # Values can be templated\n#      annotations: {}\n#        # kubernetes.io/ingress.class: nginx\n#        # kubernetes.io/tls-acme: \"true\"\n#      paths: \"/\"\n#      hosts:\n#        - chart-example.local\n#\n#      tls: []\n#      #  - secretName: chart-example-tls\n#      #    hosts:\n#      #      - chart-example.local\n\n\n# -- PodSecurityPolicy configuration.\n# @default -- See `values.yaml`\npodSecurityPolicy:\n  privileged: true\n  allowPrivilegeEscalation: true\n  volumes:\n    - 'secret'\n    - 'hostPath'\n    - 'downwardAPI'\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    rule: 'RunAsAny'\n  seLinux:\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'RunAsAny'\n  fsGroup:\n    rule: 'RunAsAny'\n  readOnlyRootFilesystem: true\n  requiredDropCapabilities:\n    - ALL\n\n# -- Section for crafting Promtails config file. The only directly relevant value is `config.file`\n# which is a templated string that references the other values and snippets below this key.\n# @default -- See `values.yaml`\nconfig:\n  # -- Enable Promtail config from Helm chart\n  # Set `configmap.enabled: true` and this to `false` to manage your own Promtail config\n  # See default config in `values.yaml`\n  enabled: true\n  # -- The log level of the Promtail server\n  # Must be reference in `config.file` to configure `server.log_level`\n  # See default config in `values.yaml`\n  logLevel: info\n  # -- The log format of the Promtail server\n  # Must be reference in `config.file` to configure `server.log_format`\n  # Valid formats: `logfmt, json`\n  # See default config in `values.yaml`\n  logFormat: logfmt\n  # -- The port of the Promtail server\n  # Must be reference in `config.file` to configure `server.http_listen_port`\n  # See default config in `values.yaml`\n  serverPort: 3101\n  # -- The config of clients of the Promtail server\n  # Must be reference in `config.file` to configure `clients`\n  # @default -- See `values.yaml`\n  clients:\n    - url: http://loki-gateway:80/loki/api/v1/push\n  # -- Configures where Promtail will save it's positions file, to resume reading after restarts.\n  # Must be referenced in `config.file` to configure `positions`\n  positions:\n    filename: /run/promtail/positions.yaml\n  # -- The config to enable tracing\n  enableTracing: false\n  # -- A section of reusable snippets that can be reference in `config.file`.\n  # Custom snippets may be added in order to reduce redundancy.\n  # This is especially helpful when multiple `kubernetes_sd_configs` are use which usually have large parts in common.\n  # @default -- See `values.yaml`\n  snippets:\n    pipelineStages:\n      - cri: {}\n    common:\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n          - namespace\n          - app\n        target_label: job\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_uid\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        regex: true/(.*)\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n\n    # If set to true, adds an additional label for the scrape job.\n    # This helps debug the Promtail config.\n    addScrapeJobLabel: false\n\n    # -- You can put here any keys that will be directly added to the config file's 'limits_config' block.\n    # @default -- empty\n    extraLimitsConfig: \"\"\n\n    # -- You can put here any keys that will be directly added to the config file's 'server' block.\n    # @default -- empty\n    extraServerConfigs: \"\"\n\n    # -- You can put here any additional scrape configs you want to add to the config file.\n    # @default -- empty\n    extraScrapeConfigs: \"\"\n\n    # -- You can put here any additional relabel_configs to \"kubernetes-pods\" job\n    extraRelabelConfigs: []\n\n    scrapeConfigs: |\n      # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\n      - job_name: kubernetes-pods\n        pipeline_stages:\n          {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels:\n              - __meta_kubernetes_pod_controller_name\n            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n            action: replace\n            target_label: __tmp_controller_name\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_name\n              - __meta_kubernetes_pod_label_app\n              - __tmp_controller_name\n              - __meta_kubernetes_pod_name\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: app\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n              - __meta_kubernetes_pod_label_instance\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: instance\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_component\n              - __meta_kubernetes_pod_label_component\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: component\n          {{- if .Values.config.snippets.addScrapeJobLabel }}\n          - replacement: kubernetes-pods\n            target_label: scrape_job\n          {{- end }}\n          {{- toYaml .Values.config.snippets.common | nindent 4 }}\n          {{- with .Values.config.snippets.extraRelabelConfigs }}\n          {{- toYaml . | nindent 4 }}\n          {{- end }}\n\n  # -- Config file contents for Promtail.\n  # Must be configured as string.\n  # It is templated so it can be assembled from reusable snippets in order to avoid redundancy.\n  # @default -- See `values.yaml`\n  file: |\n    server:\n      log_level: {{ .Values.config.logLevel }}\n      log_format: {{ .Values.config.logFormat }}\n      http_listen_port: {{ .Values.config.serverPort }}\n      {{- with .Values.httpPathPrefix }}\n      http_path_prefix: {{ . }}\n      {{- end }}\n      {{- tpl .Values.config.snippets.extraServerConfigs . | nindent 2 }}\n\n    clients:\n      {{- tpl (toYaml .Values.config.clients) . | nindent 2 }}\n\n    positions:\n      {{- tpl (toYaml .Values.config.positions) . | nindent 2 }}\n\n    scrape_configs:\n      {{- tpl .Values.config.snippets.scrapeConfigs . | nindent 2 }}\n      {{- tpl .Values.config.snippets.extraScrapeConfigs . | nindent 2 }}\n\n    limits_config:\n      {{- tpl .Values.config.snippets.extraLimitsConfig . | nindent 2 }}\n\n    tracing:\n      enabled: {{ .Values.config.enableTracing }}\n\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also neeed the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  k8sApi:\n    # -- Specify the k8s API endpoint port\n    port: 8443\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n\n# -- Base path to server all API routes fro\nhttpPathPrefix: \"\"\n\nsidecar:\n  configReloader:\n    enabled: false\n    image:\n      # -- The Docker registry for sidecar config-reloader\n      registry: ghcr.io\n      # -- Docker image repository for sidecar config-reloader\n      repository: jimmidyson/configmap-reload\n      # -- Docker image tag for sidecar config-reloader\n      tag: v0.12.0\n      # -- Docker image pull policy for sidecar config-reloader\n      pullPolicy: IfNotPresent\n    # Extra args for the config-reloader container.\n    extraArgs: []\n    # -- Extra environment variables for sidecar config-reloader\n    extraEnv: []\n    # -- Extra environment variables from secrets or configmaps for sidecar config-reloader\n    extraEnvFrom: []\n    # -- The security context for containers for sidecar config-reloader\n    containerSecurityContext:\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop:\n          - ALL\n      allowPrivilegeEscalation: false\n    # -- Readiness probe for sidecar config-reloader\n    readinessProbe: {}\n    # -- Liveness probe for sidecar config-reloader\n    livenessProbe: {}\n    # -- Resource requests and limits for sidecar config-reloader\n    resources: {}\n    #  limits:\n    #    cpu: 200m\n    #    memory: 128Mi\n    #  requests:\n    #    cpu: 100m\n    #    memory: 128Mi\n    config:\n      # -- The port of the config-reloader server\n      serverPort: 9533\n    serviceMonitor:\n      enabled: true\n\n# -- Extra K8s manifests to deploy\nextraObjects: []\n  # - apiVersion: \"kubernetes-client.io/v1\"\n  #   kind: ExternalSecret\n  #   metadata:\n  #     name: promtail-secrets\n  #   spec:\n  #     backendType: gcpSecretsManager\n  #     data:\n  #       - key: promtail-oauth2-creds\n  #         name: client_secret\n"
            ],
            "verify": false,
            "version": "6.17.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "tempo",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "tempo",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "tempo",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.8.2",
                "chart": "tempo",
                "first_deployed": 1761160487,
                "last_deployed": 1761178674,
                "name": "tempo",
                "namespace": "monitoring",
                "notes": "",
                "revision": 6,
                "values": "{\"affinity\":{},\"annotations\":{},\"config\":\"memberlist:\\n  cluster_label: \\\"{{ .Release.Name }}.{{ .Release.Namespace }}\\\"\\nmultitenancy_enabled: {{ .Values.tempo.multitenancyEnabled }}\\nusage_report:\\n  reporting_enabled: {{ .Values.tempo.reportingEnabled }}\\ncompactor:\\n  compaction:\\n    block_retention: {{ .Values.tempo.retention }}\\ndistributor:\\n  receivers:\\n    {{- toYaml .Values.tempo.receivers | nindent 8 }}\\ningester:\\n  {{- toYaml .Values.tempo.ingester | nindent 6 }}\\nserver:\\n  {{- toYaml .Values.tempo.server | nindent 6 }}\\nstorage:\\n  {{- toYaml .Values.tempo.storage | nindent 6 }}\\nquerier:\\n  {{- toYaml .Values.tempo.querier | nindent 6 }}\\nquery_frontend:\\n  {{- toYaml .Values.tempo.queryFrontend | nindent 6 }}\\noverrides:\\n  {{- toYaml .Values.tempo.overrides | nindent 6 }}\\n  {{- if .Values.tempo.metricsGenerator.enabled }}\\nmetrics_generator:\\n      storage:\\n        path: \\\"/tmp/tempo\\\"\\n        remote_write:\\n          - url: {{ .Values.tempo.metricsGenerator.remoteWriteUrl }}\\n      traces_storage:\\n        path: \\\"/tmp/traces\\\"\\n  {{- end }}\\n\",\"extraLabels\":{},\"extraVolumes\":[],\"fullnameOverride\":\"\",\"global\":{\"commonLabels\":{}},\"hostAliases\":[],\"labels\":{},\"nameOverride\":\"\",\"networkPolicy\":{\"allowExternal\":true,\"egress\":{\"blockDNSResolution\":false,\"enabled\":false,\"ports\":[],\"to\":[]},\"enabled\":false,\"explicitNamespacesSelector\":{},\"ingress\":true},\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"enableStatefulSetAutoDeletePVC\":false,\"enabled\":false,\"size\":\"10Gi\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":1,\"securityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001},\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"loadBalancerIP\":null,\"protocol\":\"TCP\",\"targetPort\":\"\",\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"imagePullSecrets\":[],\"labels\":{},\"name\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":\"\"},\"tempo\":{\"extraArgs\":{},\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"ingester\":{},\"livenessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/ready\",\"port\":3200},\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"memBallastSizeMbs\":1024,\"metricsGenerator\":{\"enabled\":true,\"remoteWriteUrl\":\"http://prometheus.monitoring:9090/api/v1/write\"},\"multitenancyEnabled\":false,\"overrides\":{\"defaults\":{},\"per_tenant_override_config\":\"/conf/overrides.yaml\"},\"per_tenant_overrides\":{},\"pullPolicy\":\"IfNotPresent\",\"querier\":{},\"queryFrontend\":{},\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/ready\",\"port\":3200},\"initialDelaySeconds\":20,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"receivers\":{\"jaeger\":{\"protocols\":{\"grpc\":{\"endpoint\":\"0.0.0.0:14250\"},\"thrift_binary\":{\"endpoint\":\"0.0.0.0:6832\"},\"thrift_compact\":{\"endpoint\":\"0.0.0.0:6831\"},\"thrift_http\":{\"endpoint\":\"0.0.0.0:14268\"}}},\"opencensus\":null,\"otlp\":{\"protocols\":{\"grpc\":{\"endpoint\":\"0.0.0.0:4317\"},\"http\":{\"endpoint\":\"0.0.0.0:4318\"}}}},\"reportingEnabled\":true,\"repository\":\"grafana/tempo\",\"resources\":{},\"retention\":\"24h\",\"securityContext\":{},\"server\":{\"http_listen_port\":3200},\"storage\":{\"trace\":{\"backend\":\"local\",\"local\":{\"path\":\"/var/tempo/traces\"},\"wal\":{\"path\":\"/var/tempo/wal\"}}},\"tag\":\"\",\"updateStrategy\":\"RollingUpdate\"},\"tempoQuery\":{\"enabled\":false,\"extraArgs\":{},\"extraEnv\":[],\"extraVolumeMounts\":[],\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraPaths\":[],\"hosts\":[\"query.tempo.example.com\"],\"labels\":{},\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"pullPolicy\":\"IfNotPresent\",\"repository\":\"grafana/tempo-query\",\"resources\":{},\"securityContext\":{},\"service\":{\"port\":16686},\"tag\":null},\"tolerations\":[]}",
                "version": "1.23.3"
              }
            ],
            "name": "tempo",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "global:\n  # -- Common labels for all object directly managed by this chart.\n  commonLabels: {}\n\n# -- Overrides the chart's name\nnameOverride: \"\"\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: \"\"\n\n# -- Define the amount of instances\nreplicas: 1\n\n# -- Number of old history to retain to allow rollback (If not set, default Kubernetes value is set to 10)\n# revisionHistoryLimit: 1\n\n# -- labels for tempo\nlabels: {}\n\n# -- Annotations for the StatefulSet\nannotations: {}\n\ntempo:\n  repository: grafana/tempo\n  tag: \"\"\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  # pullSecrets:\n  #   - myRegistryKeySecretName\n\n  updateStrategy: RollingUpdate\n  resources: {}\n  #  requests:\n  #    cpu: 1000m\n  #    memory: 4Gi\n  #  limits:\n  #    cpu: 2000m\n  #    memory: 6Gi\n\n  memBallastSizeMbs: 1024\n  multitenancyEnabled: false\n  # -- If true, Tempo will report anonymous usage data about the shape of a deployment to Grafana Labs\n  reportingEnabled: true\n  metricsGenerator:\n    # -- If true, enables Tempo's metrics generator (https://grafana.com/docs/tempo/next/metrics-generator/)\n    enabled: true\n    remoteWriteUrl: \"http://prometheus.monitoring:9090/api/v1/write\"\n  # -- Configuration options for the ingester.\n  # Refers to: https://grafana.com/docs/tempo/latest/configuration/#ingester\n  ingester: {}\n  #  flush_check_period: 10s\n  #  trace_idle_period: 10s\n  #  max_block_duration: 30m\n  #  complete_block_timeout: 1h\n  # -- Configuration options for the querier.\n  # Refers to: https://grafana.com/docs/tempo/latest/configuration/#querier\n  querier: {}\n  #  max_concurrent_queries: 20\n  # -- Configuration options for the query-fronted.\n  # Refers to: https://grafana.com/docs/tempo/latest/configuration/#query-frontend\n  queryFrontend: {}\n  #  search:\n  #    concurrent_jobs: 2000\n  retention: 24h\n  # -- The standard overrides configuration section. This can include a `defaults` object for applying to all tenants (not to be confused with the `global` property of the same name, which overrides `max_byte_per_trace` for all tenants). For an example on how to enable the metrics generator using the `overrides` object, see the 'Activate metrics generator' section below. Refer to [Standard overrides](https://grafana.com/docs/tempo/latest/configuration/#standard-overrides) for more details.\n  overrides:\n    # -- Default config values for all tenants, can be overridden by per-tenant overrides. If a tenant's specific overrides are not found in the `per_tenant_overrides` block, the values in this `default` block will be used. Configs inside this block should follow the new overrides indentation format\n    defaults: {}\n    #  metrics_generator:\n    #    processors:\n    #      - service-graphs\n    #      - span-metrics\n\n    # -- Path to the per tenant override config file. The values of the `per_tenant_overrides` config below will be written to the default path `/conf/overrides.yaml`. Users can set tenant-specific overrides settings in a separate file and point per_tenant_override_config to it if not using the per_tenant_overrides block below.\n    per_tenant_override_config: /conf/overrides.yaml\n  # -- The `per tenant` aka `tenant-specific` runtime overrides. This allows overriding values set in the configuration on a per-tenant basis. Note that *all* values must be given for each per-tenant configuration block. Refer to [Runtime overrides](https://grafana.com/docs/tempo/latest/configuration/#runtime-overrides) and [Tenant-Specific overrides](https://grafana.com/docs/tempo/latest/configuration/#tenant-specific-overrides) documentation for more details.\n  per_tenant_overrides: {}\n    # 'tenant-id':\n    #  metrics_generator:\n    #    processors:\n    #      - service-graphs\n    #      - span-metrics\n\n  # Tempo server configuration.\n  # Refers to: https://grafana.com/docs/tempo/latest/configuration/#server\n  server:\n    # -- HTTP server listen port\n    http_listen_port: 3200\n  # Readiness and Liveness Probe Configuration Options\n  livenessProbe:\n    httpGet:\n      path: /ready\n      port: 3200\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 3\n    successThreshold: 1\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: 3200\n    initialDelaySeconds: 20\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 3\n    successThreshold: 1\n  storage:\n    trace:\n      # tempo storage backend.\n      # Refers to: https://grafana.com/docs/tempo/latest/configuration/#storage\n      ## Use s3 for example\n      # backend: s3\n      # store traces in s3\n      # s3:\n      #   bucket: \u003cyour s3 bucket\u003e                        # store traces in this bucket\n      #   endpoint: s3.dualstack.us-east-2.amazonaws.com  # api endpoint\n      #   access_key: ...                                 # optional. access key when using static credentials.\n      #   secret_key: ...                                 # optional. secret key when using static credentials.\n      #   insecure: false                                 # optional. enable if endpoint is http\n      backend: local\n      local:\n        path: /var/tempo/traces\n      wal:\n        path: /var/tempo/wal\n  # this configuration will listen on all ports and protocols that tempo is capable of.\n  # the receives all come from the OpenTelemetry collector.  more configuration information can\n  # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/master/receiver\n  receivers:\n    jaeger:\n      protocols:\n        grpc:\n          endpoint: 0.0.0.0:14250\n        thrift_binary:\n          endpoint: 0.0.0.0:6832\n        thrift_compact:\n          endpoint: 0.0.0.0:6831\n        thrift_http:\n          endpoint: 0.0.0.0:14268\n    opencensus:\n    otlp:\n      protocols:\n        grpc:\n          endpoint: \"0.0.0.0:4317\"\n        http:\n          endpoint: \"0.0.0.0:4318\"\n  securityContext: {}\n    # allowPrivilegeEscalation: false\n    #  capabilities:\n    #    drop:\n    #    - ALL\n    # readOnlyRootFilesystem: true\n  ## Additional container arguments\n  extraArgs: {}\n  # -- Environment variables to add\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the ingester pods\n  extraEnvFrom: []\n  # -- Volume mounts to add\n  extraVolumeMounts: []\n  # - name: extra-volume\n  #   mountPath: /mnt/volume\n  #   readOnly: true\n  #   existingClaim: volume-claim\n\n# -- Tempo configuration file contents\n# @default -- Dynamically generated tempo configmap\nconfig: |\n    memberlist:\n      cluster_label: \"{{ .Release.Name }}.{{ .Release.Namespace }}\"\n    multitenancy_enabled: {{ .Values.tempo.multitenancyEnabled }}\n    usage_report:\n      reporting_enabled: {{ .Values.tempo.reportingEnabled }}\n    compactor:\n      compaction:\n        block_retention: {{ .Values.tempo.retention }}\n    distributor:\n      receivers:\n        {{- toYaml .Values.tempo.receivers | nindent 8 }}\n    ingester:\n      {{- toYaml .Values.tempo.ingester | nindent 6 }}\n    server:\n      {{- toYaml .Values.tempo.server | nindent 6 }}\n    storage:\n      {{- toYaml .Values.tempo.storage | nindent 6 }}\n    querier:\n      {{- toYaml .Values.tempo.querier | nindent 6 }}\n    query_frontend:\n      {{- toYaml .Values.tempo.queryFrontend | nindent 6 }}\n    overrides:\n      {{- toYaml .Values.tempo.overrides | nindent 6 }}\n      {{- if .Values.tempo.metricsGenerator.enabled }}\n    metrics_generator:\n          storage:\n            path: \"/tmp/tempo\"\n            remote_write:\n              - url: {{ .Values.tempo.metricsGenerator.remoteWriteUrl }}\n          traces_storage:\n            path: \"/tmp/traces\"\n      {{- end }}\n\ntempoQuery:\n  repository: grafana/tempo-query\n  tag: null\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## Refers to: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  # pullSecrets:\n  #   - myRegistryKeySecretName\n\n  # -- if False the tempo-query container is not deployed\n  enabled: false\n\n  service:\n    port: 16686\n\n  ingress:\n    enabled: false\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n    # Values can be templated\n    annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n    labels: {}\n    path: /\n\n    # pathType is only for k8s \u003e= 1.1=\n    pathType: Prefix\n\n    hosts:\n      - query.tempo.example.com\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n    ## Or for k8s \u003e 1.19\n    # - path: /*\n    #   pathType: Prefix\n    #   backend:\n    #     service:\n    #       name: ssl-redirect\n    #       port:\n    #         name: use-annotation\n\n\n    tls: []\n    #  - secretName: tempo-query-tls\n    #    hosts:\n    #      - query.tempo.example.com\n\n  resources: {}\n  #  requests:\n  #    cpu: 1000m\n  #    memory: 4Gi\n  #  limits:\n  #    cpu: 2000m\n  #    memory: 6Gi\n\n  ## Additional container arguments\n  extraArgs: {}\n  # -- Environment variables to add\n  extraEnv: []\n  # -- Volume mounts to add\n  extraVolumeMounts: []\n  # - name: extra-volume\n  #   mountPath: /mnt/volume\n  #   readOnly: true\n  #   existingClaim: volume-claim\n  securityContext: {}\n    # allowPrivilegeEscalation: false\n    #  capabilities:\n    #    drop:\n    #    - ALL\n    # readOnlyRootFilesystem: false # fails if true, do not enable\n\n# -- securityContext for container\nsecurityContext:\n  runAsUser: 10001\n  runAsGroup: 10001\n  fsGroup: 10001\n  runAsNonRoot: true\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n  # -- Labels for the service account\n  labels: {}\n  automountServiceAccountToken: true\n\nservice:\n  type: ClusterIP\n  clusterIP: \"\"\n  # -- (string) IP address, in case of 'type: LoadBalancer'\n  loadBalancerIP:\n  # -- If service type is LoadBalancer, the exposed protocol can either be \"UDP\", \"TCP\" or \"UDP,TCP\"\n  protocol: \"TCP\"\n\n  annotations: {}\n  labels: {}\n  targetPort: \"\"\n\nserviceMonitor:\n  enabled: false\n  interval: \"\"\n  additionalLabels: {}\n  annotations: {}\n  # scrapeTimeout: 10s\n\npersistence:\n  enabled: false\n    # -- Enable StatefulSetAutoDeletePVC feature\n  enableStatefulSetAutoDeletePVC: false\n  # storageClassName: local-path\n  accessModes:\n    - ReadWriteOnce\n  size: 10Gi\n\n# -- Pod Annotations\npodAnnotations: {}\n\n# -- Pod (extra) Labels\npodLabels: {}\n\n# Apply extra labels to common labels.\nextraLabels: {}\n\n# -- Volumes to add\nextraVolumes: []\n\n# -- Node labels for pod assignment. See: https://kubernetes.io/docs/user-guide/node-selection/\nnodeSelector: {}\n\n# -- Tolerations for pod assignment. See: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\ntolerations: []\n\n# -- Affinity for pod assignment. See: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\naffinity: {}\n\n# -- The name of the PriorityClass\npriorityClassName: null\n# -- hostAliases to add\nhostAliases: []\n#  - ip: 1.2.3.4\n#    hostnames:\n#      - domain.tld\nnetworkPolicy:\n  ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources. Only Ingress traffic is filtered for now.\n  ##\n  enabled: false\n  ## @param networkPolicy.allowExternal Don't require client label for connections\n  ## The Policy model to apply. When set to false, only pods with the correct\n  ## client label will have network access to  tempo port defined.\n  ## When true, tempo will accept connections from any source\n  ## (with the correct destination port).\n  ##\n  ingress: true\n  ## @param networkPolicy.ingress When true enables the creation\n  ## an ingress network policy\n  ##\n  allowExternal: true\n  ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed\n  ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace\n  ## and that match other criteria, the ones that have the good label, can reach the tempo.\n  ## But sometimes, we want the tempo to be accessible to clients from other namespaces, in this case, we can use this\n  ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.\n  ##\n  ## Example:\n  ## explicitNamespacesSelector:\n  ##   matchLabels:\n  ##     role: frontend\n  ##   matchExpressions:\n  ##    - {key: role, operator: In, values: [frontend]}\n  ##\n  explicitNamespacesSelector: {}\n  ##\n  egress:\n    ## @param networkPolicy.egress.enabled When enabled, an egress network policy will be\n    ## created allowing tempo to connect to external data sources from kubernetes cluster.\n    enabled: false\n    ##\n    ## @param networkPolicy.egress.blockDNSResolution When enabled, DNS resolution will be blocked\n    ## for all pods in the tempo namespace.\n    blockDNSResolution: false\n    ##\n    ## @param networkPolicy.egress.ports Add individual ports to be allowed by the egress\n    ports: []\n    ## Add ports to the egress by specifying - port: \u003cport number\u003e\n    ## E.X.\n    ## - port: 80\n    ## - port: 443\n    ##\n    ## @param networkPolicy.egress.to Allow egress traffic to specific destinations\n    to: []\n    ## Add destinations to the egress by specifying - ipBlock: \u003cCIDR\u003e\n    ## E.X.\n    ## to:\n    ##  - namespaceSelector:\n    ##    matchExpressions:\n  ##    - {key: role, operator: In, values: [tempo]}\n"
            ],
            "verify": false,
            "version": "1.23.3",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    }
  ],
  "check_results": null
}
